#!/usr/bin/env python3
"""MOAI í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""

from transformers import AutoTokenizer

def main():
    print("=" * 60)
    print("ğŸ§ª MOAI í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained('./tokenizers/moai')
    
    print(f"\nğŸ“Š í† í¬ë‚˜ì´ì € ì •ë³´:")
    print(f"   - Vocab í¬ê¸°: {tokenizer.vocab_size:,}")
    print(f"   - PAD: {tokenizer.pad_token} (id={tokenizer.pad_token_id})")
    print(f"   - BOS: {tokenizer.bos_token} (id={tokenizer.bos_token_id})")
    print(f"   - EOS: {tokenizer.eos_token} (id={tokenizer.eos_token_id})")
    print(f"   - UNK: {tokenizer.unk_token} (id={tokenizer.unk_token_id})")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        ("í•œêµ­ì–´ ê¸°ë³¸", "ì•ˆë…•í•˜ì„¸ìš”. í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."),
        ("ì˜ì–´ ê¸°ë³¸", "Hello, world! This is a test."),
        ("BCì¹´ë“œ ê¸ˆìœµ", "BCì¹´ë“œ ì‹ ìš©ì¹´ë“œ ê²°ì œ ì„œë¹„ìŠ¤ ì•ˆë‚´"),
        ("ê¸ˆìœµ ìš©ì–´", "ì‹ ìš©ëŒ€ì¶œ ê¸ˆë¦¬ ë° í• ë¶€ ì„œë¹„ìŠ¤ ë¬¸ì˜"),
        ("í˜¼í•© í…ìŠ¤íŠ¸", "Hello ì•ˆë…• é‡‘è finance í…ŒìŠ¤íŠ¸"),
        ("ì½”ë“œ", "def hello(): print('Hello, World!')"),
        ("ìˆ«ì", "2024ë…„ 1ì›” 15ì¼ ê¸ˆì•¡: 1,234,567ì›"),
    ]
    
    print("\n" + "=" * 60)
    print("ğŸ“ í† í°í™” í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for name, text in test_cases:
        tokens = tokenizer.tokenize(text)
        ids = tokenizer.encode(text, add_special_tokens=False)
        
        print(f"\n[{name}]")
        print(f"   ì…ë ¥: {text}")
        print(f"   í† í° ìˆ˜: {len(tokens)}")
        print(f"   í† í°: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        print(f"   ID: {ids[:10]}{'...' if len(ids) > 10 else ''}")
    
    # ì¸ì½”ë“œ/ë””ì½”ë“œ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("ğŸ”„ ì¸ì½”ë“œ/ë””ì½”ë“œ ì™•ë³µ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    test_text = "BCì¹´ë“œ ê¸ˆìœµ ì„œë¹„ìŠ¤: Hello World! ì‹ ìš©ì¹´ë“œ ê²°ì œ"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)
    
    print(f"\n   ì›ë³¸: {test_text}")
    print(f"   ì¸ì½”ë”©: {encoded}")
    print(f"   ë””ì½”ë”©: {decoded}")
    print(f"   ì¼ì¹˜: {'âœ…' if test_text == decoded else 'âŒ'}")
    
    # ì±„íŒ… í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("ğŸ’¬ íŠ¹ìˆ˜ í† í° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    special_tokens = ["<pad>", "<s>", "</s>", "<unk>", "<|endoftext|>", "<|im_start|>", "<|im_end|>"]
    for token in special_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        print(f"   {token}: id={token_id}")
    
    print("\n" + "=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
