#!/usr/bin/env python3
"""
í† í¬ë‚˜ì´ì € ì†ë„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
GIL ìš°íšŒê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""
import os
import time
from transformers import AutoTokenizer

# TOKENIZERS_PARALLELISM í™œì„±í™”
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
print("ğŸ”„ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-3B",
    use_fast=True,
)

# Fast Tokenizer í™•ì¸
print(f"âœ“ Is Fast Tokenizer: {tokenizer.is_fast}")
print(f"âœ“ Tokenizer type: {type(tokenizer)}")
print()

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬)
test_texts = [
    "This is a sample text for tokenization. " * 50  # ~50 words
] * 10000  # 10,000 samples

print(f"ğŸ“Š Testing with {len(test_texts):,} samples...")
print()

# í…ŒìŠ¤íŠ¸ 1: ë‹¨ìˆœ í† í¬ë‚˜ì´ì§• (Python GIL ì˜í–¥ ìµœì†Œ)
print("Test 1: Pure tokenization (minimal Python overhead)")
start = time.time()
result = tokenizer(test_texts, truncation=False, padding=False)
elapsed = time.time() - start
speed = len(test_texts) / elapsed
print(f"  Time: {elapsed:.2f}s")
print(f"  Speed: {speed:,.0f} examples/s")
print()

# í…ŒìŠ¤íŠ¸ 2: ë°°ì¹˜ í† í¬ë‚˜ì´ì§• (datasets.mapê³¼ ìœ ì‚¬)
print("Test 2: Batched processing (similar to datasets.map)")
batch_size = 10000
num_batches = len(test_texts) // batch_size

start = time.time()
for i in range(num_batches):
    batch = test_texts[i * batch_size:(i + 1) * batch_size]
    _ = tokenizer(batch, truncation=False, padding=False)
elapsed = time.time() - start
speed = len(test_texts) / elapsed
print(f"  Time: {elapsed:.2f}s")
print(f"  Speed: {speed:,.0f} examples/s")
print()

# í…ŒìŠ¤íŠ¸ 3: ì‘ì€ ë°°ì¹˜ (datasets.map ê¸°ë³¸ ë™ì‘)
print("Test 3: Small batches (1000 samples per batch)")
batch_size = 1000
num_batches = len(test_texts) // batch_size

start = time.time()
for i in range(num_batches):
    batch = test_texts[i * batch_size:(i + 1) * batch_size]
    _ = tokenizer(batch, truncation=False, padding=False)
elapsed = time.time() - start
speed = len(test_texts) / elapsed
print(f"  Time: {elapsed:.2f}s")
print(f"  Speed: {speed:,.0f} examples/s")
print()

print("="*60)
print("ğŸ“ ê²°ë¡ :")
print("  - Fast Tokenizerê°€ 50,000+ examples/së¥¼ ë‹¬ì„±í•œë‹¤ë©´:")
print("    â†’ GIL ë¬¸ì œ ì—†ìŒ, ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œë„ ì¶©ë¶„")
print("  - Fast Tokenizerê°€ ì—¬ì „íˆ ëŠë¦¬ë‹¤ë©´:")
print("    â†’ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë²„í—¤ë“œê°€ ë³‘ëª©")
print("    â†’ Multiprocessing ë³‘í–‰ì´ í•„ìš”")
print("="*60)

