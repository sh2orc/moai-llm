"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

import argparse
import os
import hashlib
import time
import gc
import logging
from pathlib import Path
from typing import Optional, Dict, Any
try:
    import orjson as json  # Rust-based, 10-50x faster
except ImportError:
    import json  # Fallback to standard json

try:
    import psutil  # For memory monitoring
except ImportError:
    psutil = None  # Optional dependency

import torch
from transformers import (
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset, disable_caching
import datasets

# Enable memory-efficient settings for large datasets
datasets.config.IN_MEMORY_MAX_SIZE = 0  # Force memory mapping (no in-memory)

from moai_llm.config import MoaiConfig
from moai_llm.modeling.model import MoaiForCausalLM

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    
    ì´ë ‡ê²Œ í•˜ë©´ max_seq_lengthë¡œ ì˜ë¦¬ë”ë¼ë„ ë‹¤ìŒ ì²­í¬ì—ì„œ 
    ì´ì–´ì„œ EOSê¹Œì§€ ì˜¨ì „íˆ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID
    
    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np
    
    # 1. ì´ ê¸¸ì´ ê³„ì‚° (ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹ìš©)
    total_len = 0
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        total_len += len(input_ids)
        if len(input_ids) > 0 and input_ids[-1] != eos_token_id:
            total_len += 1  # EOS ì¶”ê°€ë  ì˜ˆì •
    
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences into ~{total_len:,} tokens")
    
    # 2. numpy ë°°ì—´ë¡œ ë¹ ë¥´ê²Œ ì—°ê²° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    all_tokens = np.empty(total_len, dtype=np.int32)
    offset = 0
    
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)
        
        if seq_len == 0:
            continue
            
        # ë°°ì—´ì— ë³µì‚¬
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len
        
        # EOS ì¶”ê°€
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1
    
    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]
    
    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í•  (list comprehensionìœ¼ë¡œ ë¹ ë¥´ê²Œ)
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []
    
    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk = all_tokens[start:end].tolist()
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ (< 128) ë²„ë¦¼
        if len(chunk) < 128:
            logger.info(f"  Dropping short final chunk of {len(chunk)} tokens")
            continue
            
        chunks.append({
            "input_ids": chunk,
            "attention_mask": [1] * len(chunk),
        })
    
    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    
    return chunks


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None):
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"
    
    DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    # DDP í™˜ê²½ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© - Trainer ì´ˆê¸°í™” ì „ì—ë„ ì‘ë™)
    try:
        # í™˜ê²½ ë³€ìˆ˜ë¡œ rank í™•ì¸ (torchrunì´ ì„¤ì •)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        rank = int(os.environ.get("RANK", -1))
        world_size = int(os.environ.get("WORLD_SIZE", -1))
        
        # distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        is_distributed = False
        is_main_process = True
        
        # í™˜ê²½ ë³€ìˆ˜ë¡œ distributed ì—¬ë¶€ í™•ì¸
        if rank >= 0 and world_size > 1:
            is_distributed = True
            is_main_process = rank == 0
        elif torch.distributed.is_available():
            # torch.distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            try:
                if torch.distributed.is_initialized():
                    is_distributed = True
                    is_main_process = torch.distributed.get_rank() == 0
            except (AttributeError, RuntimeError, ValueError):
                pass
    except (AttributeError, RuntimeError, ValueError):
        is_distributed = False
        is_main_process = True
    
    # rank ë³€ìˆ˜ ë³´ì¡´ (ë¡œê¹…ìš©)
    try:
        current_rank = rank if 'rank' in locals() and rank >= 0 else (
            torch.distributed.get_rank() if is_distributed and torch.distributed.is_initialized() else 0
        )
    except (AttributeError, RuntimeError, ValueError):
        current_rank = 0
    
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ - ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    load_kwargs = {
        "keep_in_memory": False,  # ë””ìŠ¤í¬ì— ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ë¡œ ìœ ì§€
    }
    if dataset_config:
        load_kwargs["name"] = dataset_config
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
    # ë‹¤ë¥¸ rankë“¤ì€ ìµœì¢… ë³€í™˜ ê²°ê³¼ë§Œ ë¡œë“œ
    if is_distributed:
        # Path import (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
        from pathlib import Path as PathLib
        # ë¨¼ì € ìµœì¢… ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        cache_home = os.environ.get("HF_HOME", os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache/huggingface")))
        cache_hash = hashlib.md5(f"{dataset_name}_{dataset_config}".encode()).hexdigest()[:16]
        dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
        filter_marker_path = PathLib(cache_home) / "datasets" / f".{cache_hash}_filtered.marker"
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ëª¨ë“  rankê°€ ë¡œë“œ (ì¬ì‹œì‘ ì‹œ ì•ˆì „)
        if dataset_save_path.exists() and filter_marker_path.exists():
            logger.info(f"    [Rank {current_rank}] âœ… Using existing processed dataset from: {dataset_save_path}")
            from datasets import Dataset
            import time
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            logger.info(f"    [Rank {current_rank}] Loaded {len(converted):,} samples in {load_time:.1f}s")
            
            # barrier ë™ê¸°í™”
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                pass
            
            # ë³€í™˜ ê²°ê³¼ ë°˜í™˜ (ë‚˜ë¨¸ì§€ ë¡œì§ ê±´ë„ˆë›°ê¸°)
            return converted
        
        # barrierëŠ” distributedê°€ ì™„ì „íˆ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ì‚¬ìš©
        try:
            if torch.distributed.is_initialized():
                # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë™ê¸°í™” ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                torch.distributed.barrier()
        except (RuntimeError, ValueError, AttributeError):
            # barrier ì‹¤íŒ¨ ì‹œ í™˜ê²½ ë³€ìˆ˜ë§Œìœ¼ë¡œ ë™ê¸°í™” (rank 0ë§Œ ë‹¤ìš´ë¡œë“œ)
            pass
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
            logger.info(f"    [Rank 0] Downloading dataset...")
            if dataset_config:
                logger.info(f"    Config: {dataset_config}")
            raw_dataset = load_dataset(dataset_name, **load_kwargs)
            logger.info(f"    [Rank 0] Dataset download completed")
            
            # train split ì‚¬ìš©
            train_data = raw_dataset.get("train", raw_dataset)
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚˜ì¤‘ì— ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì•„ë¬´ê²ƒë„ ì•ˆ í•¨)
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete processing...")
            train_data = None  # ë‚˜ì¤‘ì— ìºì‹œì—ì„œ ë¡œë“œ
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        if dataset_config:
            logger.info(f"    Config: {dataset_config}")
        raw_dataset = load_dataset(dataset_name, **load_kwargs)
        
        # train split ì‚¬ìš©
        train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥í•œ ìµœì í™” íŒŒë¼ë¯¸í„°
    dataset_num_proc = int(os.getenv("DATASET_NUM_PROC", min(8, os.cpu_count() or 2)))
    dataset_batch_size = int(os.getenv("DATASET_BATCH_SIZE", 1000))
    dataset_writer_batch_size = int(os.getenv("DATASET_WRITER_BATCH_SIZE", 10000))
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë³€í™˜í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ìºì‹œë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ì™„ë£Œ ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
        cache_home = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
        config_str = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        cache_hash = hashlib.md5(config_str.encode()).hexdigest()[:16]
        cache_marker = PathLib(cache_home) / "datasets" / f".{cache_hash}_converted.marker"
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë³€í™˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ ë¹ ë¥´ê²Œ)
            logger.info(f"    [Rank 0] Converting dataset with {dataset_num_proc} processes "
                       f"(batch_size={dataset_batch_size}, writer_batch_size={dataset_writer_batch_size})...")
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=dataset_batch_size,
                num_proc=dataset_num_proc,
                remove_columns=train_data.column_names,
                load_from_cache_file=True,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,  # ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš©
                desc=f"Converting {dataset_name}",
            )
            
            # ë³€í™˜ ì™„ë£Œ ë§ˆì»¤ ìƒì„± (filter ì „ì—!)
            cache_marker.parent.mkdir(parents=True, exist_ok=True)
            cache_marker.touch()
            logger.info(f"    [Rank 0] Created conversion marker: {cache_marker}")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ)
            filter_num_proc = min(dataset_num_proc // 2, 4)
            logger.info(f"    [Rank 0] Filtering empty texts with {filter_num_proc} processes...")
            converted = converted.filter(
                lambda x: len(x["text"]) > 0, 
                num_proc=filter_num_proc,  # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,
                load_from_cache_file=True,  # ìºì‹œ í™œìš©
            )
            
            logger.info(f"    [Rank 0] Conversion completed: {len(converted):,} samples")
            
            # ìµœì¢… ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë‹¤ë¥¸ rankë“¤ì´ ì•ˆì „í•˜ê²Œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
            dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
            
            # ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ì†ë„ í–¥ìƒ)
            if dataset_save_path.exists():
                logger.info(f"    [Rank 0] Dataset already saved at: {dataset_save_path}")
            else:
                logger.info(f"    [Rank 0] Saving final dataset to: {dataset_save_path}")
                import time
                save_start = time.time()
                # num_shards ì§€ì •ìœ¼ë¡œ ë³‘ë ¬ ì €ì¥ ìµœì í™”
                converted.save_to_disk(
                    str(dataset_save_path),
                    num_shards=dataset_num_proc,  # ë³‘ë ¬ ì €ì¥
                )
                save_time = time.time() - save_start
                logger.info(f"    [Rank 0] Dataset saved in {save_time:.1f}s")
            
            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ìƒì„±
            filter_marker = PathLib(str(cache_marker).replace("_converted.marker", "_filtered.marker"))
            filter_marker.touch()
            logger.info(f"    [Rank 0] Created filter marker: {filter_marker}")
            
            # ë³€í™˜ ì™„ë£Œ í›„ barrier
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                import time
                time.sleep(1)
                
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” í•„í„° ë§ˆì»¤ ëŒ€ê¸° í›„ ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ!
            import time
            max_wait_time = 3600  # ìµœëŒ€ 1ì‹œê°„ ëŒ€ê¸°
            check_interval = 5
            
            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ëŒ€ê¸° (ë³€í™˜ ë§ˆì»¤ëŠ” ê±´ë„ˆë›°ê³  ë°”ë¡œ í•„í„° ë§ˆì»¤ë§Œ í™•ì¸)
            filter_marker = PathLib(str(cache_marker).replace("_converted.marker", "_filtered.marker"))
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete all processing...")
            waited = 0
            while not filter_marker.exists() and waited < max_wait_time:
                time.sleep(check_interval)
                waited += check_interval
                if waited % 60 == 0:  # 1ë¶„ë§ˆë‹¤ ë¡œê·¸
                    logger.info(f"    [Rank {current_rank}] Still waiting... ({waited}s elapsed)")
            
            if not filter_marker.exists():
                raise TimeoutError(f"Rank {current_rank}: Dataset processing timeout after {max_wait_time}s")
            
            logger.info(f"    [Rank {current_rank}] Processing complete, loading final result from cache...")
            
            # barrier ë™ê¸°í™”
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                time.sleep(2)
            
            # rank 0ì´ ì €ì¥í•œ ìµœì¢… ë°ì´í„°ì…‹ì„ ì§ì ‘ ë¡œë“œ (ìºì‹œ ì¶©ëŒ ì—†ìŒ!)
            dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
            logger.info(f"    [Rank {current_rank}] Loading final dataset from: {dataset_save_path}")
            
            # íŒŒì¼ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì§§ì€ ëŒ€ê¸° (íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™”)
            max_attempts = 60  # ìµœëŒ€ 60ë²ˆ ì‹œë„ (30ì´ˆ)
            for attempt in range(max_attempts):
                if dataset_save_path.exists() and (dataset_save_path / "dataset_info.json").exists():
                    break
                time.sleep(0.5)
            else:
                logger.warning(f"    [Rank {current_rank}] Dataset files not fully ready, proceeding anyway...")
            
            from datasets import Dataset
            import time
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            
            logger.info(f"    [Rank {current_rank}] Loaded from disk in {load_time:.1f}s: {len(converted):,} samples")
            
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        logger.info(f"    Converting dataset with {dataset_num_proc} processes...")
        converted = train_data.map(
            convert_batch,
            batched=True,
            batch_size=dataset_batch_size,
            num_proc=dataset_num_proc,
            remove_columns=train_data.column_names,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
            desc=f"Converting {dataset_name}",
        )
        
        logger.info(f"    Filtering empty texts...")
        filter_num_proc = min(dataset_num_proc // 2, 4)
        converted = converted.filter(
            lambda x: len(x["text"]) > 0, 
            num_proc=filter_num_proc,
            load_from_cache_file=True,  # ìºì‹œ ì‚¬ìš©
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
        )
    
    # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ í”¼í•˜ê³  Datasetì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    logger.info(f"    â†’ {len(converted):,} samples")
    return converted  # Dataset ê°ì²´ ë°˜í™˜


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    from datasets import Dataset, concatenate_datasets
    
    datasets_list = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            datasets_list.append(Dataset.from_list(file_data))
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ (Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ)
            if isinstance(ds_data, Dataset):
                datasets_list.append(ds_data)
            else:
                # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë§Œ ë³€í™˜
                datasets_list.append(Dataset.from_list(ds_data))
    
    if not datasets_list:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    # ì—¬ëŸ¬ Datasetì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©
    if len(datasets_list) == 1:
        combined_dataset = datasets_list[0]
    else:
        logger.info(f"  Concatenating {len(datasets_list)} datasets...")
        combined_dataset = concatenate_datasets(datasets_list)
    
    logger.info(f"  Total: {len(combined_dataset):,} samples")
    
    dataset = {"train": combined_dataset}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Foundation Model pretrainìš©)
    
    íŠ¹ìˆ˜ í† í° ì—†ì´ ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ì´ê²ƒì€ Next Token Predictionì„ ìœ„í•œ Foundation Model í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"instruction": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"messages": [...]}: ëª¨ë“  ë©”ì‹œì§€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"conversations": [...]}: ëª¨ë“  ëŒ€í™” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    """
    # ì•ˆì „í•œ ë¬¸ìì—´ ì¶”ì¶œ í•¨ìˆ˜
    def safe_str(val) -> str:
        return (val or "").strip()
    
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item and item["text"]:
        return item["text"]
    
    # input/output í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "output" in item:
        inp = safe_str(item["input"])
        out = safe_str(item["output"])
        if not out:
            return None
        return f"{inp}\n\n{out}" if inp else out
    
    # instruction/output í¬ë§· (Alpaca) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "instruction" in item and "output" in item:
        inst = safe_str(item["instruction"])
        out = safe_str(item["output"])
        # input í•„ë“œë„ ìˆìœ¼ë©´ í•©ì¹¨
        inp = safe_str(item.get("input"))
        if inp:
            inst = f"{inst}\n{inp}" if inst else inp
        if not inst and not out:
            return None
        return f"{inst}\n\n{out}" if inst else out
    
    # messages í¬ë§· (OpenAI Chat) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "messages" in item and item["messages"]:
        texts = []
        for msg in item["messages"]:
            if msg:
                content = safe_str(msg.get("content"))
                if content:
                    texts.append(content)
        return "\n\n".join(texts) if texts else None
    
    # conversations í¬ë§· (ShareGPT) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "conversations" in item and item["conversations"]:
        texts = []
        for conv in item["conversations"]:
            if conv:
                value = safe_str(conv.get("value"))
                if value:
                    texts.append(value)
        return "\n\n".join(texts) if texts else None
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "content" in item:
        parts = []
        inp = safe_str(item.get("input"))
        reasoning = safe_str(item.get("reasoning_content"))
        content = safe_str(item.get("content"))
        if inp:
            parts.append(inp)
        if reasoning:
            parts.append(reasoning)
        if content:
            parts.append(content)
        return "\n\n".join(parts) if parts else None
    
    # prompt/response í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "response" in item:
        prompt = safe_str(item["prompt"])
        response = safe_str(item["response"])
        if not response:
            return None
        return f"{prompt}\n\n{response}" if prompt else response
    
    # question/answer í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "question" in item and "answer" in item:
        question = safe_str(item["question"])
        answer = safe_str(item["answer"])
        if not answer:
            return None
        return f"{question}\n\n{answer}" if question else answer
    
    # prompt/completion í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "completion" in item:
        prompt = safe_str(item["prompt"])
        completion = safe_str(item["completion"])
        if not completion:
            return None
        return f"{prompt}\n\n{completion}" if prompt else completion
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    import time
    tok_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        use_fast=True,  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì € ê°•ì œ ì‚¬ìš©
    )
    tok_time = time.time() - tok_start
    logger.info(f"âœ“ Tokenizer loaded in {tok_time:.1f}s")

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        logger.info(f"  (This may take 20-30s for 8 GPUs...)")
        model_start = time.time()
        model = MoaiForCausalLM.from_pretrained(pretrained_model, dtype=dtype)
        model_time = time.time() - model_start
        logger.info(f"âœ“ Model loaded in {model_time:.1f}s")
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train_sequential(args):
    """
    ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•™ìŠµ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    ê° ë°ì´í„°ì…‹ì— ëŒ€í•´:
    1. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
    2. í† í°í™” ë° í•™ìŠµ
    3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    4. ë©”ëª¨ë¦¬ í•´ì œ
    5. ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ (ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ resume)
    """
    import gc
    
    # DDP í™˜ê²½ ì •ë³´ ì¶œë ¥
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    if world_size > 1:
        logger.info(f"ğŸŒ Distributed Training: Rank {rank}/{world_size}")
        logger.info(f"â³ Initializing DDP environment... (may take 10-20s)")
    else:
        logger.info(f"ğŸ’» Single GPU Training")
    
    dataset_names = args.dataset if args.dataset else []
    train_files = args.train_file if args.train_file else []
    
    # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
    all_sources = []
    for ds in dataset_names:
        all_sources.append(("hf", ds))
    for f in train_files:
        all_sources.append(("file", f))
    
    logger.info(f"ğŸ“‹ Processing {len(all_sources)} datasets sequentially:")
    for i, (src_type, src_name) in enumerate(all_sources):
        logger.info(f"  {i+1}. [{src_type}] {src_name}")
    
    # í† í¬ë‚˜ì´ì €ëŠ” í•œ ë²ˆë§Œ ë¡œë“œ (ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì¬ì‚¬ìš©)
    logger.info("â³ Loading tokenizer (once for all datasets)...")
    logger.info("   (This may take 5-10s...)")
    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer_path,
        trust_remote_code=True,
        use_fast=True,
    )
    logger.info(f"âœ“ Tokenizer loaded: {args.tokenizer_path}")
    
    current_checkpoint = args.pretrained_model
    
    for idx, (src_type, src_name) in enumerate(all_sources):
        logger.info("="*80)
        logger.info(f"ğŸ”„ [{idx+1}/{len(all_sources)}] Processing: {src_name}")
        logger.info("="*80)
        
        # 1. ëª¨ë¸ ë¡œë“œ (í† í¬ë‚˜ì´ì €ëŠ” ì¬ì‚¬ìš©)
        if idx == 0:
            logger.info(f"ğŸ”„ Loading model from: {current_checkpoint or 'scratch'}")
        else:
            logger.info(f"ğŸ”„ Loading model from previous stage: {current_checkpoint}")
        
        model, _ = setup_model_and_tokenizer(
            tokenizer_path=args.tokenizer_path,
            model_config=args.model_config,
            pretrained_model=current_checkpoint,
            use_flash_attention=args.flash_attention,
            use_compile=args.compile,
            use_bf16=args.bf16,
            use_fp16=args.fp16,
        )
        logger.info(f"âœ“ Model loaded")
        
        # 2. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
        if src_type == "hf":
            dataset, text_column = load_pretrain_dataset(
                dataset_names=[src_name],
                dataset_config=args.dataset_config if idx == 0 else None,
                train_files=None,
                text_column=args.text_column,
            )
        else:
            dataset, text_column = load_pretrain_dataset(
                dataset_names=None,
                dataset_config=None,
                train_files=[src_name],
                text_column=args.text_column,
            )
        
        # 3. í† í°í™” (DDP ìµœì í™”: rank 0ë§Œ í† í¬ë‚˜ì´ì§•, ë‚˜ë¨¸ì§€ëŠ” ë¡œë“œ)
        logger.info("ğŸ”¤ Tokenizing dataset...")
        
        # Fast Tokenizer ê²€ì¦
        if not tokenizer.is_fast:
            logger.warning("âš ï¸ WARNING: Using slow tokenizer! This will be very slow.")
            logger.warning("âš ï¸ Please ensure your tokenizer supports fast mode.")
        else:
            logger.info("âœ… Using Fast Tokenizer (Rust-based)")
        
        # DDP í™˜ê²½ í™•ì¸
        is_distributed = int(os.environ.get("WORLD_SIZE", 1)) > 1
        is_main_process = int(os.environ.get("RANK", 0)) == 0
        current_rank = int(os.environ.get("RANK", 0))
        
        # í† í¬ë‚˜ì´ì§• ìºì‹œ ê²½ë¡œ
        cache_home = os.environ.get("HF_HOME", os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache/huggingface")))
        dataset_hash = hashlib.md5(f"{src_name}_{idx}".encode()).hexdigest()[:16]
        tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
        tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"
        
        # í† í¬ë‚˜ì´ì € ì›Œë°ì—… (JIT ì»´íŒŒì¼ ë° ìºì‹œ ì´ˆê¸°í™”)
        if is_main_process or not is_distributed:
            logger.info("ğŸ”¥ Warming up tokenizer...")
            # ì‘ì€ ìƒ˜í”Œë¡œ ì›Œë°ì—… (JIT ì»´íŒŒì¼ ë° ìºì‹œ ì´ˆê¸°í™”)
            warmup_texts = ["Hello world " * 100] * 10
            _ = tokenizer(warmup_texts, truncation=False, padding=False)
            logger.info("âœ… Tokenizer warmed up")
        
        if args.packing:
            logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode)")
            
            # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
            def batch_tokenize(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=False,
                    padding=False,
                    add_special_tokens=True,
                )
            
            # DDP í™˜ê²½ì—ì„œ rank 0ë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
            if is_distributed:
                if is_main_process:
                    # ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if tokenized_cache_path.exists() and tokenized_marker.exists():
                        logger.info(f"  [Rank 0] âœ… Loading cached tokenized dataset from: {tokenized_cache_path}")
                        from datasets import Dataset as HFDataset
                        tokenized_ds = HFDataset.load_from_disk(str(tokenized_cache_path))
                    else:
                        # Fast TokenizerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬í™”ë˜ë¯€ë¡œ num_proc=1ì´ ìµœì !
                        logger.info(f"  [Rank 0] âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                        tokenized_ds = dataset["train"].map(
                            batch_tokenize,
                            batched=True,
                            batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                            num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                            remove_columns=dataset["train"].column_names,
                            load_from_cache_file=True,
                            writer_batch_size=100000,
                            keep_in_memory=False,
                            desc="Tokenizing",
                        )
                        
                        # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ì €ì¥
                        logger.info(f"  [Rank 0] ğŸ’¾ Saving tokenized dataset to: {tokenized_cache_path}")
                        tokenized_ds.save_to_disk(str(tokenized_cache_path), num_shards=8)
                        tokenized_marker.touch()
                        logger.info(f"  [Rank 0] âœ… Tokenizing completed: {len(tokenized_ds):,} samples")
                    
                    # barrier ë™ê¸°í™”
                    try:
                        if torch.distributed.is_initialized():
                            torch.distributed.barrier()
                    except (RuntimeError, ValueError, AttributeError):
                        import time
                        time.sleep(1)
                else:
                    # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
                    import time
                    max_wait = 7200  # ìµœëŒ€ 2ì‹œê°„
                    waited = 0
                    logger.info(f"  [Rank {current_rank}] Waiting for rank 0 to complete tokenizing...")
                    while not tokenized_marker.exists() and waited < max_wait:
                        time.sleep(5)
                        waited += 5
                        if waited % 60 == 0:
                            logger.info(f"  [Rank {current_rank}] Still waiting... ({waited}s)")
                    
                    if not tokenized_marker.exists():
                        raise TimeoutError(f"Rank {current_rank}: Tokenizing timeout after {max_wait}s")
                    
                    # barrier ë™ê¸°í™”
                    try:
                        if torch.distributed.is_initialized():
                            torch.distributed.barrier()
                    except (RuntimeError, ValueError, AttributeError):
                        time.sleep(2)
                    
                    # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¡œë“œ
                    logger.info(f"  [Rank {current_rank}] ğŸ“¥ Loading tokenized dataset from: {tokenized_cache_path}")
                    from datasets import Dataset as HFDataset
                    load_start = time.time()
                    tokenized_ds = HFDataset.load_from_disk(str(tokenized_cache_path))
                    load_time = time.time() - load_start
                    logger.info(f"  [Rank {current_rank}] âœ… Loaded {len(tokenized_ds):,} samples in {load_time:.1f}s")
            else:
                # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: ì¼ë°˜ í† í¬ë‚˜ì´ì§•
                logger.info(f"  âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                tokenized_ds = dataset["train"].map(
                    batch_tokenize,
                    batched=True,
                    batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                    num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                    remove_columns=dataset["train"].column_names,
                    load_from_cache_file=True,
                    writer_batch_size=100000,
                    keep_in_memory=False,
                    desc="Tokenizing",
                )
            
            # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í™œìš©)
            if psutil:
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                if available_memory_gb > 50:  # 50GB ì´ìƒ ì—¬ìœ ê°€ ìˆìœ¼ë©´
                    logger.info(f"âœ… Sufficient RAM ({available_memory_gb:.1f}GB available), using in-memory processing")
                else:
                    logger.info(f"âš ï¸ Limited RAM ({available_memory_gb:.1f}GB available), using disk-based processing")
            
            tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
            del tokenized_ds
            gc.collect()
            
            concatenated_chunks = concatenate_sequences(
                tokenized_sequences=tokenized_list,
                max_seq_length=args.max_seq_length,
                eos_token_id=tokenizer.eos_token_id,
            )
            
            from datasets import Dataset as HFDataset
            tokenized_dataset = HFDataset.from_list(concatenated_chunks)
            
            # ë©”ëª¨ë¦¬ í•´ì œ
            del tokenized_list
            del concatenated_chunks
            gc.collect()
        else:
            # Non-packing ëª¨ë“œ
            def tokenize_function(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=True,
                    max_length=args.max_seq_length,
                    padding=False,
                    return_special_tokens_mask=True,
                )
            
            # DDP í™˜ê²½ì—ì„œ rank 0ë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
            if is_distributed:
                if is_main_process:
                    # ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if tokenized_cache_path.exists() and tokenized_marker.exists():
                        logger.info(f"  [Rank 0] âœ… Loading cached tokenized dataset from: {tokenized_cache_path}")
                        from datasets import Dataset as HFDataset
                        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                    else:
                        logger.info(f"  [Rank 0] âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                        tokenized_dataset = dataset["train"].map(
                            tokenize_function,
                            batched=True,
                            batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                            num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                            remove_columns=dataset["train"].column_names,
                            load_from_cache_file=True,
                            writer_batch_size=100000,
                            keep_in_memory=False,
                            desc="Tokenizing",
                        )
                        
                        # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ì €ì¥
                        logger.info(f"  [Rank 0] ğŸ’¾ Saving tokenized dataset to: {tokenized_cache_path}")
                        tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=8)
                        tokenized_marker.touch()
                        logger.info(f"  [Rank 0] âœ… Tokenizing completed: {len(tokenized_dataset):,} samples")
                    
                    # barrier ë™ê¸°í™”
                    try:
                        if torch.distributed.is_initialized():
                            torch.distributed.barrier()
                    except (RuntimeError, ValueError, AttributeError):
                        import time
                        time.sleep(1)
                else:
                    # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
                    import time
                    max_wait = 7200  # ìµœëŒ€ 2ì‹œê°„
                    waited = 0
                    logger.info(f"  [Rank {current_rank}] Waiting for rank 0 to complete tokenizing...")
                    while not tokenized_marker.exists() and waited < max_wait:
                        time.sleep(5)
                        waited += 5
                        if waited % 60 == 0:
                            logger.info(f"  [Rank {current_rank}] Still waiting... ({waited}s)")
                    
                    if not tokenized_marker.exists():
                        raise TimeoutError(f"Rank {current_rank}: Tokenizing timeout after {max_wait}s")
                    
                    # barrier ë™ê¸°í™”
                    try:
                        if torch.distributed.is_initialized():
                            torch.distributed.barrier()
                    except (RuntimeError, ValueError, AttributeError):
                        time.sleep(2)
                    
                    # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¡œë“œ
                    logger.info(f"  [Rank {current_rank}] ğŸ“¥ Loading tokenized dataset from: {tokenized_cache_path}")
                    from datasets import Dataset as HFDataset
                    load_start = time.time()
                    tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                    load_time = time.time() - load_start
                    logger.info(f"  [Rank {current_rank}] âœ… Loaded {len(tokenized_dataset):,} samples in {load_time:.1f}s")
            else:
                # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: ì¼ë°˜ í† í¬ë‚˜ì´ì§•
                logger.info(f"  âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                tokenized_dataset = dataset["train"].map(
                    tokenize_function,
                    batched=True,
                    batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                    num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                    remove_columns=dataset["train"].column_names,
                    load_from_cache_file=True,
                    writer_batch_size=100000,
                    keep_in_memory=False,
                    desc="Tokenizing",
                )
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë©”ëª¨ë¦¬ í•´ì œ
        del dataset
        gc.collect()
        
        logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")
        
        # 4. í•™ìŠµ
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê° ë°ì´í„°ì…‹ë³„)
        stage_output_dir = f"{args.output_dir}/stage_{idx+1}"
        
        training_args = TrainingArguments(
            output_dir=stage_output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps if idx == 0 else 100,  # ì²« ë²ˆì§¸ë§Œ full warmup
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            save_total_limit=2,
            bf16=args.bf16,
            fp16=args.fp16,
            gradient_checkpointing=args.gradient_checkpointing,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            report_to="none",
            max_steps=args.max_steps if args.max_steps > 0 else -1,
            # ì¶”ê°€ ìµœì í™” ì˜µì…˜
            dataloader_pin_memory=True,
            dataloader_prefetch_factor=4,
            dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
            optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
            ddp_find_unused_parameters=False,
            tf32=True,
            group_by_length=False,
            max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            processing_class=tokenizer,  # tokenizer deprecated in v5.0
        )
        
        logger.info(f"ğŸƒ Training on dataset {idx+1}/{len(all_sources)}...")
        trainer.train()
        
        # 5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (DDP í™˜ê²½ì—ì„œ ë™ê¸°í™” í•„ìš”)
        checkpoint_path = f"{stage_output_dir}/checkpoint"
        trainer.save_model(checkpoint_path)
        
        # DDP í™˜ê²½ì—ì„œ ëª¨ë“  rankê°€ ì €ì¥ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        
        # ì €ì¥ í™•ì¸ ë° dtype ìœ ì§€ (rank 0ì—ì„œ)
        is_main_process = not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0
        if is_main_process:
            # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ì¬ì €ì¥
            model_dtype = next(model.parameters()).dtype
            if model_dtype in (torch.bfloat16, torch.float16):
                logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format...")
                model.save_pretrained(checkpoint_path, torch_dtype=model_dtype, safe_serialization=True)
            
            saved_files = list(Path(checkpoint_path).glob("*.safetensors")) + \
                         list(Path(checkpoint_path).glob("*.bin"))
            if saved_files:
                logger.info(f"ğŸ’¾ Saved checkpoint to: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸  No model files found in: {checkpoint_path}")
        
        # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path
        
        # 6. ë©”ëª¨ë¦¬ í•´ì œ
        del model
        del tokenizer
        del tokenized_dataset
        del trainer
        gc.collect()
        
        try:
            torch.cuda.empty_cache()
        except:
            pass
        
        # DDP barrier (ë‹¤ìŒ ë‹¨ê³„ ì‹œì‘ ì „ ë™ê¸°í™”)
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        
        logger.info(f"âœ… Completed dataset {idx+1}/{len(all_sources)}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    logger.info("="*80)
    logger.info("ğŸ¯ Sequential training completed!")
    logger.info(f"ğŸ“ Final model: {current_checkpoint}")
    logger.info("="*80)


def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""

    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info("="*80)
    
    # DDP í™˜ê²½ ì •ë³´ ì¶œë ¥
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    if world_size > 1:
        logger.info(f"ğŸŒ Distributed Training: Rank {rank}/{world_size}")
        logger.info(f"â³ Initializing DDP environment... (may take 10-20s)")
    else:
        logger.info(f"ğŸ’» Single GPU Training")

    # Sequential ëª¨ë“œ: ê° ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    if args.sequential and args.dataset and len(args.dataset) > 1:
        logger.info("ğŸ“¦ Sequential mode: Processing datasets one by one")
        train_sequential(args)
        return

    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("â³ Loading model and tokenizer... (may take 20-30s for 8 GPUs)")
    model, tokenizer = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    if args.mode == "pretrain":
        dataset, text_column = load_pretrain_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            dataset_config=args.dataset_config,
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        )

    # 3. í† í°í™” (DDP ìµœì í™”: rank 0ë§Œ í† í¬ë‚˜ì´ì§•, ë‚˜ë¨¸ì§€ëŠ” ë¡œë“œ)
    logger.info("ğŸ”¤ Tokenizing dataset...")
    
    # Fast Tokenizer ê²€ì¦
    if not tokenizer.is_fast:
        logger.warning("âš ï¸ WARNING: Using slow tokenizer! This will be very slow.")
        logger.warning("âš ï¸ Please ensure your tokenizer supports fast mode.")
    else:
        logger.info("âœ… Using Fast Tokenizer (Rust-based)")
    
    # DDP í™˜ê²½ í™•ì¸
    is_distributed = int(os.environ.get("WORLD_SIZE", 1)) > 1
    is_main_process = int(os.environ.get("RANK", 0)) == 0
    current_rank = int(os.environ.get("RANK", 0))
    
    # í† í¬ë‚˜ì´ì§• ìºì‹œ ê²½ë¡œ
    cache_home = os.environ.get("HF_HOME", os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache/huggingface")))
    dataset_names_str = "_".join(args.dataset) if args.dataset else "local"
    dataset_hash = hashlib.md5(dataset_names_str.encode()).hexdigest()[:16]
    tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
    tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

    # í† í¬ë‚˜ì´ì € ì›Œë°ì—… (JIT ì»´íŒŒì¼ ë° ìºì‹œ ì´ˆê¸°í™”)
    if is_main_process or not is_distributed:
        logger.info("ğŸ”¥ Warming up tokenizer...")
        # ì‘ì€ ìƒ˜í”Œë¡œ ì›Œë°ì—… (JIT ì»´íŒŒì¼ ë° ìºì‹œ ì´ˆê¸°í™”)
        warmup_texts = ["Hello world " * 100] * 10
        _ = tokenizer(warmup_texts, truncation=False, padding=False)
        logger.info("âœ… Tokenizer warmed up")

    # Packing ëª¨ë“œ: ì‹œí€€ìŠ¤ ì—°ê²° ë°©ì‹ ì‚¬ìš© (pretrain/sft ë‘˜ ë‹¤ ì§€ì›)
    if args.packing:
        logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode) for {args.mode}")
        
        # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
        def batch_tokenize(examples):
            return tokenizer(
                examples[text_column],
                truncation=False,  # ì—°ê²°í•  ê²ƒì´ë¯€ë¡œ truncation ì•ˆí•¨
                padding=False,
                add_special_tokens=True,
            )
        
        # DDP í™˜ê²½ì—ì„œ rank 0ë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
        if is_distributed:
            if is_main_process:
                # ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if tokenized_cache_path.exists() and tokenized_marker.exists():
                    logger.info(f"  [Rank 0] âœ… Loading cached tokenized dataset from: {tokenized_cache_path}")
                    from datasets import Dataset as HFDataset
                    tokenized_ds = HFDataset.load_from_disk(str(tokenized_cache_path))
                else:
                    logger.info(f"  [Rank 0] âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                    tokenized_ds = dataset["train"].map(
                        batch_tokenize,
                        batched=True,
                        batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                        num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                        remove_columns=dataset["train"].column_names,
                        load_from_cache_file=True,
                        writer_batch_size=100000,
                        keep_in_memory=False,
                        desc="Tokenizing",
                    )
                    
                    # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ì €ì¥
                    logger.info(f"  [Rank 0] ğŸ’¾ Saving tokenized dataset to: {tokenized_cache_path}")
                    tokenized_ds.save_to_disk(str(tokenized_cache_path), num_shards=8)
                    tokenized_marker.touch()
                    logger.info(f"  [Rank 0] âœ… Tokenizing completed: {len(tokenized_ds):,} samples")
                
                # barrier ë™ê¸°í™”
                try:
                    if torch.distributed.is_initialized():
                        torch.distributed.barrier()
                except (RuntimeError, ValueError, AttributeError):
                    import time
                    time.sleep(1)
            else:
                # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
                import time
                max_wait = 7200  # ìµœëŒ€ 2ì‹œê°„
                waited = 0
                logger.info(f"  [Rank {current_rank}] Waiting for rank 0 to complete tokenizing...")
                while not tokenized_marker.exists() and waited < max_wait:
                    time.sleep(5)
                    waited += 5
                    if waited % 60 == 0:
                        logger.info(f"  [Rank {current_rank}] Still waiting... ({waited}s)")
                
                if not tokenized_marker.exists():
                    raise TimeoutError(f"Rank {current_rank}: Tokenizing timeout after {max_wait}s")
                
                # barrier ë™ê¸°í™”
                try:
                    if torch.distributed.is_initialized():
                        torch.distributed.barrier()
                except (RuntimeError, ValueError, AttributeError):
                    time.sleep(2)
                
                # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¡œë“œ
                logger.info(f"  [Rank {current_rank}] ğŸ“¥ Loading tokenized dataset from: {tokenized_cache_path}")
                from datasets import Dataset as HFDataset
                load_start = time.time()
                tokenized_ds = HFDataset.load_from_disk(str(tokenized_cache_path))
                load_time = time.time() - load_start
                logger.info(f"  [Rank {current_rank}] âœ… Loaded {len(tokenized_ds):,} samples in {load_time:.1f}s")
        else:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: ì¼ë°˜ í† í¬ë‚˜ì´ì§•
            logger.info(f"  âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
            tokenized_ds = dataset["train"].map(
                batch_tokenize,
                batched=True,
                batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=True,
                writer_batch_size=100000,
                keep_in_memory=False,
                desc="Tokenizing",
            )
        
        # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìŠ¤ë§ˆíŠ¸ ë©”ëª¨ë¦¬ í™œìš©)
        if psutil:
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
            if available_memory_gb > 50:  # 50GB ì´ìƒ ì—¬ìœ ê°€ ìˆìœ¼ë©´
                logger.info(f"âœ… Sufficient RAM ({available_memory_gb:.1f}GB available), using in-memory processing")
            else:
                logger.info(f"âš ï¸ Limited RAM ({available_memory_gb:.1f}GB available), using disk-based processing")
        
        tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
        del tokenized_ds
        
        # ì‹œí€€ìŠ¤ ì—°ê²° ë° ì²­í‚¹
        concatenated_chunks = concatenate_sequences(
            tokenized_sequences=tokenized_list,
            max_seq_length=args.max_seq_length,
            eos_token_id=tokenizer.eos_token_id,
        )
        
        del tokenized_list
        
        # Datasetìœ¼ë¡œ ë³€í™˜
        from datasets import Dataset as HFDataset
        tokenized_dataset = HFDataset.from_list(concatenated_chunks)
        
        del concatenated_chunks
        
    else:
        # ê¸°ì¡´ ë°©ì‹: ê°œë³„ ìƒ˜í”Œ í† í°í™” with truncation
        def tokenize_function(examples):
            return tokenizer(
                examples[text_column],
                truncation=True,
                max_length=args.max_seq_length,
                padding=False,
                return_special_tokens_mask=True,
            )

        # DDP í™˜ê²½ì—ì„œ rank 0ë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
        if is_distributed:
            if is_main_process:
                # ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if tokenized_cache_path.exists() and tokenized_marker.exists():
                    logger.info(f"  [Rank 0] âœ… Loading cached tokenized dataset from: {tokenized_cache_path}")
                    from datasets import Dataset as HFDataset
                    tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                else:
                    logger.info(f"  [Rank 0] âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
                    tokenized_dataset = dataset["train"].map(
                        tokenize_function,
                        batched=True,
                        batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                        num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                        remove_columns=dataset["train"].column_names,
                        load_from_cache_file=True,
                        writer_batch_size=100000,
                        keep_in_memory=False,
                        desc="Tokenizing",
                    )
                    
                    # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ì €ì¥
                    logger.info(f"  [Rank 0] ğŸ’¾ Saving tokenized dataset to: {tokenized_cache_path}")
                    tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=8)
                    tokenized_marker.touch()
                    logger.info(f"  [Rank 0] âœ… Tokenizing completed: {len(tokenized_dataset):,} samples")
                
                # barrier ë™ê¸°í™”
                try:
                    if torch.distributed.is_initialized():
                        torch.distributed.barrier()
                except (RuntimeError, ValueError, AttributeError):
                    import time
                    time.sleep(1)
            else:
                # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
                import time
                max_wait = 7200  # ìµœëŒ€ 2ì‹œê°„
                waited = 0
                logger.info(f"  [Rank {current_rank}] Waiting for rank 0 to complete tokenizing...")
                while not tokenized_marker.exists() and waited < max_wait:
                    time.sleep(5)
                    waited += 5
                    if waited % 60 == 0:
                        logger.info(f"  [Rank {current_rank}] Still waiting... ({waited}s)")
                
                if not tokenized_marker.exists():
                    raise TimeoutError(f"Rank {current_rank}: Tokenizing timeout after {max_wait}s")
                
                # barrier ë™ê¸°í™”
                try:
                    if torch.distributed.is_initialized():
                        torch.distributed.barrier()
                except (RuntimeError, ValueError, AttributeError):
                    time.sleep(2)
                
                # í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë¡œë“œ
                logger.info(f"  [Rank {current_rank}] ğŸ“¥ Loading tokenized dataset from: {tokenized_cache_path}")
                from datasets import Dataset as HFDataset
                load_start = time.time()
                tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                load_time = time.time() - load_start
                logger.info(f"  [Rank {current_rank}] âœ… Loaded {len(tokenized_dataset):,} samples in {load_time:.1f}s")
        else:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: ì¼ë°˜ í† í¬ë‚˜ì´ì§•
            logger.info(f"  âš¡ Tokenizing with num_proc=1 (Fast Tokenizer internal parallelization)...")
            tokenized_dataset = dataset["train"].map(
                tokenize_function,
                batched=True,
                batch_size=100000,  # Fast TokenizerëŠ” í° ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥
                num_proc=1,  # Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™”ë§Œ ì‚¬ìš©
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=True,
                writer_batch_size=100000,
                keep_in_memory=False,
                desc="Tokenizing",
            )

    logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")

    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments (ìµœì í™” ì˜µì…˜ í¬í•¨)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to="none",
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        # ì¶”ê°€ ìµœì í™” ì˜µì…˜
        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        dataloader_prefetch_factor=4,  # ë¯¸ë¦¬ ë°°ì¹˜ ë¡œë“œ (ì¦ê°€)
        dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
        optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
        ddp_find_unused_parameters=False,  # DDP ìµœì í™”
        tf32=True,  # TF32 ì‚¬ìš© (Ampere GPU)
        group_by_length=False,  # ê¸¸ì´ë³„ ê·¸ë£¹í•‘ ë¹„í™œì„±í™” (packing ì‚¬ìš©ì‹œ)
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Packing: {args.packing}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    if args.resume_from_checkpoint:
        logger.info(f"  Resume from: {args.resume_from_checkpoint}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))
    # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ì €ì¥
    model_dtype = next(model.parameters()).dtype
    if model_dtype in (torch.bfloat16, torch.float16):
        logger.info(f"ğŸ’¾ Saving model in {model_dtype} format...")
        model.save_pretrained(str(final_path), torch_dtype=model_dtype, safe_serialization=True)

    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info("="*80)


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # ëª¨ë“œ
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["pretrain", "sft"],
        help="Training mode: pretrain or sft"
    )

    # ë°ì´í„° (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)
    parser.add_argument(
        "--dataset",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
        help="HuggingFace dataset name(s). Multiple datasets can be specified."
    )
    parser.add_argument("--dataset_config", type=str, help="Dataset config/subset (for single dataset)")
    parser.add_argument(
        "--train_file",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        help="Local train file(s) (txt or json). Multiple files can be specified."
    )
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")

    # ëª¨ë¸
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/",
        help="Tokenizer path"
    )
    parser.add_argument("--model_config", type=str, help="Model config JSON file")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path (for SFT)")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size per device")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=48, help="Number of processes for tokenization (default: 48 for high-performance CPUs)")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    # ì¦‰ì‹œ ë¡œê·¸ ì¶œë ¥ (ì‚¬ìš©ìê°€ í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ ì¤‘ì„ì„ ì•Œ ìˆ˜ ìˆë„ë¡)
    print("="*80)
    print("ğŸš€ MOAI-LLM Training Starting...")
    print("â³ Initializing Python environment and loading libraries...")
    print("="*80)
    import sys
    sys.stdout.flush()  # ë²„í¼ í”ŒëŸ¬ì‹œë¡œ ì¦‰ì‹œ ì¶œë ¥
    
    main()
