"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

# Early initialization
import os
import sys
import time as time_module
from pathlib import Path as PathType

# Check rank early
rank = int(os.environ.get("RANK", 0))
world_size = int(os.environ.get("WORLD_SIZE", 1))
is_main = (rank == 0)

# ë™ê¸°í™” ë§ˆì»¤ íŒŒì¼
import_marker = PathType("/tmp/.moai_import_done")

if is_main:
    # Rank 0: ë¨¼ì € import
    print(f"[IMPORT] Rank 0: Importing modules (world_size={world_size})...", flush=True)
    sys.stdout.flush()
    
    # ì´ì „ ë§ˆì»¤ ì œê±°
    if import_marker.exists():
        import_marker.unlink()
    
    import argparse
    import hashlib
    import time
    import gc
    import logging
    from pathlib import Path
    from typing import Optional, Dict, Any
    
    try:
        import orjson as json
    except ImportError:
        import json
    
    try:
        import psutil
    except ImportError:
        psutil = None
    
    print(f"[IMPORT] Rank 0: Importing torch...", flush=True)
    sys.stdout.flush()
    import torch
    
    print(f"[IMPORT] Rank 0: Importing transformers...", flush=True)
    sys.stdout.flush()
    from transformers import (
        AutoTokenizer,
        Trainer,
        TrainingArguments,
        DataCollatorForLanguageModeling,
    )
    
    print(f"[IMPORT] Rank 0: Importing datasets...", flush=True)
    sys.stdout.flush()
    from datasets import load_dataset, disable_caching
    import datasets
    datasets.config.IN_MEMORY_MAX_SIZE = 0
    
    print(f"[IMPORT] Rank 0: Importing moai_llm...", flush=True)
    sys.stdout.flush()
    from moai_llm.config import MoaiConfig
    from moai_llm.modeling.model import MoaiForCausalLM
    
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
    logger = logging.getLogger(__name__)
    
    # ë§ˆì»¤ ìƒì„± (ë‹¤ë¥¸ rankë“¤ì´ import ì‹œì‘ ê°€ëŠ¥)
    import_marker.touch()
    print(f"[IMPORT] Rank 0: âœ… All modules imported!", flush=True)
    sys.stdout.flush()
else:
    # ë‹¤ë¥¸ rankë“¤: ë§ˆì»¤ ëŒ€ê¸°
    print(f"[IMPORT] Rank {rank}: Waiting for rank 0...", flush=True)
    sys.stdout.flush()
    
    max_wait = 300  # 5ë¶„
    waited = 0
    while not import_marker.exists() and waited < max_wait:
        time_module.sleep(0.5)
        waited += 0.5
    
    if not import_marker.exists():
        print(f"[IMPORT] Rank {rank}: Timeout waiting for rank 0!", flush=True)
        sys.exit(1)
    
    # ì´ì œ ì•ˆì „í•˜ê²Œ import
    import argparse
    import hashlib
    import time
    import gc
    import logging
    from pathlib import Path
    from typing import Optional, Dict, Any
    
    try:
        import orjson as json
    except ImportError:
        import json
    
    try:
        import psutil
    except ImportError:
        psutil = None
    
    import torch
    from transformers import (
        AutoTokenizer,
        Trainer,
        TrainingArguments,
        DataCollatorForLanguageModeling,
    )
    from datasets import load_dataset, disable_caching
    import datasets
    datasets.config.IN_MEMORY_MAX_SIZE = 0
    from moai_llm.config import MoaiConfig
    from moai_llm.modeling.model import MoaiForCausalLM
    
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
    logger = logging.getLogger(__name__)
    
    print(f"[IMPORT] Rank {rank}: âœ… Modules imported!", flush=True)
    sys.stdout.flush()


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    
    ì´ë ‡ê²Œ í•˜ë©´ max_seq_lengthë¡œ ì˜ë¦¬ë”ë¼ë„ ë‹¤ìŒ ì²­í¬ì—ì„œ 
    ì´ì–´ì„œ EOSê¹Œì§€ ì˜¨ì „íˆ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID
    
    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np
    
    # 1. ì´ ê¸¸ì´ ê³„ì‚° (ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹ìš©)
    total_len = 0
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        total_len += len(input_ids)
        if len(input_ids) > 0 and input_ids[-1] != eos_token_id:
            total_len += 1  # EOS ì¶”ê°€ë  ì˜ˆì •
    
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences into ~{total_len:,} tokens")
    
    # 2. numpy ë°°ì—´ë¡œ ë¹ ë¥´ê²Œ ì—°ê²° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    all_tokens = np.empty(total_len, dtype=np.int32)
    offset = 0
    
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)
        
        if seq_len == 0:
            continue
            
        # ë°°ì—´ì— ë³µì‚¬
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len
        
        # EOS ì¶”ê°€
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1
    
    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]
    
    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í•  (list comprehensionìœ¼ë¡œ ë¹ ë¥´ê²Œ)
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []
    
    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk = all_tokens[start:end].tolist()
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ (< 128) ë²„ë¦¼
        if len(chunk) < 128:
            logger.info(f"  Dropping short final chunk of {len(chunk)} tokens")
            continue
            
        chunks.append({
            "input_ids": chunk,
            "attention_mask": [1] * len(chunk),
        })
    
    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    
    return chunks


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None):
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"
    
    DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    # DDP í™˜ê²½ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© - Trainer ì´ˆê¸°í™” ì „ì—ë„ ì‘ë™)
    try:
        # í™˜ê²½ ë³€ìˆ˜ë¡œ rank í™•ì¸ (torchrunì´ ì„¤ì •)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        rank = int(os.environ.get("RANK", -1))
        world_size = int(os.environ.get("WORLD_SIZE", -1))
        
        # distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        is_distributed = False
        is_main_process = True
        
        # í™˜ê²½ ë³€ìˆ˜ë¡œ distributed ì—¬ë¶€ í™•ì¸
        if rank >= 0 and world_size > 1:
            is_distributed = True
            is_main_process = rank == 0
        elif torch.distributed.is_available():
            # torch.distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            try:
                if torch.distributed.is_initialized():
                    is_distributed = True
                    is_main_process = torch.distributed.get_rank() == 0
            except (AttributeError, RuntimeError, ValueError):
                pass
    except (AttributeError, RuntimeError, ValueError):
        is_distributed = False
        is_main_process = True
    
    # rank ë³€ìˆ˜ ë³´ì¡´ (ë¡œê¹…ìš©)
    try:
        current_rank = rank if 'rank' in locals() and rank >= 0 else (
            torch.distributed.get_rank() if is_distributed and torch.distributed.is_initialized() else 0
        )
    except (AttributeError, RuntimeError, ValueError):
        current_rank = 0
    
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ - ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    load_kwargs = {
        "keep_in_memory": False,  # ë””ìŠ¤í¬ì— ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ë¡œ ìœ ì§€
    }
    if dataset_config:
        load_kwargs["name"] = dataset_config
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
    # ë‹¤ë¥¸ rankë“¤ì€ ìµœì¢… ë³€í™˜ ê²°ê³¼ë§Œ ë¡œë“œ
    if is_distributed:
        # Path import (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
        from pathlib import Path as PathLib
        # ë¨¼ì € ìµœì¢… ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        cache_home = os.environ.get("HF_HOME", os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache/huggingface")))
        cache_hash = hashlib.md5(f"{dataset_name}_{dataset_config}".encode()).hexdigest()[:16]
        dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
        filter_marker_path = PathLib(cache_home) / "datasets" / f".{cache_hash}_filtered.marker"
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ëª¨ë“  rankê°€ ë¡œë“œ (ì¬ì‹œì‘ ì‹œ ì•ˆì „)
        if dataset_save_path.exists() and filter_marker_path.exists():
            logger.info(f"    [Rank {current_rank}] âœ… Using existing processed dataset from: {dataset_save_path}")
            from datasets import Dataset
            import time
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            logger.info(f"    [Rank {current_rank}] Loaded {len(converted):,} samples in {load_time:.1f}s")
            
            # barrier ë™ê¸°í™”
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                pass
            
            # ë³€í™˜ ê²°ê³¼ ë°˜í™˜ (ë‚˜ë¨¸ì§€ ë¡œì§ ê±´ë„ˆë›°ê¸°)
            return converted
        
        # barrierëŠ” distributedê°€ ì™„ì „íˆ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ì‚¬ìš©
        try:
            if torch.distributed.is_initialized():
                # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë™ê¸°í™” ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                torch.distributed.barrier()
        except (RuntimeError, ValueError, AttributeError):
            # barrier ì‹¤íŒ¨ ì‹œ í™˜ê²½ ë³€ìˆ˜ë§Œìœ¼ë¡œ ë™ê¸°í™” (rank 0ë§Œ ë‹¤ìš´ë¡œë“œ)
            pass
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
            logger.info(f"    [Rank 0] Downloading dataset...")
            if dataset_config:
                logger.info(f"    Config: {dataset_config}")
            raw_dataset = load_dataset(dataset_name, **load_kwargs)
            logger.info(f"    [Rank 0] Dataset download completed")
            
            # train split ì‚¬ìš©
            train_data = raw_dataset.get("train", raw_dataset)
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚˜ì¤‘ì— ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì•„ë¬´ê²ƒë„ ì•ˆ í•¨)
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete processing...")
            train_data = None  # ë‚˜ì¤‘ì— ìºì‹œì—ì„œ ë¡œë“œ
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        if dataset_config:
            logger.info(f"    Config: {dataset_config}")
        raw_dataset = load_dataset(dataset_name, **load_kwargs)
        
        # train split ì‚¬ìš©
        train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥í•œ ìµœì í™” íŒŒë¼ë¯¸í„°
    # ë†’ì€ num_proc = ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ í† í¬ë‚˜ì´ì € ì‹¤í–‰ â†’ ë¹ ë¦„!
    dataset_num_proc = int(os.getenv("DATASET_NUM_PROC", min(48, os.cpu_count() or 8)))
    dataset_batch_size = int(os.getenv("DATASET_BATCH_SIZE", 1000))
    dataset_writer_batch_size = int(os.getenv("DATASET_WRITER_BATCH_SIZE", 10000))
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë³€í™˜í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ìºì‹œë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ì™„ë£Œ ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
        cache_home = os.getenv("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
        config_str = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        cache_hash = hashlib.md5(config_str.encode()).hexdigest()[:16]
        cache_marker = PathLib(cache_home) / "datasets" / f".{cache_hash}_converted.marker"
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë³€í™˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ ë¹ ë¥´ê²Œ)
            logger.info(f"    [Rank 0] Converting dataset with {dataset_num_proc} processes "
                       f"(batch_size={dataset_batch_size}, writer_batch_size={dataset_writer_batch_size})...")
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=dataset_batch_size,
                num_proc=dataset_num_proc,
                remove_columns=train_data.column_names,
                load_from_cache_file=True,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,  # ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš©
                desc=f"Converting {dataset_name}",
            )
            
            # ë³€í™˜ ì™„ë£Œ ë§ˆì»¤ ìƒì„± (filter ì „ì—!)
            cache_marker.parent.mkdir(parents=True, exist_ok=True)
            cache_marker.touch()
            logger.info(f"    [Rank 0] Created conversion marker: {cache_marker}")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ)
            filter_num_proc = min(dataset_num_proc // 2, 4)
            logger.info(f"    [Rank 0] Filtering empty texts with {filter_num_proc} processes...")
            converted = converted.filter(
                lambda x: len(x["text"]) > 0, 
                num_proc=filter_num_proc,  # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,
                load_from_cache_file=True,  # ìºì‹œ í™œìš©
            )
            
            logger.info(f"    [Rank 0] Conversion completed: {len(converted):,} samples")
            
            # ìµœì¢… ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë‹¤ë¥¸ rankë“¤ì´ ì•ˆì „í•˜ê²Œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
            dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
            
            # ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ì†ë„ í–¥ìƒ)
            if dataset_save_path.exists():
                logger.info(f"    [Rank 0] Dataset already saved at: {dataset_save_path}")
            else:
                logger.info(f"    [Rank 0] Saving final dataset to: {dataset_save_path}")
                import time
                save_start = time.time()
                # num_shards ì§€ì •ìœ¼ë¡œ ë³‘ë ¬ ì €ì¥ ìµœì í™”
                converted.save_to_disk(
                    str(dataset_save_path),
                    num_shards=dataset_num_proc,  # ë³‘ë ¬ ì €ì¥
                )
                save_time = time.time() - save_start
                logger.info(f"    [Rank 0] Dataset saved in {save_time:.1f}s")
            
            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ìƒì„±
            filter_marker = PathLib(str(cache_marker).replace("_converted.marker", "_filtered.marker"))
            filter_marker.touch()
            logger.info(f"    [Rank 0] Created filter marker: {filter_marker}")
            
            # ë³€í™˜ ì™„ë£Œ í›„ barrier
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                import time
                time.sleep(1)
                
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” í•„í„° ë§ˆì»¤ ëŒ€ê¸° í›„ ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ!
            import time
            max_wait_time = 3600  # ìµœëŒ€ 1ì‹œê°„ ëŒ€ê¸°
            check_interval = 5
            
            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ëŒ€ê¸° (ë³€í™˜ ë§ˆì»¤ëŠ” ê±´ë„ˆë›°ê³  ë°”ë¡œ í•„í„° ë§ˆì»¤ë§Œ í™•ì¸)
            filter_marker = PathLib(str(cache_marker).replace("_converted.marker", "_filtered.marker"))
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete all processing...")
            waited = 0
            while not filter_marker.exists() and waited < max_wait_time:
                time.sleep(check_interval)
                waited += check_interval
                if waited % 60 == 0:  # 1ë¶„ë§ˆë‹¤ ë¡œê·¸
                    logger.info(f"    [Rank {current_rank}] Still waiting... ({waited}s elapsed)")
            
            if not filter_marker.exists():
                raise TimeoutError(f"Rank {current_rank}: Dataset processing timeout after {max_wait_time}s")
            
            logger.info(f"    [Rank {current_rank}] Processing complete, loading final result from cache...")
            
            # barrier ë™ê¸°í™”
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                time.sleep(2)
            
            # rank 0ì´ ì €ì¥í•œ ìµœì¢… ë°ì´í„°ì…‹ì„ ì§ì ‘ ë¡œë“œ (ìºì‹œ ì¶©ëŒ ì—†ìŒ!)
            dataset_save_path = PathLib(cache_home) / "datasets" / f"{cache_hash}_final"
            logger.info(f"    [Rank {current_rank}] Loading final dataset from: {dataset_save_path}")
            
            # íŒŒì¼ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì§§ì€ ëŒ€ê¸° (íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™”)
            max_attempts = 60  # ìµœëŒ€ 60ë²ˆ ì‹œë„ (30ì´ˆ)
            for attempt in range(max_attempts):
                if dataset_save_path.exists() and (dataset_save_path / "dataset_info.json").exists():
                    break
                time.sleep(0.5)
            else:
                logger.warning(f"    [Rank {current_rank}] Dataset files not fully ready, proceeding anyway...")
            
            from datasets import Dataset
            import time
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            
            logger.info(f"    [Rank {current_rank}] Loaded from disk in {load_time:.1f}s: {len(converted):,} samples")
            
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        logger.info(f"    Converting dataset with {dataset_num_proc} processes...")
        converted = train_data.map(
            convert_batch,
            batched=True,
            batch_size=dataset_batch_size,
            num_proc=dataset_num_proc,
            remove_columns=train_data.column_names,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
            desc=f"Converting {dataset_name}",
        )
        
        logger.info(f"    Filtering empty texts...")
        filter_num_proc = min(dataset_num_proc // 2, 4)
        converted = converted.filter(
            lambda x: len(x["text"]) > 0, 
            num_proc=filter_num_proc,
            load_from_cache_file=True,  # ìºì‹œ ì‚¬ìš©
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
        )
    
    # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ í”¼í•˜ê³  Datasetì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    logger.info(f"    â†’ {len(converted):,} samples")
    return converted  # Dataset ê°ì²´ ë°˜í™˜


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    from datasets import Dataset, concatenate_datasets
    
    datasets_list = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            datasets_list.append(Dataset.from_list(file_data))
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ (Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ)
            if isinstance(ds_data, Dataset):
                datasets_list.append(ds_data)
            else:
                # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë§Œ ë³€í™˜
                datasets_list.append(Dataset.from_list(ds_data))
    
    if not datasets_list:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    # ì—¬ëŸ¬ Datasetì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©
    if len(datasets_list) == 1:
        combined_dataset = datasets_list[0]
    else:
        logger.info(f"  Concatenating {len(datasets_list)} datasets...")
        combined_dataset = concatenate_datasets(datasets_list)
    
    logger.info(f"  Total: {len(combined_dataset):,} samples")
    
    dataset = {"train": combined_dataset}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Foundation Model pretrainìš©)
    
    íŠ¹ìˆ˜ í† í° ì—†ì´ ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ì´ê²ƒì€ Next Token Predictionì„ ìœ„í•œ Foundation Model í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"instruction": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"messages": [...]}: ëª¨ë“  ë©”ì‹œì§€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"conversations": [...]}: ëª¨ë“  ëŒ€í™” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    """
    # ì•ˆì „í•œ ë¬¸ìì—´ ì¶”ì¶œ í•¨ìˆ˜
    def safe_str(val) -> str:
        return (val or "").strip()
    
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item and item["text"]:
        return item["text"]
    
    # input/output í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "output" in item:
        inp = safe_str(item["input"])
        out = safe_str(item["output"])
        if not out:
            return None
        return f"{inp}\n\n{out}" if inp else out
    
    # instruction/output í¬ë§· (Alpaca) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "instruction" in item and "output" in item:
        inst = safe_str(item["instruction"])
        out = safe_str(item["output"])
        # input í•„ë“œë„ ìˆìœ¼ë©´ í•©ì¹¨
        inp = safe_str(item.get("input"))
        if inp:
            inst = f"{inst}\n{inp}" if inst else inp
        if not inst and not out:
            return None
        return f"{inst}\n\n{out}" if inst else out
    
    # messages í¬ë§· (OpenAI Chat) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "messages" in item and item["messages"]:
        texts = []
        for msg in item["messages"]:
            if msg:
                content = safe_str(msg.get("content"))
                if content:
                    texts.append(content)
        return "\n\n".join(texts) if texts else None
    
    # conversations í¬ë§· (ShareGPT) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "conversations" in item and item["conversations"]:
        texts = []
        for conv in item["conversations"]:
            if conv:
                value = safe_str(conv.get("value"))
                if value:
                    texts.append(value)
        return "\n\n".join(texts) if texts else None
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "content" in item:
        parts = []
        inp = safe_str(item.get("input"))
        reasoning = safe_str(item.get("reasoning_content"))
        content = safe_str(item.get("content"))
        if inp:
            parts.append(inp)
        if reasoning:
            parts.append(reasoning)
        if content:
            parts.append(content)
        return "\n\n".join(parts) if parts else None
    
    # prompt/response í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "response" in item:
        prompt = safe_str(item["prompt"])
        response = safe_str(item["response"])
        if not response:
            return None
        return f"{prompt}\n\n{response}" if prompt else response
    
    # question/answer í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "question" in item and "answer" in item:
        question = safe_str(item["question"])
        answer = safe_str(item["answer"])
        if not answer:
            return None
        return f"{question}\n\n{answer}" if question else answer
    
    # prompt/completion í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "completion" in item:
        prompt = safe_str(item["prompt"])
        completion = safe_str(item["completion"])
        if not completion:
            return None
        return f"{prompt}\n\n{completion}" if prompt else completion
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    import time
    tok_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        use_fast=True,  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì € ê°•ì œ ì‚¬ìš©
    )
    tok_time = time.time() - tok_start
    logger.info(f"âœ“ Tokenizer loaded in {tok_time:.1f}s")

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        logger.info(f"  (This may take 20-30s for 8 GPUs...)")
        model_start = time.time()
        model = MoaiForCausalLM.from_pretrained(pretrained_model, dtype=dtype)
        model_time = time.time() - model_start
        logger.info(f"âœ“ Model loaded in {model_time:.1f}s")
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train_sequential(args):
    """
    ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•™ìŠµ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    âš¡ ìµœì í™”ëœ ìˆœì„œ:
    1. ëª¨ë“  ë°ì´í„°ì…‹ì„ ë¨¼ì € í† í°í™” (DDP ì „, multiprocessing ì‚¬ìš©!)
    2. DDP ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
    3. ê° ë°ì´í„°ì…‹ìœ¼ë¡œ ìˆœì°¨ í•™ìŠµ (ì´ë¯¸ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©)
    4. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë° ë©”ëª¨ë¦¬ í•´ì œ
    """
    import gc
    import sys
    
    # DDP í™˜ê²½ ì •ë³´
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    is_distributed = world_size > 1
    is_main_process = rank == 0
    
    if is_main_process:
        logger.info(f"ğŸŒ Environment: {world_size} GPU(s), Sequential Mode")
        logger.info(f"âš¡ Strategy: Pre-tokenize all datasets, then train sequentially")
    sys.stdout.flush()
    
    # ========================================================================
    # STEP 0: ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    # ========================================================================
    dataset_names = args.dataset if args.dataset else []
    train_files = args.train_file if args.train_file else []
    
    all_sources = []
    for ds in dataset_names:
        all_sources.append(("hf", ds))
    for f in train_files:
        all_sources.append(("file", f))
    
    if is_main_process:
        logger.info(f"ğŸ“‹ Sequential Mode: Processing {len(all_sources)} datasets")
        for i, (src_type, src_name) in enumerate(all_sources):
            logger.info(f"  {i+1}. [{src_type}] {src_name}")
    sys.stdout.flush()
    
    # ========================================================================
    # STEP 1: í† í¬ë‚˜ì´ì € ë¡œë“œ (DDP ì „!)
    # ========================================================================
    if is_main_process:
        logger.info("ğŸ“ [Rank 0] Loading tokenizer...")
    sys.stdout.flush()
    
    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer_path,
        trust_remote_code=True,
        use_fast=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    if is_main_process:
        if not tokenizer.is_fast:
            logger.warning("âš ï¸ WARNING: Using slow tokenizer!")
        else:
            logger.info("âœ… [Rank 0] Using Fast Tokenizer (Rust-based)")
    sys.stdout.flush()
    
    # ========================================================================
    # STEP 2: ëª¨ë“  ë°ì´í„°ì…‹ì„ ë¨¼ì € í† í°í™” (DDP ì „! Rank 0ë§Œ ì‹¤í–‰!)
    # ========================================================================
    tokenized_datasets_info = []  # ê° ë°ì´í„°ì…‹ì˜ ì •ë³´ ì €ì¥
    
    # âš¡ Rank 0ë§Œ í† í°í™” ìˆ˜í–‰, ë‹¤ë¥¸ RankëŠ” ì™„ì „íˆ ëŒ€ê¸°
    if is_main_process:
        logger.info("="*80)
        logger.info("âš¡ STEP 2: Pre-tokenizing all datasets (Rank 0 only, before DDP)")
        logger.info("="*80)
        sys.stdout.flush()
        
        for idx, (src_type, src_name) in enumerate(all_sources):
            logger.info(f"")
            logger.info(f"ğŸ“¦ [{idx+1}/{len(all_sources)}] Dataset: {src_name}")
            sys.stdout.flush()
            
            # ë°ì´í„°ì…‹ ë¡œë“œ
            logger.info(f"  Loading dataset...")
            
            if src_type == "hf":
                dataset, text_column = load_pretrain_dataset(
                    dataset_names=[src_name],
                    dataset_config=args.dataset_config if idx == 0 else None,
                    train_files=None,
                    text_column=args.text_column,
                )
            else:
                dataset, text_column = load_pretrain_dataset(
                    dataset_names=None,
                    dataset_config=None,
                    train_files=[src_name],
                    text_column=args.text_column,
                )
            
            # í† í°í™” ìºì‹œ ê²½ë¡œ
            cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
            dataset_hash = hashlib.md5(f"{src_name}_seq_{idx}".encode()).hexdigest()[:16]
            tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
            tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"
            
            from datasets import Dataset as HFDataset
            
            if tokenized_cache_path.exists() and tokenized_marker.exists():
                # ìºì‹œê°€ ìˆìœ¼ë©´ ë¡œë“œë§Œ
                logger.info(f"  âœ… Loading cached tokenized dataset")
                tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                logger.info(f"  âœ… Loaded {len(tokenized_dataset):,} samples")
            else:
                # ìºì‹œê°€ ì—†ìœ¼ë©´ í† í°í™”
                logger.info(f"  ğŸ”¤ Tokenizing with BATCH ITERATOR...")
                
                if args.packing:
                    import time
                    
                    train_data = dataset["train"]
                    total_samples = len(train_data)
                    batch_size = 50000  # 5ë§Œ ê°œì”© ë°°ì¹˜
                    
                    logger.info(f"  âš¡ Batch Iterator Tokenization")
                    logger.info(f"     Total samples: {total_samples:,}")
                    logger.info(f"     Batch size: {batch_size:,}")
                    sys.stdout.flush()
                    
                    all_input_ids = []
                    start_time = time.time()
                    samples_done = 0
                    
                    # iter()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¹ ë¥´ê²Œ ìˆœíšŒ
                    for batch in train_data.iter(batch_size=batch_size):
                        texts = batch[text_column]
                        
                        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§• (Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬ ì²˜ë¦¬)
                        tokenized = tokenizer(
                            texts,
                            truncation=False,
                            padding=False,
                            add_special_tokens=True,
                        )
                        
                        all_input_ids.extend(tokenized["input_ids"])
                        samples_done += len(texts)
                        
                        # ì§„í–‰ë¥  ì¶œë ¥ (10ë§Œ ê°œë§ˆë‹¤)
                        if samples_done % 100000 == 0 or samples_done == total_samples:
                            elapsed = time.time() - start_time
                            samples_per_sec = samples_done / elapsed if elapsed > 0 else 0
                            eta = (total_samples - samples_done) / samples_per_sec if samples_per_sec > 0 else 0
                            
                            logger.info(f"  ğŸ“¦ Progress: {samples_done:,}/{total_samples:,} "
                                       f"({100*samples_done/total_samples:.1f}%) "
                                       f"[{samples_per_sec:.0f} samples/s, ETA: {eta/60:.1f}min]")
                            sys.stdout.flush()
                    
                    total_time = time.time() - start_time
                    logger.info(f"  âœ… Tokenization completed in {total_time/60:.1f} minutes")
                    logger.info(f"     Average speed: {total_samples/total_time:.0f} samples/s")
                    sys.stdout.flush()
                    
                    # Packing
                    logger.info(f"  ğŸ“¦ Packing sequences...")
                    sys.stdout.flush()
                    tokenized_list = [{"input_ids": ids} for ids in all_input_ids]
                    del all_input_ids
                    gc.collect()
                    
                    concatenated_chunks = concatenate_sequences(
                        tokenized_sequences=tokenized_list,
                        max_seq_length=args.max_seq_length,
                        eos_token_id=tokenizer.eos_token_id,
                    )
                    del tokenized_list
                    gc.collect()
                    
                    tokenized_dataset = HFDataset.from_list(concatenated_chunks)
                    del concatenated_chunks
                    gc.collect()
                else:
                    def tokenize_function(examples):
                        return tokenizer(
                            examples[text_column],
                            truncation=True,
                            max_length=args.max_seq_length,
                            padding=False,
                            return_special_tokens_mask=True,
                        )
                    
                    # âš¡ ìµœì í™”: num_proc=1 + Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™” (ê°€ì¥ ë¹ ë¦„!)
                    os.environ["TOKENIZERS_PARALLELISM"] = "true"  # Fast Tokenizer ë³‘ë ¬í™” í™œì„±í™”
                    import multiprocessing
                    cpu_count = multiprocessing.cpu_count()
                    
                    # ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³„ì‚°
                    optimal_num_proc = int(os.getenv("DATASET_NUM_PROC", min(48, multiprocessing.cpu_count())))
                    
                    logger.info(f"  âš¡ Parallel Tokenization: {optimal_num_proc} processes ({cpu_count} CPUs)")
                    logger.info(f"     Strategy: Each process runs tokenizer independently â†’ FAST!")
                    
                    tokenized_dataset = dataset["train"].map(
                        tokenize_function,
                        batched=True,
                        batch_size=5000,
                        num_proc=optimal_num_proc,  # âš¡ 48ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì‹¤í–‰!
                        remove_columns=dataset["train"].column_names,
                        load_from_cache_file=False,
                        writer_batch_size=100000,
                        keep_in_memory=False,
                        desc=f"Tokenizing {src_name} (num_proc={optimal_num_proc})",
                    )
                
                # ìºì‹œ ì €ì¥
                logger.info(f"  ğŸ’¾ Saving tokenized dataset...")
                tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=8)
                tokenized_marker.touch()
                logger.info(f"  âœ… Tokenized: {len(tokenized_dataset):,} samples")
            
            # ì •ë³´ ì €ì¥
            tokenized_datasets_info.append({
                'name': src_name,
                'cache_path': tokenized_cache_path,
                'num_samples': len(tokenized_dataset),
            })
            
            # ë©”ëª¨ë¦¬ í•´ì œ
            del dataset
            del tokenized_dataset
            gc.collect()
        
        logger.info("="*80)
        logger.info("âœ… All datasets pre-tokenized!")
        logger.info("="*80)
        sys.stdout.flush()
    else:
        # ë‹¤ë¥¸ Rankë“¤ì€ Rank 0ì´ ëª¨ë“  í† í°í™”ë¥¼ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°
        logger.info(f"[Rank {rank}] Waiting for rank 0 to complete all tokenization...")
        sys.stdout.flush()
        
        # ë§ˆì§€ë§‰ ë°ì´í„°ì…‹ì˜ ë§ˆì»¤ë¥¼ ê¸°ë‹¤ë¦¼
        import time as time_module
        cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
        last_src_name = all_sources[-1][1]
        last_idx = len(all_sources) - 1
        dataset_hash = hashlib.md5(f"{last_src_name}_seq_{last_idx}".encode()).hexdigest()[:16]
        last_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"
        
        max_wait = 7200
        waited = 0
        while not last_marker.exists() and waited < max_wait:
            time_module.sleep(10)
            waited += 10
            if waited % 60 == 0:
                logger.info(f"[Rank {rank}] Still waiting... ({waited}s)")
        
        if not last_marker.exists():
            raise TimeoutError(f"Rank {rank}: Tokenizing timeout after {max_wait}s")
        
        logger.info(f"[Rank {rank}] âœ… Rank 0 completed tokenization! Loading datasets...")
        sys.stdout.flush()
        
        # í† í°í™”ëœ ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ
        from datasets import Dataset as HFDataset
        for idx, (src_type, src_name) in enumerate(all_sources):
            cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))
            dataset_hash = hashlib.md5(f"{src_name}_seq_{idx}".encode()).hexdigest()[:16]
            tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
            
            tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
            
            tokenized_datasets_info.append({
                'name': src_name,
                'cache_path': tokenized_cache_path,
                'num_samples': len(tokenized_dataset),
            })
            
            del tokenized_dataset
            gc.collect()
        
        logger.info(f"[Rank {rank}] âœ… All dataset info loaded!")
        sys.stdout.flush()
    
    # ========================================================================
    # Barrier: ëª¨ë“  Rank ë™ê¸°í™” (í† í°í™” ì™„ë£Œ í›„)
    # ========================================================================
    if is_distributed:
        import torch.distributed as dist
        if dist.is_initialized():
            logger.info(f"[Rank {rank}] Synchronizing with other ranks...")
            sys.stdout.flush()
            dist.barrier()
            logger.info(f"[Rank {rank}] âœ… All ranks synchronized!")
            sys.stdout.flush()
    
    # ========================================================================
    # Tokenize-only ëª¨ë“œ: ì—¬ê¸°ì„œ ì¢…ë£Œ
    # ========================================================================
    if hasattr(args, '_tokenize_only') and args._tokenize_only:
        logger.info("="*80)
        logger.info("âœ… Tokenization completed! Exiting (tokenize-only mode)")
        logger.info("="*80)
        return
    
    # ========================================================================
    # STEP 3: W&B ì´ˆê¸°í™” (ì„ íƒì )
    # ========================================================================
    if args.use_wandb:
        try:
            import wandb
            if is_main_process:
                wandb.init(
                    project=args.wandb_project,
                    name=args.wandb_run_name,
                    config=vars(args),
                )
                logger.info(f"ğŸ“Š W&B initialized: {args.wandb_project}")
        except ImportError:
            logger.warning("âš ï¸ wandb not installed, falling back to tensorboard")
            args.use_wandb = False
    
    # ========================================================================
    # STEP 4: ìˆœì°¨ í•™ìŠµ (ê° í† í°í™”ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ)
    # ========================================================================
    if is_main_process:
        logger.info("="*80)
        logger.info("ğŸ¯ STEP 4: Sequential Training")
        logger.info("="*80)
    sys.stdout.flush()
    
    current_checkpoint = args.pretrained_model
    
    for idx, dataset_info in enumerate(tokenized_datasets_info):
        if is_main_process:
            logger.info("")
            logger.info("="*80)
            logger.info(f"ğŸš€ Training [{idx+1}/{len(tokenized_datasets_info)}]: {dataset_info['name']}")
            logger.info(f"   Samples: {dataset_info['num_samples']:,}")
            logger.info("="*80)
        sys.stdout.flush()
        
        # ëª¨ë¸ ë¡œë“œ (í† í¬ë‚˜ì´ì €ëŠ” ì¬ì‚¬ìš©)
        if is_main_process:
            if idx == 0:
                logger.info(f"â³ Loading model from: {current_checkpoint or 'scratch'}")
            else:
                logger.info(f"â³ Resuming from: {current_checkpoint}")
        
        model, _ = setup_model_and_tokenizer(
            tokenizer_path=args.tokenizer_path,
            model_config=args.model_config,
            pretrained_model=current_checkpoint,
            use_flash_attention=args.flash_attention,
            use_compile=args.compile,
            use_bf16=args.bf16,
            use_fp16=args.fp16,
        )
        if is_main_process:
            logger.info(f"âœ“ Model loaded")
        
        # í† í°í™”ëœ ë°ì´í„°ì…‹ ë¡œë“œ
        if is_main_process:
            logger.info(f"ğŸ“¥ Loading tokenized dataset...")
        
        from datasets import Dataset as HFDataset
        tokenized_dataset = HFDataset.load_from_disk(str(dataset_info['cache_path']))
        
        if is_main_process:
            logger.info(f"âœ“ Loaded {len(tokenized_dataset):,} samples")
        
        # Data Collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # Training Arguments
        stage_output_dir = f"{args.output_dir}/stage_{idx+1}"
        
        training_args = TrainingArguments(
            output_dir=stage_output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps if idx == 0 else 100,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            save_total_limit=2,
            bf16=args.bf16,
            fp16=args.fp16,
            gradient_checkpointing=args.gradient_checkpointing,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            report_to=["wandb"] if args.use_wandb else ["tensorboard"],
            max_steps=args.max_steps if args.max_steps > 0 else -1,
            save_safetensors=True,
            dataloader_pin_memory=True,
            dataloader_prefetch_factor=4,
            dataloader_drop_last=True,
            optim="adamw_torch_fused",
            ddp_find_unused_parameters=False,
            tf32=True,
            group_by_length=False,
            max_grad_norm=1.0,
            gradient_checkpointing_kwargs={"use_reentrant": False} if args.gradient_checkpointing else None,
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        if is_main_process:
            logger.info(f"ğŸƒ Starting training...")
        trainer.train()
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = f"{stage_output_dir}/checkpoint"
        trainer.save_model(checkpoint_path)
        
        # DDP barrier
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        
        # dtype ìœ ì§€í•˜ë©° ì €ì¥ (rank 0ë§Œ)
        if is_main_process:
            model_dtype = next(model.parameters()).dtype
            if model_dtype in (torch.bfloat16, torch.float16):
                logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format...")
                model.save_pretrained(checkpoint_path, torch_dtype=model_dtype, safe_serialization=True)
            logger.info(f"âœ… Stage {idx+1} completed: {checkpoint_path}")
        
        # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path
        
        # ë©”ëª¨ë¦¬ í•´ì œ
        del model
        del tokenized_dataset
        del trainer
        gc.collect()
        
        try:
            torch.cuda.empty_cache()
        except:
            pass
        
        # DDP barrier
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
    
    # ìµœì¢… ì™„ë£Œ
    if is_main_process:
        logger.info("="*80)
        logger.info("ğŸ‰ Sequential training completed!")
        logger.info(f"ğŸ“ Final model: {current_checkpoint}")
        logger.info("="*80)


# ============================================================================
# Main Train Function (Concatenated Mode)
# ============================================================================
def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    import sys
    sys.stdout.flush()  # ì¦‰ì‹œ ì¶œë ¥
    
    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info("="*80)
    sys.stdout.flush()
    
    # W&B ì´ˆê¸°í™” (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
    if args.use_wandb:
        try:
            import wandb
            # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ì´ˆê¸°í™”
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            rank = int(os.environ.get("RANK", 0))
            if rank == 0:
                wandb.init(
                    project=args.wandb_project,
                    name=args.wandb_run_name,
                    config=vars(args),
                )
                logger.info(f"ğŸ“Š W&B initialized: {args.wandb_project}")
        except ImportError:
            logger.warning("âš ï¸ wandb not installed, falling back to tensorboard")
            args.use_wandb = False
    
    # Sequential ëª¨ë“œ: ê° ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    if args.sequential and args.dataset and len(args.dataset) > 1:
        logger.info("ğŸ“¦ Sequential mode: Processing datasets one by one")
        train_sequential(args)
        return

    # ============================================================================
    # DDP í™˜ê²½ í™•ì¸ (STEP 0 ì „ì— ë¨¼ì € í™•ì¸)
    # ============================================================================
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_distributed = world_size > 1
    is_main_process = rank == 0
    
    if is_main_process:
        logger.info(f"ğŸŒ Environment: {world_size} GPU(s), Rank {rank}")
    sys.stdout.flush()

    # ============================================================================
    # STEP 0: í† í¬ë‚˜ì´ì €ë§Œ ë¨¼ì € ë¡œë“œ (DDP ì „!)
    # ============================================================================
    if is_main_process:
        logger.info("ğŸ“ [Rank 0] Loading tokenizer (before DDP)...")
    sys.stdout.flush()
    
    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer_path,
        use_fast=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    if is_main_process:
        if not tokenizer.is_fast:
            logger.warning("âš ï¸ WARNING: Using slow tokenizer! This will be very slow.")
        else:
            logger.info("âœ… [Rank 0] Using Fast Tokenizer (Rust-based)")
    sys.stdout.flush()

    # ============================================================================
    # STEP 1: ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í¬ë‚˜ì´ì§• (DDP ì „! multiprocessing ì‚¬ìš© ê°€ëŠ¥!)
    # ============================================================================
    # DDP í™˜ê²½ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_distributed = world_size > 1
    is_main_process = rank == 0
    
    if is_main_process:
        logger.info("ğŸ“š [Rank 0] Loading datasets (may take 2-5 minutes for large datasets)...")
        logger.info("âš¡ Rank 0 will process data, others will load from cache!")
        sys.stdout.flush()
    else:
        logger.info(f"ğŸ“š [Rank {rank}] Waiting for rank 0 to complete data processing...")
        sys.stdout.flush()
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    import time
    load_start = time.time()
    
    if args.mode == "pretrain":
        if is_main_process:
            logger.info(f"[Rank 0] Loading {len(args.dataset) if args.dataset else 0} datasets...")
        dataset, text_column = load_pretrain_dataset(
            dataset_names=args.dataset,
            dataset_config=args.dataset_config,
            train_files=args.train_file,
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_names=args.dataset,
            train_files=args.train_file,
        )
    
    load_time = time.time() - load_start
    if is_main_process:
        logger.info(f"âœ… [Rank 0] Dataset loaded in {load_time:.1f}s: {len(dataset['train']):,} samples")
    else:
        logger.info(f"âœ… [Rank {rank}] Dataset loaded in {load_time:.1f}s: {len(dataset['train']):,} samples")

    # í† í¬ë‚˜ì´ì§• ìºì‹œ ê²½ë¡œ
    cache_home = os.environ.get("HF_HOME", os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache/huggingface")))
    dataset_names_str = "_".join(args.dataset) if args.dataset else "local"
    dataset_hash = hashlib.md5(dataset_names_str.encode()).hexdigest()[:16]
    tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
    tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

    # Rank 0ë§Œ í† í°í™” ìˆ˜í–‰
    if is_main_process:
        logger.info("ğŸ”¤ [Rank 0] Tokenizing dataset...")
        
        # í† í¬ë‚˜ì´ì € ì›Œë°ì—…
        logger.info("ğŸ”¥ Warming up tokenizer...")
        warmup_texts = ["Hello world " * 100] * 10
        _ = tokenizer(warmup_texts, truncation=False, padding=False)
        logger.info("âœ… Tokenizer warmed up")

    # ìºì‹œ í™•ì¸ ë° ë¡œë“œ
    from datasets import Dataset as HFDataset
    
    if tokenized_cache_path.exists() and tokenized_marker.exists():
        # ìºì‹œê°€ ìˆìœ¼ë©´ ëª¨ë“  rankê°€ ë¡œë“œ
        if is_main_process:
            logger.info(f"âœ… [Rank 0] Loading cached tokenized dataset from: {tokenized_cache_path}")
        else:
            logger.info(f"âœ… [Rank {rank}] Loading cached tokenized dataset from: {tokenized_cache_path}")
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        if is_main_process:
            logger.info(f"âœ… [Rank 0] Loaded {len(tokenized_dataset):,} samples from cache")
        else:
            logger.info(f"âœ… [Rank {rank}] Loaded {len(tokenized_dataset):,} samples from cache")
    elif is_main_process:
        # Packing ëª¨ë“œ: ì‹œí€€ìŠ¤ ì—°ê²° ë°©ì‹ ì‚¬ìš©
        if args.packing:
            logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode)")
            
            # ë°°ì¹˜ í† í°í™”
            def batch_tokenize(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=False,
                    padding=False,
                    add_special_tokens=True,
                )
            
            # Multiprocessing ì‚¬ìš© (DDP ì „ì´ë¯€ë¡œ ììœ ë¡­ê²Œ!)
            os.environ["TOKENIZERS_PARALLELISM"] = "false"  # datasetsê°€ multiprocessing ì‹œ ê°•ì œ
            import multiprocessing
            cpu_count = multiprocessing.cpu_count()
            optimal_num_proc = min(32, max(16, cpu_count // 6))
            
            logger.info(f"âš¡ Multiprocessing tokenization: {optimal_num_proc} processes")
            logger.info(f"   CPU cores: {cpu_count}, batch_size=50000")
            logger.info(f"   Expected time: {len(dataset['train']) / (optimal_num_proc * 7000) / 60:.1f} minutes")
            
            tokenized_ds = dataset["train"].map(
                batch_tokenize,
                batched=True,
                batch_size=50000,
                num_proc=optimal_num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=False,
                writer_batch_size=100000,
                keep_in_memory=False,
                desc="Tokenizing",
            )
            
            logger.info("ğŸ“¦ Packing sequences...")
            tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
            del tokenized_ds
            
            concatenated_chunks = concatenate_sequences(
                tokenized_sequences=tokenized_list,
                max_seq_length=args.max_seq_length,
                eos_token_id=tokenizer.eos_token_id,
            )
            del tokenized_list
            
            from datasets import Dataset as HFDataset
            tokenized_dataset = HFDataset.from_list(concatenated_chunks)
            del concatenated_chunks
            
        else:
            # ì¼ë°˜ mode: truncation
            def tokenize_function(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=True,
                    max_length=args.max_seq_length,
                    padding=False,
                    return_special_tokens_mask=True,
                )
            
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            import multiprocessing
            cpu_count = multiprocessing.cpu_count()
            optimal_num_proc = min(32, max(16, cpu_count // 6))
            
            logger.info(f"âš¡ Multiprocessing tokenization: {optimal_num_proc} processes")
            logger.info(f"   CPU cores: {cpu_count}, batch_size=50000")
            
            tokenized_dataset = dataset["train"].map(
                tokenize_function,
                batched=True,
                batch_size=50000,
                num_proc=optimal_num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=False,
                writer_batch_size=100000,
                keep_in_memory=False,
                desc="Tokenizing",
            )
        
        # ìºì‹œ ì €ì¥ (rank 0ë§Œ)
        logger.info(f"ğŸ’¾ [Rank 0] Saving tokenized dataset to: {tokenized_cache_path}")
        tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=8)
        tokenized_marker.touch()
        logger.info(f"âœ… [Rank 0] Tokenized and saved: {len(tokenized_dataset):,} samples")
    else:
        # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
        import time
        max_wait = 7200  # ìµœëŒ€ 2ì‹œê°„
        waited = 0
        while not tokenized_marker.exists() and waited < max_wait:
            time.sleep(5)
            waited += 5
            if waited % 60 == 0:
                logger.info(f"  [Rank {rank}] Still waiting for rank 0... ({waited}s)")
        
        if not tokenized_marker.exists():
            raise TimeoutError(f"Rank {rank}: Tokenizing timeout after {max_wait}s")
        
        logger.info(f"ğŸ“¥ [Rank {rank}] Loading tokenized dataset from: {tokenized_cache_path}")
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        logger.info(f"âœ… [Rank {rank}] Loaded {len(tokenized_dataset):,} samples")
    
    if is_main_process:
        logger.info(f"âœ… Tokenization complete: {len(tokenized_dataset):,} samples ready for training")
    else:
        logger.info(f"âœ… [Rank {rank}] Ready for training with {len(tokenized_dataset):,} samples")
    
    # ============================================================================
    # STEP 2: DDP ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
    # ============================================================================
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    if world_size > 1:
        logger.info(f"ğŸŒ Distributed Training: Rank {rank}/{world_size}")
        logger.info(f"â³ Initializing DDP environment...")
    else:
        logger.info(f"ğŸ’» Single GPU Training")
    
    logger.info("â³ Loading model...")
    model, _ = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )
    
    # ============================================================================
    # STEP 3: í•™ìŠµ ì‹œì‘
    # ============================================================================
    logger.info("ğŸš€ Starting training with pre-tokenized data...")
    
    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments (ìµœì í™” ì˜µì…˜ í¬í•¨)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to=["wandb"] if args.use_wandb else ["tensorboard"],
        save_safetensors=True,
        ddp_find_unused_parameters=False,
        # ì¶”ê°€ ìµœì í™” ì˜µì…˜
        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        dataloader_prefetch_factor=4,  # ë¯¸ë¦¬ ë°°ì¹˜ ë¡œë“œ
        dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
        optim="adamw_torch_fused",  # Fused Adam (faster)
        tf32=True,  # TF32 ì‚¬ìš© (Ampere GPU)
        group_by_length=False,  # ê¸¸ì´ë³„ ê·¸ë£¹í•‘ ë¹„í™œì„±í™” (packing ì‚¬ìš©ì‹œ)
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        gradient_checkpointing_kwargs={"use_reentrant": False} if args.gradient_checkpointing else None,
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Packing: {args.packing}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    logger.info(f"  Logging: {'wandb' if args.use_wandb else 'tensorboard'}")
    if args.resume_from_checkpoint:
        logger.info(f"  Resume from: {args.resume_from_checkpoint}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving final model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))
    
    # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ëª…ì‹œì  ì €ì¥
    model_dtype = next(model.parameters()).dtype
    if model_dtype in (torch.bfloat16, torch.float16):
        logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format for compatibility...")
        model.save_pretrained(str(final_path), torch_dtype=model_dtype, safe_serialization=True)
    
    tokenizer.save_pretrained(str(final_path))
    
    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info(f"ğŸ“Š Model dtype: {model_dtype}")
    logger.info("="*80)


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # í•„ìˆ˜ ì¸ì
    parser.add_argument("--mode", type=str, required=True, choices=["pretrain", "sft"], help="Training mode")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Path to tokenizer")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for model checkpoints")

    # ë°ì´í„°ì…‹ (ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë˜ëŠ” ì—¬ëŸ¬ íŒŒì¼ ì§€ì›)
    parser.add_argument("--dataset", type=str, nargs="*", help="HuggingFace dataset names")
    parser.add_argument("--dataset_config", type=str, default=None, help="Dataset configuration name")
    parser.add_argument("--train_file", type=str, nargs="*", help="Training data files (JSONL, JSON, Parquet, etc.)")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name for dataset")

    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_config", type=str, help="Model config file (JSON)")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )
    
    # Tokenize only ëª¨ë“œ (DDP ì „ì— í† í°í™”ë§Œ ìˆ˜í–‰)
    parser.add_argument(
        "--tokenize_only",
        action="store_true",
        help="Only tokenize datasets and exit (no training). "
             "Use this to pre-tokenize before running torchrun."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=48, help="Number of processes for tokenization (default: 48 for high-performance CPUs)")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )
    
    # Logging ì˜µì…˜
    parser.add_argument(
        "--use_wandb",
        action="store_true",
        help="Use Weights & Biases for logging (default: tensorboard)"
    )
    parser.add_argument(
        "--wandb_project",
        type=str,
        default="moai-llm",
        help="W&B project name (default: moai-llm)"
    )
    parser.add_argument(
        "--wandb_run_name",
        type=str,
        default=None,
        help="W&B run name (default: auto-generated)"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # Tokenize only ëª¨ë“œ: DDP ì—†ì´ í† í°í™”ë§Œ ìˆ˜í–‰
    if args.tokenize_only:
        print("="*80)
        print("ğŸ”¥ Tokenize-Only Mode: Pre-tokenizing datasets (no DDP)")
        print("="*80)
        
        # Sequentialì´ í•„ìš”
        if not args.sequential:
            args.sequential = True
            print("âš¡ Automatically enabling --sequential mode for tokenization")
        
        # í•µì‹¬: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ Fast Tokenizer ì‚¬ìš©
        # num_proc=1 â†’ datasetsê°€ TOKENIZERS_PARALLELISM=false ì„¤ì • ì•ˆí•¨
        # TOKENIZERS_PARALLELISM=true â†’ Fast Tokenizer ë‚´ë¶€ ë³‘ë ¬í™” í™œì„±í™”
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
        os.environ["RAYON_NUM_THREADS"] = str(os.cpu_count() or 96)
        os.environ["DATASET_NUM_PROC"] = "1"  # í•µì‹¬! ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë³€í™˜
        print(f"âš¡ DATASET_NUM_PROC=1 + TOKENIZERS_PARALLELISM=true + RAYON_NUM_THREADS={os.cpu_count()}")
        
        # train_sequential í˜¸ì¶œ (í† í°í™” ë¶€ë¶„ë§Œ ì‹¤í–‰ë¨)
        print("ğŸš€ Calling train_sequential for tokenization...")
        
        # DDP í™˜ê²½ ë³€ìˆ˜ ì œê±° (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰)
        os.environ.pop("RANK", None)
        os.environ.pop("WORLD_SIZE", None)
        os.environ.pop("LOCAL_RANK", None)
        os.environ.pop("MASTER_ADDR", None)
        os.environ.pop("MASTER_PORT", None)
        
        # tokenizationë§Œ ìˆ˜í–‰í•˜ê³  trainingì€ ìŠ¤í‚µí•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
        args._tokenize_only = True
        
        train_sequential(args)
        
        print("="*80)
        print("âœ… Tokenization completed! Now run torchrun for training.")
        print("="*80)
        return

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    # ê°€ì¥ ë¨¼ì € ì¶œë ¥ (torchrunì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œëŒ€ë¡œ ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸)
    import sys
    import os
    
    # ì¦‰ì‹œ ì¶œë ¥
    rank = int(os.environ.get("RANK", -1))
    print(f"[INIT] Rank {rank}: Python script started!", flush=True)
    sys.stdout.flush()
    
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    if rank == 0:
        print("="*80, flush=True)
        print("ğŸš€ MOAI-LLM Training Starting...", flush=True)
        print(f"ğŸŒ World size: {world_size} GPUs", flush=True)
        print("="*80, flush=True)
    
    sys.stdout.flush()
    main()

