"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

import argparse
import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
try:
    import orjson as json  # Rust-based, 10-50x faster
except ImportError:
    import json  # Fallback to standard json

import torch
from transformers import (
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset, disable_caching
import datasets

# Enable memory-efficient settings for large datasets
datasets.config.IN_MEMORY_MAX_SIZE = 0  # Force memory mapping (no in-memory)

from moai_llm.config import MoaiConfig
from moai_llm.modeling.model import MoaiForCausalLM

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    
    ì´ë ‡ê²Œ í•˜ë©´ max_seq_lengthë¡œ ì˜ë¦¬ë”ë¼ë„ ë‹¤ìŒ ì²­í¬ì—ì„œ 
    ì´ì–´ì„œ EOSê¹Œì§€ ì˜¨ì „íˆ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID
    
    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np
    
    # 1. ì´ ê¸¸ì´ ê³„ì‚° (ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹ìš©)
    total_len = 0
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        total_len += len(input_ids)
        if len(input_ids) > 0 and input_ids[-1] != eos_token_id:
            total_len += 1  # EOS ì¶”ê°€ë  ì˜ˆì •
    
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences into ~{total_len:,} tokens")
    
    # 2. numpy ë°°ì—´ë¡œ ë¹ ë¥´ê²Œ ì—°ê²° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    all_tokens = np.empty(total_len, dtype=np.int32)
    offset = 0
    
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)
        
        if seq_len == 0:
            continue
            
        # ë°°ì—´ì— ë³µì‚¬
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len
        
        # EOS ì¶”ê°€
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1
    
    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]
    
    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í•  (list comprehensionìœ¼ë¡œ ë¹ ë¥´ê²Œ)
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []
    
    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk = all_tokens[start:end].tolist()
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ (< 128) ë²„ë¦¼
        if len(chunk) < 128:
            logger.info(f"  Dropping short final chunk of {len(chunk)} tokens")
            continue
            
        chunks.append({
            "input_ids": chunk,
            "attention_mask": [1] * len(chunk),
        })
    
    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    
    return chunks


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None):
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"
    
    DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    # DDP í™˜ê²½ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© - Trainer ì´ˆê¸°í™” ì „ì—ë„ ì‘ë™)
    try:
        # í™˜ê²½ ë³€ìˆ˜ë¡œ rank í™•ì¸ (torchrunì´ ì„¤ì •)
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        rank = int(os.environ.get("RANK", -1))
        world_size = int(os.environ.get("WORLD_SIZE", -1))
        
        # distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        is_distributed = False
        is_main_process = True
        
        # í™˜ê²½ ë³€ìˆ˜ë¡œ distributed ì—¬ë¶€ í™•ì¸
        if rank >= 0 and world_size > 1:
            is_distributed = True
            is_main_process = rank == 0
        elif torch.distributed.is_available():
            # torch.distributedê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            try:
                if torch.distributed.is_initialized():
                    is_distributed = True
                    is_main_process = torch.distributed.get_rank() == 0
            except (AttributeError, RuntimeError, ValueError):
                pass
    except (AttributeError, RuntimeError, ValueError):
        is_distributed = False
        is_main_process = True
    
    # rank ë³€ìˆ˜ ë³´ì¡´ (ë¡œê¹…ìš©)
    try:
        current_rank = rank if 'rank' in locals() and rank >= 0 else (
            torch.distributed.get_rank() if is_distributed and torch.distributed.is_initialized() else 0
        )
    except (AttributeError, RuntimeError, ValueError):
        current_rank = 0
    
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
    if is_distributed:
        # barrierëŠ” distributedê°€ ì™„ì „íˆ ì´ˆê¸°í™”ëœ í›„ì—ë§Œ ì‚¬ìš©
        try:
            if torch.distributed.is_initialized():
                # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë™ê¸°í™” ì§€ì ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°
                torch.distributed.barrier()
        except (RuntimeError, ValueError, AttributeError):
            # barrier ì‹¤íŒ¨ ì‹œ í™˜ê²½ ë³€ìˆ˜ë§Œìœ¼ë¡œ ë™ê¸°í™” (rank 0ë§Œ ë‹¤ìš´ë¡œë“œ)
            pass
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
            logger.info(f"    [Rank 0] Downloading dataset...")
            if dataset_config:
                logger.info(f"    Config: {dataset_config}")
                raw_dataset = load_dataset(dataset_name, dataset_config)
            else:
                raw_dataset = load_dataset(dataset_name)
            logger.info(f"    [Rank 0] Dataset download completed")
            
            # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ barrier (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                pass
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” rank 0ì´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°
            logger.info(f"    [Rank {current_rank}] Waiting for dataset download...")
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                # barrier ì‹¤íŒ¨ ì‹œ ì§§ì€ ëŒ€ê¸° í›„ ì§„í–‰ (rank 0ì´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œí–ˆì„ ê²ƒìœ¼ë¡œ ê°€ì •)
                import time
                time.sleep(2)
            
            # barrier í†µê³¼ í›„ ìºì‹œëœ ë°ì´í„°ì…‹ ë¡œë“œ (ë‹¤ìš´ë¡œë“œ ì—†ì´)
            logger.info(f"    [Rank {current_rank}] Loading cached dataset...")
            if dataset_config:
                raw_dataset = load_dataset(dataset_name, dataset_config, download_mode="reuse_cache_if_exists")
            else:
                raw_dataset = load_dataset(dataset_name, download_mode="reuse_cache_if_exists")
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        if dataset_config:
            logger.info(f"    Config: {dataset_config}")
            raw_dataset = load_dataset(dataset_name, dataset_config)
        else:
            raw_dataset = load_dataset(dataset_name)
    
    # train split ì‚¬ìš©
    train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë³€í™˜í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•˜ì—¬ ìºì‹œ íŒŒì¼ ì¶©ëŒ ë°©ì§€
    if is_distributed:
        # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë³€í™˜ ì‹œì‘ ì „ì— ë™ê¸°í™”
        try:
            if torch.distributed.is_initialized():
                torch.distributed.barrier()
        except (RuntimeError, ValueError, AttributeError):
            pass
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë³€í™˜ (ìºì‹œ íŒŒì¼ ì“°ê¸°)
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ìºì‹œ íŒŒì¼ ì¶©ëŒ ë°©ì§€
            logger.info(f"    [Rank 0] Converting dataset...")
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=500,
                num_proc=1,  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë³€í™˜ (ìºì‹œ íŒŒì¼ ì¶©ëŒ ë°©ì§€)
                remove_columns=train_data.column_names,
                load_from_cache_file=True,
                desc=f"Converting {dataset_name}",
            )
            logger.info(f"    [Rank 0] Dataset conversion completed")
            
            # ë³€í™˜ ì™„ë£Œ í›„ barrier (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì§„í–‰í•˜ë„ë¡)
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                import time
                time.sleep(1)
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” rank 0ì´ ë³€í™˜ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°
            logger.info(f"    [Rank {current_rank}] Waiting for dataset conversion...")
            try:
                if torch.distributed.is_initialized():
                    torch.distributed.barrier()
            except (RuntimeError, ValueError, AttributeError):
                import time
                # rank 0ì´ ë³€í™˜ì„ ì™„ë£Œí•  ì¶©ë¶„í•œ ì‹œê°„ ëŒ€ê¸°
                time.sleep(10)
            
            # barrier í†µê³¼ í›„ ìºì‹œëœ ë°ì´í„°ì…‹ ë¡œë“œ (ë³€í™˜ ì—†ì´, ì½ê¸° ì „ìš©)
            logger.info(f"    [Rank {current_rank}] Loading cached converted dataset...")
            # ìºì‹œê°€ ì™„ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì§§ì€ ëŒ€ê¸°
            import time
            time.sleep(2)
            
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=500,
                num_proc=1,  # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ìºì‹œ ì½ê¸°ë§Œ
                remove_columns=train_data.column_names,
                load_from_cache_file=True,  # ìºì‹œì—ì„œë§Œ ë¡œë“œ
                desc=f"Loading cached {dataset_name}",
            )
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        converted = train_data.map(
            convert_batch,
            batched=True,
            batch_size=500,
            num_proc=min(4, os.cpu_count() or 2),
            remove_columns=train_data.column_names,
            load_from_cache_file=True,
            desc=f"Converting {dataset_name}",
        )
    
    # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë³€ê²½ - ë²„í¼ ê³µê°„ ë¶€ì¡± ë°©ì§€)
    # ë³‘ë ¬ í•„í„°ë§ì€ í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹  ë²„í¼ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©
    # TOKENIZERS_PARALLELISM=trueê°€ ì„¤ì •ë˜ì–´ ìˆì–´ Rust ë ˆë²¨ì—ì„œ ë³‘ë ¬í™”ë¨
    converted = converted.filter(lambda x: len(x["text"]) > 0, num_proc=1)
    
    # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ í”¼í•˜ê³  Datasetì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    logger.info(f"    â†’ {len(converted):,} samples")
    return converted  # Dataset ê°ì²´ ë°˜í™˜


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    from datasets import Dataset, concatenate_datasets
    
    datasets_list = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            datasets_list.append(Dataset.from_list(file_data))
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ (Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ)
            if isinstance(ds_data, Dataset):
                datasets_list.append(ds_data)
            else:
                # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë§Œ ë³€í™˜
                datasets_list.append(Dataset.from_list(ds_data))
    
    if not datasets_list:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    # ì—¬ëŸ¬ Datasetì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©
    if len(datasets_list) == 1:
        combined_dataset = datasets_list[0]
    else:
        logger.info(f"  Concatenating {len(datasets_list)} datasets...")
        combined_dataset = concatenate_datasets(datasets_list)
    
    logger.info(f"  Total: {len(combined_dataset):,} samples")
    
    dataset = {"train": combined_dataset}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Foundation Model pretrainìš©)
    
    íŠ¹ìˆ˜ í† í° ì—†ì´ ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ì´ê²ƒì€ Next Token Predictionì„ ìœ„í•œ Foundation Model í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"instruction": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"messages": [...]}: ëª¨ë“  ë©”ì‹œì§€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"conversations": [...]}: ëª¨ë“  ëŒ€í™” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    """
    # ì•ˆì „í•œ ë¬¸ìì—´ ì¶”ì¶œ í•¨ìˆ˜
    def safe_str(val) -> str:
        return (val or "").strip()
    
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item and item["text"]:
        return item["text"]
    
    # input/output í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "output" in item:
        inp = safe_str(item["input"])
        out = safe_str(item["output"])
        if not out:
            return None
        return f"{inp}\n\n{out}" if inp else out
    
    # instruction/output í¬ë§· (Alpaca) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "instruction" in item and "output" in item:
        inst = safe_str(item["instruction"])
        out = safe_str(item["output"])
        # input í•„ë“œë„ ìˆìœ¼ë©´ í•©ì¹¨
        inp = safe_str(item.get("input"))
        if inp:
            inst = f"{inst}\n{inp}" if inst else inp
        if not inst and not out:
            return None
        return f"{inst}\n\n{out}" if inst else out
    
    # messages í¬ë§· (OpenAI Chat) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "messages" in item and item["messages"]:
        texts = []
        for msg in item["messages"]:
            if msg:
                content = safe_str(msg.get("content"))
                if content:
                    texts.append(content)
        return "\n\n".join(texts) if texts else None
    
    # conversations í¬ë§· (ShareGPT) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "conversations" in item and item["conversations"]:
        texts = []
        for conv in item["conversations"]:
            if conv:
                value = safe_str(conv.get("value"))
                if value:
                    texts.append(value)
        return "\n\n".join(texts) if texts else None
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "content" in item:
        parts = []
        inp = safe_str(item.get("input"))
        reasoning = safe_str(item.get("reasoning_content"))
        content = safe_str(item.get("content"))
        if inp:
            parts.append(inp)
        if reasoning:
            parts.append(reasoning)
        if content:
            parts.append(content)
        return "\n\n".join(parts) if parts else None
    
    # prompt/response í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "response" in item:
        prompt = safe_str(item["prompt"])
        response = safe_str(item["response"])
        if not response:
            return None
        return f"{prompt}\n\n{response}" if prompt else response
    
    # question/answer í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "question" in item and "answer" in item:
        question = safe_str(item["question"])
        answer = safe_str(item["answer"])
        if not answer:
            return None
        return f"{question}\n\n{answer}" if question else answer
    
    # prompt/completion í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "completion" in item:
        prompt = safe_str(item["prompt"])
        completion = safe_str(item["completion"])
        if not completion:
            return None
        return f"{prompt}\n\n{completion}" if prompt else completion
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        model = MoaiForCausalLM.from_pretrained(pretrained_model, dtype=dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train_sequential(args):
    """
    ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•™ìŠµ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    ê° ë°ì´í„°ì…‹ì— ëŒ€í•´:
    1. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
    2. í† í°í™” ë° í•™ìŠµ
    3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    4. ë©”ëª¨ë¦¬ í•´ì œ
    5. ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ (ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ resume)
    """
    import gc
    
    dataset_names = args.dataset if args.dataset else []
    train_files = args.train_file if args.train_file else []
    
    # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
    all_sources = []
    for ds in dataset_names:
        all_sources.append(("hf", ds))
    for f in train_files:
        all_sources.append(("file", f))
    
    logger.info(f"ğŸ“‹ Processing {len(all_sources)} datasets sequentially:")
    for i, (src_type, src_name) in enumerate(all_sources):
        logger.info(f"  {i+1}. [{src_type}] {src_name}")
    
    current_checkpoint = args.pretrained_model
    
    for idx, (src_type, src_name) in enumerate(all_sources):
        logger.info("="*80)
        logger.info(f"ğŸ”„ [{idx+1}/{len(all_sources)}] Processing: {src_name}")
        logger.info("="*80)
        
        # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = setup_model_and_tokenizer(
            tokenizer_path=args.tokenizer_path,
            model_config=args.model_config,
            pretrained_model=current_checkpoint,
            use_flash_attention=args.flash_attention,
            use_compile=args.compile,
            use_bf16=args.bf16,
            use_fp16=args.fp16,
        )
        
        # 2. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
        if src_type == "hf":
            dataset, text_column = load_pretrain_dataset(
                dataset_names=[src_name],
                dataset_config=args.dataset_config if idx == 0 else None,
                train_files=None,
                text_column=args.text_column,
            )
        else:
            dataset, text_column = load_pretrain_dataset(
                dataset_names=None,
                dataset_config=None,
                train_files=[src_name],
                text_column=args.text_column,
            )
        
        # 3. í† í°í™”
        logger.info("ğŸ”¤ Tokenizing dataset...")
        
        if args.packing:
            logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode)")
            
            # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
            def batch_tokenize(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=False,
                    padding=False,
                    add_special_tokens=True,
                )
            
            logger.info("  Batch tokenizing...")
            # ë²„í¼ ê³µê°„ ë¶€ì¡± ë°©ì§€ë¥¼ ìœ„í•´ num_proc ì œí•œ
            # TOKENIZERS_PARALLELISM=trueê°€ ì„¤ì •ë˜ì–´ ìˆì–´ Rust ë ˆë²¨ì—ì„œ ë³‘ë ¬í™”ë¨
            safe_num_proc = min(args.num_proc, 4)  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì œí•œ
            tokenized_ds = dataset["train"].map(
                batch_tokenize,
                batched=True,
                batch_size=1000,
                num_proc=safe_num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=True,  # Use cache to avoid re-tokenizing
                desc="Tokenizing",
            )
            
            # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
            del tokenized_ds
            gc.collect()
            
            concatenated_chunks = concatenate_sequences(
                tokenized_sequences=tokenized_list,
                max_seq_length=args.max_seq_length,
                eos_token_id=tokenizer.eos_token_id,
            )
            
            from datasets import Dataset as HFDataset
            tokenized_dataset = HFDataset.from_list(concatenated_chunks)
            
            # ë©”ëª¨ë¦¬ í•´ì œ
            del tokenized_list
            del concatenated_chunks
            gc.collect()
        else:
            def tokenize_function(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=True,
                    max_length=args.max_seq_length,
                    padding=False,
                    return_special_tokens_mask=True,
                )

            # ë²„í¼ ê³µê°„ ë¶€ì¡± ë°©ì§€ë¥¼ ìœ„í•´ num_proc ì œí•œ
            safe_num_proc = min(args.num_proc, 4)  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì œí•œ
            tokenized_dataset = dataset["train"].map(
                tokenize_function,
                batched=True,
                batch_size=1000,
                num_proc=safe_num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=True,  # Use cache to avoid re-tokenizing
                desc="Tokenizing",
            )
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë©”ëª¨ë¦¬ í•´ì œ
        del dataset
        gc.collect()
        
        logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")
        
        # 4. í•™ìŠµ
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê° ë°ì´í„°ì…‹ë³„)
        stage_output_dir = f"{args.output_dir}/stage_{idx+1}"
        
        training_args = TrainingArguments(
            output_dir=stage_output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps if idx == 0 else 100,  # ì²« ë²ˆì§¸ë§Œ full warmup
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            save_total_limit=2,
            bf16=args.bf16,
            fp16=args.fp16,
            gradient_checkpointing=args.gradient_checkpointing,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            report_to="none",
            max_steps=args.max_steps if args.max_steps > 0 else -1,
            # ì¶”ê°€ ìµœì í™” ì˜µì…˜
            dataloader_pin_memory=True,
            dataloader_prefetch_factor=4,
            dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
            optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
            ddp_find_unused_parameters=False,
            tf32=True,
            group_by_length=False,
            max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            processing_class=tokenizer,  # tokenizer deprecated in v5.0
        )
        
        logger.info(f"ğŸƒ Training on dataset {idx+1}/{len(all_sources)}...")
        trainer.train()
        
        # 5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (DDP í™˜ê²½ì—ì„œ ë™ê¸°í™” í•„ìš”)
        checkpoint_path = f"{stage_output_dir}/checkpoint"
        trainer.save_model(checkpoint_path)
        
        # DDP í™˜ê²½ì—ì„œ ëª¨ë“  rankê°€ ì €ì¥ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        
        # ì €ì¥ í™•ì¸ ë° dtype ìœ ì§€ (rank 0ì—ì„œ)
        is_main_process = not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0
        if is_main_process:
            # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ì¬ì €ì¥
            model_dtype = next(model.parameters()).dtype
            if model_dtype in (torch.bfloat16, torch.float16):
                logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format...")
                model.save_pretrained(checkpoint_path, torch_dtype=model_dtype, safe_serialization=True)
            
            saved_files = list(Path(checkpoint_path).glob("*.safetensors")) + \
                         list(Path(checkpoint_path).glob("*.bin"))
            if saved_files:
                logger.info(f"ğŸ’¾ Saved checkpoint to: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸  No model files found in: {checkpoint_path}")
        
        # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path
        
        # 6. ë©”ëª¨ë¦¬ í•´ì œ
        del model
        del tokenizer
        del tokenized_dataset
        del trainer
        gc.collect()
        
        try:
            torch.cuda.empty_cache()
        except:
            pass
        
        # DDP barrier (ë‹¤ìŒ ë‹¨ê³„ ì‹œì‘ ì „ ë™ê¸°í™”)
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
        
        logger.info(f"âœ… Completed dataset {idx+1}/{len(all_sources)}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    logger.info("="*80)
    logger.info("ğŸ¯ Sequential training completed!")
    logger.info(f"ğŸ“ Final model: {current_checkpoint}")
    logger.info("="*80)


def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""

    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info("="*80)

    # Sequential ëª¨ë“œ: ê° ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    if args.sequential and args.dataset and len(args.dataset) > 1:
        logger.info("ğŸ“¦ Sequential mode: Processing datasets one by one")
        train_sequential(args)
        return

    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    if args.mode == "pretrain":
        dataset, text_column = load_pretrain_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            dataset_config=args.dataset_config,
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        )

    # 3. í† í°í™”
    logger.info("ğŸ”¤ Tokenizing dataset...")

    # Packing ëª¨ë“œ: ì‹œí€€ìŠ¤ ì—°ê²° ë°©ì‹ ì‚¬ìš© (pretrain/sft ë‘˜ ë‹¤ ì§€ì›)
    if args.packing:
        logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode) for {args.mode}")
        
        # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
        def batch_tokenize(examples):
            return tokenizer(
                examples[text_column],
                truncation=False,  # ì—°ê²°í•  ê²ƒì´ë¯€ë¡œ truncation ì•ˆí•¨
                padding=False,
                add_special_tokens=True,
            )
        
        logger.info("  Batch tokenizing...")
        # ë²„í¼ ê³µê°„ ë¶€ì¡± ë°©ì§€ë¥¼ ìœ„í•´ num_proc ì œí•œ
        safe_num_proc = min(args.num_proc, 4)  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì œí•œ
        tokenized_ds = dataset["train"].map(
            batch_tokenize,
            batched=True,
            batch_size=1000,
            num_proc=safe_num_proc,
            remove_columns=dataset["train"].column_names,
            load_from_cache_file=True,  # Use cache to avoid re-tokenizing
            desc="Tokenizing",
        )
        
        # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
        del tokenized_ds
        
        # ì‹œí€€ìŠ¤ ì—°ê²° ë° ì²­í‚¹
        concatenated_chunks = concatenate_sequences(
            tokenized_sequences=tokenized_list,
            max_seq_length=args.max_seq_length,
            eos_token_id=tokenizer.eos_token_id,
        )
        
        del tokenized_list
        
        # Datasetìœ¼ë¡œ ë³€í™˜
        from datasets import Dataset as HFDataset
        tokenized_dataset = HFDataset.from_list(concatenated_chunks)
        
        del concatenated_chunks
        
    else:
        # ê¸°ì¡´ ë°©ì‹: ê°œë³„ ìƒ˜í”Œ í† í°í™” with truncation
        def tokenize_function(examples):
            return tokenizer(
                examples[text_column],
                truncation=True,
                max_length=args.max_seq_length,
                padding=False,
                return_special_tokens_mask=True,
            )

        # ë²„í¼ ê³µê°„ ë¶€ì¡± ë°©ì§€ë¥¼ ìœ„í•´ num_proc ì œí•œ
        safe_num_proc = min(args.num_proc, 4)  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì œí•œ
        tokenized_dataset = dataset["train"].map(
            tokenize_function,
            batched=True,
            batch_size=1000,
            num_proc=safe_num_proc,
            remove_columns=dataset["train"].column_names,
            load_from_cache_file=True,  # Use cache to avoid re-tokenizing
            desc="Tokenizing",
        )

    logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")

    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments (ìµœì í™” ì˜µì…˜ í¬í•¨)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to="none",
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        # ì¶”ê°€ ìµœì í™” ì˜µì…˜
        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        dataloader_prefetch_factor=4,  # ë¯¸ë¦¬ ë°°ì¹˜ ë¡œë“œ (ì¦ê°€)
        dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
        optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
        ddp_find_unused_parameters=False,  # DDP ìµœì í™”
        tf32=True,  # TF32 ì‚¬ìš© (Ampere GPU)
        group_by_length=False,  # ê¸¸ì´ë³„ ê·¸ë£¹í•‘ ë¹„í™œì„±í™” (packing ì‚¬ìš©ì‹œ)
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Packing: {args.packing}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    if args.resume_from_checkpoint:
        logger.info(f"  Resume from: {args.resume_from_checkpoint}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))
    # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ì €ì¥
    model_dtype = next(model.parameters()).dtype
    if model_dtype in (torch.bfloat16, torch.float16):
        logger.info(f"ğŸ’¾ Saving model in {model_dtype} format...")
        model.save_pretrained(str(final_path), torch_dtype=model_dtype, safe_serialization=True)

    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info("="*80)


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # ëª¨ë“œ
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["pretrain", "sft"],
        help="Training mode: pretrain or sft"
    )

    # ë°ì´í„° (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)
    parser.add_argument(
        "--dataset",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
        help="HuggingFace dataset name(s). Multiple datasets can be specified."
    )
    parser.add_argument("--dataset_config", type=str, help="Dataset config/subset (for single dataset)")
    parser.add_argument(
        "--train_file",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        help="Local train file(s) (txt or json). Multiple files can be specified."
    )
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")

    # ëª¨ë¸
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/",
        help="Tokenizer path"
    )
    parser.add_argument("--model_config", type=str, help="Model config JSON file")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path (for SFT)")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size per device")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=4, help="Number of processes for tokenization")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    main()
