"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

import argparse
import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
try:
    import orjson as json  # Rust-based, 10-50x faster
except ImportError:
    import json  # Fallback to standard json

import torch
from transformers import (
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset

from moai_llm.config import MoaiConfig
from moai_llm.modeling.model import MoaiForCausalLM

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
    
    ì´ë ‡ê²Œ í•˜ë©´ max_seq_lengthë¡œ ì˜ë¦¬ë”ë¼ë„ ë‹¤ìŒ ì²­í¬ì—ì„œ 
    ì´ì–´ì„œ EOSê¹Œì§€ ì˜¨ì „íˆ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID
    
    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np
    
    # 1. ì´ ê¸¸ì´ ê³„ì‚° (ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹ìš©)
    total_len = 0
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        total_len += len(input_ids)
        if len(input_ids) > 0 and input_ids[-1] != eos_token_id:
            total_len += 1  # EOS ì¶”ê°€ë  ì˜ˆì •
    
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences into ~{total_len:,} tokens")
    
    # 2. numpy ë°°ì—´ë¡œ ë¹ ë¥´ê²Œ ì—°ê²° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    all_tokens = np.empty(total_len, dtype=np.int32)
    offset = 0
    
    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)
        
        if seq_len == 0:
            continue
            
        # ë°°ì—´ì— ë³µì‚¬
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len
        
        # EOS ì¶”ê°€
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1
    
    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]
    
    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í•  (list comprehensionìœ¼ë¡œ ë¹ ë¥´ê²Œ)
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []
    
    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk = all_tokens[start:end].tolist()
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ (< 128) ë²„ë¦¼
        if len(chunk) < 128:
            logger.info(f"  Dropping short final chunk of {len(chunk)} tokens")
            continue
            
        chunks.append({
            "input_ids": chunk,
            "attention_mask": [1] * len(chunk),
        })
    
    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    
    return chunks


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None) -> list:
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    
    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"
    """
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    if dataset_config:
        logger.info(f"    Config: {dataset_config}")
        raw_dataset = load_dataset(dataset_name, dataset_config)
    else:
        raw_dataset = load_dataset(dataset_name)
    
    # train split ì‚¬ìš©
    train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë¹ ë¦„)
    converted = train_data.map(
        convert_batch,
        batched=True,
        batch_size=5000,  # Increased for less overhead
        num_proc=min(16, os.cpu_count() or 4),
        remove_columns=train_data.column_names,
        load_from_cache_file=False,
        desc=f"Converting {dataset_name}",
    )
    
    # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
    converted = converted.filter(lambda x: len(x["text"]) > 0, num_proc=4)
    
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    formatted_data = [{"text": t} for t in converted["text"]]
    
    logger.info(f"    â†’ {len(formatted_data):,} samples")
    return formatted_data


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")
    
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (pretrainìš©)
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        - {"instruction": "...", "output": "..."}: Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        - {"messages": [...]}: Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        - {"conversations": [...]}: Chat í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item:
        return item["text"]
    
    # input/output í¬ë§·
    if "input" in item and "output" in item:
        text = f"<|im_start|>user\n{item['input']}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
        return text
    
    # instruction/output í¬ë§· (Alpaca)
    if "instruction" in item and "output" in item:
        text = f"<|im_start|>user\n{item['instruction']}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
        return text
    
    # messages í¬ë§· (OpenAI Chat)
    if "messages" in item:
        text = ""
        for msg in item["messages"]:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        return text.strip()
    
    # conversations í¬ë§· (ShareGPT)
    if "conversations" in item:
        text = ""
        for conv in item["conversations"]:
            role = "user" if conv.get("from") == "human" else "assistant"
            value = conv.get("value", "")
            text += f"<|im_start|>{role}\n{value}<|im_end|>\n"
        return text.strip()
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content)
    if "input" in item and "content" in item:
        text = f"<|im_start|>user\n{item['input']}<|im_end|>\n"
        # reasoning_contentê°€ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
        if item.get("reasoning_content"):
            text += f"<|im_start|>assistant\n<think>\n{item['reasoning_content']}\n</think>\n{item['content']}<|im_end|>"
        else:
            text += f"<|im_start|>assistant\n{item['content']}<|im_end|>"
        return text
    
    # prompt/response í¬ë§·
    if "prompt" in item and "response" in item:
        text = f"<|im_start|>user\n{item['prompt']}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{item['response']}<|im_end|>"
        return text
    
    # question/answer í¬ë§·
    if "question" in item and "answer" in item:
        text = f"<|im_start|>user\n{item['question']}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{item['answer']}<|im_end|>"
        return text
    
    # prompt/completion í¬ë§·
    if "prompt" in item and "completion" in item:
        text = f"<|im_start|>user\n{item['prompt']}<|im_end|>\n"
        text += f"<|im_start|>assistant\n{item['completion']}<|im_end|>"
        return text
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        model = MoaiForCausalLM.from_pretrained(pretrained_model, torch_dtype=dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.torch_dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train_sequential(args):
    """
    ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” í•™ìŠµ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    ê° ë°ì´í„°ì…‹ì— ëŒ€í•´:
    1. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
    2. í† í°í™” ë° í•™ìŠµ
    3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    4. ë©”ëª¨ë¦¬ í•´ì œ
    5. ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ (ì´ì „ ì²´í¬í¬ì¸íŠ¸ì—ì„œ resume)
    """
    import gc
    
    dataset_names = args.dataset if args.dataset else []
    train_files = args.train_file if args.train_file else []
    
    # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
    all_sources = []
    for ds in dataset_names:
        all_sources.append(("hf", ds))
    for f in train_files:
        all_sources.append(("file", f))
    
    logger.info(f"ğŸ“‹ Processing {len(all_sources)} datasets sequentially:")
    for i, (src_type, src_name) in enumerate(all_sources):
        logger.info(f"  {i+1}. [{src_type}] {src_name}")
    
    current_checkpoint = args.pretrained_model
    
    for idx, (src_type, src_name) in enumerate(all_sources):
        logger.info("="*80)
        logger.info(f"ğŸ”„ [{idx+1}/{len(all_sources)}] Processing: {src_name}")
        logger.info("="*80)
        
        # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        model, tokenizer = setup_model_and_tokenizer(
            tokenizer_path=args.tokenizer_path,
            model_config=args.model_config,
            pretrained_model=current_checkpoint,
            use_flash_attention=args.flash_attention,
            use_compile=args.compile,
            use_bf16=args.bf16,
            use_fp16=args.fp16,
        )
        
        # 2. í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ ë¡œë“œ
        if src_type == "hf":
            dataset, text_column = load_pretrain_dataset(
                dataset_names=[src_name],
                dataset_config=args.dataset_config if idx == 0 else None,
                train_files=None,
                text_column=args.text_column,
            )
        else:
            dataset, text_column = load_pretrain_dataset(
                dataset_names=None,
                dataset_config=None,
                train_files=[src_name],
                text_column=args.text_column,
            )
        
        # 3. í† í°í™”
        logger.info("ğŸ”¤ Tokenizing dataset...")
        
        if args.packing:
            logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode)")
            
            # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
            def batch_tokenize(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=False,
                    padding=False,
                    add_special_tokens=True,
                )
            
            logger.info("  Batch tokenizing...")
            tokenized_ds = dataset["train"].map(
                batch_tokenize,
                batched=True,
                batch_size=5000,  # Increased for less overhead
                num_proc=args.num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=False,  # Skip cache check
                desc="Tokenizing",
            )
            
            # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
            del tokenized_ds
            gc.collect()
            
            concatenated_chunks = concatenate_sequences(
                tokenized_sequences=tokenized_list,
                max_seq_length=args.max_seq_length,
                eos_token_id=tokenizer.eos_token_id,
            )
            
            from datasets import Dataset as HFDataset
            tokenized_dataset = HFDataset.from_list(concatenated_chunks)
            
            # ë©”ëª¨ë¦¬ í•´ì œ
            del tokenized_list
            del concatenated_chunks
            gc.collect()
        else:
            def tokenize_function(examples):
                return tokenizer(
                    examples[text_column],
                    truncation=True,
                    max_length=args.max_seq_length,
                    padding=False,
                    return_special_tokens_mask=True,
                )

            tokenized_dataset = dataset["train"].map(
                tokenize_function,
                batched=True,
                batch_size=5000,
                num_proc=args.num_proc,
                remove_columns=dataset["train"].column_names,
                load_from_cache_file=False,
                desc="Tokenizing",
            )
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë©”ëª¨ë¦¬ í•´ì œ
        del dataset
        gc.collect()
        
        logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")
        
        # 4. í•™ìŠµ
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê° ë°ì´í„°ì…‹ë³„)
        stage_output_dir = f"{args.output_dir}/stage_{idx+1}"
        
        training_args = TrainingArguments(
            output_dir=stage_output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=args.warmup_steps if idx == 0 else 100,  # ì²« ë²ˆì§¸ë§Œ full warmup
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            save_total_limit=2,
            bf16=args.bf16,
            fp16=args.fp16,
            gradient_checkpointing=args.gradient_checkpointing,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            report_to="none",
            max_steps=args.max_steps if args.max_steps > 0 else -1,
            # ì¶”ê°€ ìµœì í™” ì˜µì…˜
            dataloader_pin_memory=True,
            dataloader_prefetch_factor=4,
            dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
            optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
            ddp_find_unused_parameters=False,
            tf32=True,
            group_by_length=False,
            max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer,
        )
        
        logger.info(f"ğŸƒ Training on dataset {idx+1}/{len(all_sources)}...")
        trainer.train()
        
        # 5. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = f"{stage_output_dir}/checkpoint"
        trainer.save_model(checkpoint_path)
        logger.info(f"ğŸ’¾ Saved checkpoint to: {checkpoint_path}")
        
        # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path
        
        # 6. ë©”ëª¨ë¦¬ í•´ì œ
        del model
        del tokenizer
        del tokenized_dataset
        del trainer
        gc.collect()
        
        try:
            import torch
            torch.cuda.empty_cache()
        except:
            pass
        
        logger.info(f"âœ… Completed dataset {idx+1}/{len(all_sources)}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    logger.info("="*80)
    logger.info("ğŸ¯ Sequential training completed!")
    logger.info(f"ğŸ“ Final model: {current_checkpoint}")
    logger.info("="*80)


def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""

    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info("="*80)

    # Sequential ëª¨ë“œ: ê° ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    if args.sequential and args.dataset and len(args.dataset) > 1:
        logger.info("ğŸ“¦ Sequential mode: Processing datasets one by one")
        train_sequential(args)
        return

    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    if args.mode == "pretrain":
        dataset, text_column = load_pretrain_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            dataset_config=args.dataset_config,
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_names=args.dataset,  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
            train_files=args.train_file,  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        )

    # 3. í† í°í™”
    logger.info("ğŸ”¤ Tokenizing dataset...")

    # Packing ëª¨ë“œ: ì‹œí€€ìŠ¤ ì—°ê²° ë°©ì‹ ì‚¬ìš© (pretrain/sft ë‘˜ ë‹¤ ì§€ì›)
    if args.packing:
        logger.info(f"ğŸ“¦ Using sequence concatenation (packing mode) for {args.mode}")
        
        # ë°°ì¹˜ í† í°í™” (ë¹ ë¦„)
        def batch_tokenize(examples):
            return tokenizer(
                examples[text_column],
                truncation=False,  # ì—°ê²°í•  ê²ƒì´ë¯€ë¡œ truncation ì•ˆí•¨
                padding=False,
                add_special_tokens=True,
            )
        
        logger.info("  Batch tokenizing...")
        tokenized_ds = dataset["train"].map(
            batch_tokenize,
            batched=True,
            batch_size=5000,  # Increased for less overhead
            num_proc=args.num_proc,
            remove_columns=dataset["train"].column_names,
            load_from_cache_file=False,  # Skip cache check
            desc="Tokenizing",
        )
        
        # input_ids ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
        del tokenized_ds
        
        # ì‹œí€€ìŠ¤ ì—°ê²° ë° ì²­í‚¹
        concatenated_chunks = concatenate_sequences(
            tokenized_sequences=tokenized_list,
            max_seq_length=args.max_seq_length,
            eos_token_id=tokenizer.eos_token_id,
        )
        
        del tokenized_list
        
        # Datasetìœ¼ë¡œ ë³€í™˜
        from datasets import Dataset as HFDataset
        tokenized_dataset = HFDataset.from_list(concatenated_chunks)
        
        del concatenated_chunks
        
    else:
        # ê¸°ì¡´ ë°©ì‹: ê°œë³„ ìƒ˜í”Œ í† í°í™” with truncation
        def tokenize_function(examples):
            return tokenizer(
                examples[text_column],
                truncation=True,
                max_length=args.max_seq_length,
                padding=False,
                return_special_tokens_mask=True,
            )

        tokenized_dataset = dataset["train"].map(
            tokenize_function,
            batched=True,
            batch_size=5000,
            num_proc=args.num_proc,
            remove_columns=dataset["train"].column_names,
            load_from_cache_file=False,
            desc="Tokenizing",
        )

    logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")

    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments (ìµœì í™” ì˜µì…˜ í¬í•¨)
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to="none",
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        # ì¶”ê°€ ìµœì í™” ì˜µì…˜
        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        dataloader_prefetch_factor=4,  # ë¯¸ë¦¬ ë°°ì¹˜ ë¡œë“œ (ì¦ê°€)
        dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ì†ë„â†‘)
        optim="adamw_torch_fused",  # Fused Adam (faster than 8-bit)
        ddp_find_unused_parameters=False,  # DDP ìµœì í™”
        tf32=True,  # TF32 ì‚¬ìš© (Ampere GPU)
        group_by_length=False,  # ê¸¸ì´ë³„ ê·¸ë£¹í•‘ ë¹„í™œì„±í™” (packing ì‚¬ìš©ì‹œ)
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        gradient_checkpointing_kwargs={"use_reentrant": False},  # ìµœì‹  ë°©ì‹
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Packing: {args.packing}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    if args.resume_from_checkpoint:
        logger.info(f"  Resume from: {args.resume_from_checkpoint}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))

    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info("="*80)


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # ëª¨ë“œ
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["pretrain", "sft"],
        help="Training mode: pretrain or sft"
    )

    # ë°ì´í„° (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)
    parser.add_argument(
        "--dataset",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì§€ì›
        help="HuggingFace dataset name(s). Multiple datasets can be specified."
    )
    parser.add_argument("--dataset_config", type=str, help="Dataset config/subset (for single dataset)")
    parser.add_argument(
        "--train_file",
        type=str,
        nargs='+',  # ì—¬ëŸ¬ íŒŒì¼ ì§€ì›
        help="Local train file(s) (txt or json). Multiple files can be specified."
    )
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")

    # ëª¨ë¸
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/",
        help="Tokenizer path"
    )
    parser.add_argument("--model_config", type=str, help="Model config JSON file")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path (for SFT)")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size per device")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=4, help="Number of processes for tokenization")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    main()
