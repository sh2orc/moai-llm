"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

# ============================================================================
# Constants and Configuration
# ============================================================================

# Timeout settings (seconds)
IMPORT_SYNC_TIMEOUT = 300  # 5 minutes for import synchronization
TOKENIZATION_TIMEOUT = 7200  # 2 hours for tokenization
DATASET_PROCESSING_TIMEOUT = 3600  # 1 hour for dataset processing
CHECK_INTERVAL = 5  # seconds between checks

# Dataset size thresholds
DATASET_SIZE_LARGE = 5_000_000  # 5M+ samples
DATASET_SIZE_MEDIUM = 1_000_000  # 1M-5M samples

# Tokenization settings
MIN_CHUNK_LENGTH = 128  # Minimum tokens per chunk
WARMUP_STEPS_FIRST_STAGE = 2000  # For first training stage
WARMUP_STEPS_RESUME = 100  # For resumed training

# Default batch sizes (Rust Fast Tokenizer ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ìµœì í™”)
BATCH_SIZE_LARGE_DATASET = 5000  # ëŒ€ê·œëª¨: Rust ì„±ëŠ¥ ìµœëŒ€ í™œìš©
BATCH_SIZE_DEFAULT = 10000  # ê¸°ë³¸: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ í° ë°°ì¹˜
WRITER_BATCH_SIZE = 50000  # ë””ìŠ¤í¬ ì“°ê¸° ë°°ì¹˜

# Default process counts (ë©€í‹°í”„ë¡œì„¸ì‹± ë³‘ë ¬ ì²˜ë¦¬)
DEFAULT_NUM_PROC = 4  # 4ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ (ì†ë„ì™€ ë©”ëª¨ë¦¬ ê· í˜•)
FILTER_NUM_PROC_DIVISOR = 2  # í•„í„°ë§ í”„ë¡œì„¸ìŠ¤
MAX_FILTER_NUM_PROC = 2  # ìµœëŒ€ í•„í„°ë§ í”„ë¡œì„¸ìŠ¤

# Performance settings
ESTIMATED_TOKENIZATION_SPEED = 10000  # samples/sec (ë©€í‹°í”„ë¡œì„¸ì‹±)
WARMUP_TEXT_PATTERN = "Hello world " * 100
WARMUP_TEXT_COUNT = 10

# Environment variable keys
ENV_RANK = "RANK"
ENV_WORLD_SIZE = "WORLD_SIZE"
ENV_LOCAL_RANK = "LOCAL_RANK"
ENV_HF_HOME = "HF_HOME"
ENV_XDG_CACHE_HOME = "XDG_CACHE_HOME"
ENV_DATASET_NUM_PROC = "DATASET_NUM_PROC"
ENV_DATASET_BATCH_SIZE = "DATASET_BATCH_SIZE"
ENV_DATASET_WRITER_BATCH_SIZE = "DATASET_WRITER_BATCH_SIZE"
ENV_TOKENIZERS_PARALLELISM = "TOKENIZERS_PARALLELISM"


# ============================================================================
# Early initialization
# ============================================================================

# âš ï¸ ì¤‘ìš”: ëª¨ë“  import ì „ì— TOKENIZERS_PARALLELISM ì„¤ì •!
# tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ import ì‹œì ì— ì´ ê°’ì„ ìºì‹±í•˜ë¯€ë¡œ ê°€ì¥ ë¨¼ì € ì„¤ì •í•´ì•¼ í•¨
import os
os.environ["TOKENIZERS_PARALLELISM"] = "true"

import sys
import time as time_module
from pathlib import Path as PathType

# Check rank early
rank = int(os.environ.get(ENV_RANK, 0))
world_size = int(os.environ.get(ENV_WORLD_SIZE, 1))
is_main = (rank == 0)

# ë™ê¸°í™” ë§ˆì»¤ íŒŒì¼
import_marker = PathType("/tmp/.moai_import_done")


def _import_all_modules():
    """ê³µí†µ import ë¡œì§ (ì¤‘ë³µ ì œê±°)"""
    import argparse
    import hashlib
    import time
    import gc
    import logging
    from pathlib import Path
    from typing import Optional, Dict, Any

    try:
        import orjson as json
    except ImportError:
        import json

    try:
        import psutil
    except ImportError:
        psutil = None

    import torch
    from transformers import (
        AutoTokenizer,
        Trainer,
        TrainingArguments,
        DataCollatorForLanguageModeling,
    )
    from datasets import load_dataset, disable_caching
    import datasets
    datasets.config.IN_MEMORY_MAX_SIZE = 0
    from moai_llm.config import MoaiConfig
    from moai_llm.modeling.model import MoaiForCausalLM

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
    logger = logging.getLogger(__name__)

    # Return all imports as a dict for global namespace injection
    return {
        'argparse': argparse, 'hashlib': hashlib, 'time': time, 'gc': gc,
        'logging': logging, 'Path': Path, 'Optional': Optional, 'Dict': Dict, 'Any': Any,
        'json': json, 'psutil': psutil, 'torch': torch,
        'AutoTokenizer': AutoTokenizer, 'Trainer': Trainer,
        'TrainingArguments': TrainingArguments,
        'DataCollatorForLanguageModeling': DataCollatorForLanguageModeling,
        'load_dataset': load_dataset, 'disable_caching': disable_caching,
        'datasets': datasets, 'MoaiConfig': MoaiConfig,
        'MoaiForCausalLM': MoaiForCausalLM, 'logger': logger,
    }


# Import synchronization
if is_main:
    # Rank 0: ë¨¼ì € import
    print(f"[IMPORT] Rank 0: Importing modules (world_size={world_size})...", flush=True)

    # ì´ì „ ë§ˆì»¤ ì œê±°
    if import_marker.exists():
        import_marker.unlink()

    _modules = _import_all_modules()
    globals().update(_modules)

    # ë§ˆì»¤ ìƒì„± (ë‹¤ë¥¸ rankë“¤ì´ import ì‹œì‘ ê°€ëŠ¥)
    import_marker.touch()
    print(f"[IMPORT] Rank 0: âœ… All modules imported!", flush=True)
else:
    # ë‹¤ë¥¸ rankë“¤: ë§ˆì»¤ ëŒ€ê¸°
    print(f"[IMPORT] Rank {rank}: Waiting for rank 0...", flush=True)

    waited = 0
    while not import_marker.exists() and waited < IMPORT_SYNC_TIMEOUT:
        time_module.sleep(0.5)
        waited += 0.5

    if not import_marker.exists():
        print(f"[IMPORT] Rank {rank}: Timeout waiting for rank 0!", flush=True)
        sys.exit(1)

    # ì´ì œ ì•ˆì „í•˜ê²Œ import
    _modules = _import_all_modules()
    globals().update(_modules)

    print(f"[IMPORT] Rank {rank}: âœ… Modules imported!", flush=True)


# ============================================================================
# Utility Functions
# ============================================================================

def get_ddp_info() -> Dict[str, Any]:
    """
    DDP í™˜ê²½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict: rank, world_size, is_distributed, is_main_process í¬í•¨
    """
    rank = int(os.environ.get(ENV_RANK, -1))
    world_size = int(os.environ.get(ENV_WORLD_SIZE, -1))

    is_distributed = rank >= 0 and world_size > 1

    # torch.distributedë¡œ ë‹¤ì‹œ í™•ì¸
    if not is_distributed and torch.distributed.is_available():
        try:
            if torch.distributed.is_initialized():
                rank = torch.distributed.get_rank()
                world_size = torch.distributed.get_world_size()
                is_distributed = True
        except (RuntimeError, ValueError, AttributeError):
            pass

    return {
        'rank': rank if rank >= 0 else 0,
        'world_size': world_size if world_size > 0 else 1,
        'is_distributed': is_distributed,
        'is_main_process': rank == 0 if rank >= 0 else True,
    }


def ddp_barrier():
    """ì•ˆì „í•œ DDP barrier í˜¸ì¶œ"""
    try:
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
    except (RuntimeError, ValueError, AttributeError):
        pass


def get_cache_home() -> str:
    """ìºì‹œ í™ˆ ë””ë ‰í† ë¦¬ ë°˜í™˜"""
    return os.environ.get(
        ENV_HF_HOME,
        os.environ.get(ENV_XDG_CACHE_HOME, os.path.expanduser("~/.cache/huggingface"))
    )


def create_cache_path(name: str, suffix: str = "") -> Path:
    """
    ìºì‹œ ê²½ë¡œ ìƒì„±

    Args:
        name: ë°ì´í„°ì…‹ ì´ë¦„ ë˜ëŠ” ì‹ë³„ì
        suffix: ê²½ë¡œ ì ‘ë¯¸ì‚¬ (ì˜ˆ: "_tokenized", "_final")

    Returns:
        Path: ìºì‹œ ê²½ë¡œ
    """
    cache_hash = hashlib.md5(name.encode()).hexdigest()[:16]
    cache_home = get_cache_home()
    return Path(cache_home) / "datasets" / f"{cache_hash}{suffix}"


def wait_for_marker(marker_path: Path, timeout: int = TOKENIZATION_TIMEOUT,
                   check_interval: int = CHECK_INTERVAL, rank: int = 0) -> bool:
    """
    ë§ˆì»¤ íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°

    Args:
        marker_path: ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ
        timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
        rank: í˜„ì¬ rank (ë¡œê¹…ìš©)

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    import time
    waited = 0
    while not marker_path.exists() and waited < timeout:
        time.sleep(check_interval)
        waited += check_interval
        if waited % 60 == 0:  # 1ë¶„ë§ˆë‹¤ ë¡œê·¸
            logger.info(f"[Rank {rank}] Still waiting... ({waited}s elapsed)")

    return marker_path.exists()


def log_with_rank(msg: str, rank: int = None, is_main: bool = None):
    """
    Rank ì •ë³´ì™€ í•¨ê»˜ ë¡œê¹…

    Args:
        msg: ë¡œê·¸ ë©”ì‹œì§€
        rank: Rank ë²ˆí˜¸ (Noneì´ë©´ ìë™ ê°ì§€)
        is_main: Main process ì—¬ë¶€ (Noneì´ë©´ ìë™ ê°ì§€)
    """
    if rank is None or is_main is None:
        ddp_info = get_ddp_info()
        rank = ddp_info['rank']
        is_main = ddp_info['is_main_process']

    if is_main:
        logger.info(msg)
    else:
        logger.info(f"[Rank {rank}] {msg}")


def calculate_optimal_num_proc(total_samples: int, cpu_count: int, available_memory: int = None) -> int:
    """
    ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³„ì‚° (í•­ìƒ 1 ë°˜í™˜ - ë©”ëª¨ë¦¬ íš¨ìœ¨)

    num_proc > 1ì„ ì‚¬ìš©í•˜ë©´:
    1. ê° í”„ë¡œì„¸ìŠ¤ê°€ ì „ì²´ ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ â†’ ë©”ëª¨ë¦¬ í­ë°œ
    2. datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ TOKENIZERS_PARALLELISM=false ê°•ì œ ì„¤ì •
    3. Rust ë‚´ë¶€ ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”

    num_proc = 1ì„ ì‚¬ìš©í•˜ë©´:
    1. ë©”ëª¨ë¦¬ íš¨ìœ¨: ë°ì´í„°ì…‹ 1ë²ˆë§Œ ë¡œë“œ
    2. Rust Fast Tokenizerì˜ ë‚´ë¶€ ë©€í‹°ìŠ¤ë ˆë”© í™œìš© ê°€ëŠ¥
    3. ì „ì²´ì ìœ¼ë¡œ ë” ì•ˆì •ì ì´ê³  ë¹ ë¦„

    Args:
        total_samples: ì´ ìƒ˜í”Œ ìˆ˜ (ì‚¬ìš© ì•ˆë¨)
        cpu_count: CPU ì½”ì–´ ìˆ˜ (ì‚¬ìš© ì•ˆë¨)
        available_memory: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ì‚¬ìš© ì•ˆë¨)

    Returns:
        í•­ìƒ 1 (ë©”ëª¨ë¦¬ íš¨ìœ¨ + Rust ë³‘ë ¬ ì²˜ë¦¬)
    """
    return 1


def get_tokenization_env_config() -> Dict[str, int]:
    """í† í¬ë‚˜ì´ì œì´ì…˜ ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë°˜í™˜"""
    cpu_count = os.cpu_count() or 8
    return {
        'num_proc': int(os.getenv(ENV_DATASET_NUM_PROC, min(DEFAULT_NUM_PROC, cpu_count))),
        'batch_size': int(os.getenv(ENV_DATASET_BATCH_SIZE, BATCH_SIZE_DEFAULT)),
        'writer_batch_size': int(os.getenv(ENV_DATASET_WRITER_BATCH_SIZE, WRITER_BATCH_SIZE)),
    }


def get_optimal_num_shards(dataset_size: int, cpu_count: int) -> int:
    """
    ë°ì´í„°ì…‹ í¬ê¸°ì™€ CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  shard ìˆ˜ ê³„ì‚°

    Args:
        dataset_size: ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜
        cpu_count: CPU ì½”ì–´ ìˆ˜

    Returns:
        ìµœì  shard ìˆ˜ (8-64 ì‚¬ì´)
    """
    # CPU ì½”ì–´ì˜ ì ˆë°˜ì„ ê¸°ì¤€ìœ¼ë¡œ, ìµœì†Œ 8, ìµœëŒ€ 64
    base_shards = max(8, cpu_count // 2)

    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì€ ë” ë§ì€ shard ì‚¬ìš©
    if dataset_size > DATASET_SIZE_LARGE:
        return min(64, base_shards * 2)
    elif dataset_size > DATASET_SIZE_MEDIUM:
        return min(48, base_shards)
    else:
        return min(32, base_shards)


def get_optimal_prefetch_factor(gpu_memory_gb: float = None, batch_size: int = 4) -> int:
    """
    GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  prefetch factor ê³„ì‚°

    Args:
        gpu_memory_gb: GPU ë©”ëª¨ë¦¬ í¬ê¸° (GB), Noneì´ë©´ ìë™ ê°ì§€
        batch_size: ë°°ì¹˜ í¬ê¸°

    Returns:
        ìµœì  prefetch factor (2-8 ì‚¬ì´)
    """
    if gpu_memory_gb is None:
        try:
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            else:
                gpu_memory_gb = 16  # CPU ëª¨ë“œ ê¸°ë³¸ê°’
        except:
            gpu_memory_gb = 16

    # GPU ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ìµœì  prefetch
    # 40GB+ GPU (A100): 8, 24GB (RTX 3090/4090): 6, 16GB: 4, ê·¸ ì™¸: 2
    if gpu_memory_gb >= 40:
        return 8
    elif gpu_memory_gb >= 24:
        return 6
    elif gpu_memory_gb >= 16:
        return 4
    else:
        return 2


def load_files_parallel(file_paths: list, max_workers: int = 8) -> list:
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¡œë“œ

    Args:
        file_paths: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ worker ìˆ˜

    Returns:
        ë¡œë“œëœ Dataset ë¦¬ìŠ¤íŠ¸
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datasets import Dataset

    if not file_paths:
        return []

    # ë‹¨ì¼ íŒŒì¼ì€ ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆí•„ìš”
    if len(file_paths) == 1:
        logger.info(f"  Loading file: {file_paths[0]}")
        file_data = _load_single_file(file_paths[0])
        logger.info(f"    â†’ {len(file_data):,} samples")
        return [Dataset.from_list(file_data)]

    # ë³‘ë ¬ ë¡œë”©
    logger.info(f"ğŸš€ Loading {len(file_paths)} files in parallel (workers={max_workers})...")
    datasets_list = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # íŒŒì¼ ë¡œë”© ì‘ì—… ì œì¶œ
        future_to_file = {executor.submit(_load_single_file, f): f for f in file_paths}

        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                file_data = future.result()
                logger.info(f"  âœ“ Loaded {file_path}: {len(file_data):,} samples")
                datasets_list.append(Dataset.from_list(file_data))
            except Exception as e:
                logger.error(f"  âœ— Failed to load {file_path}: {e}")

    logger.info(f"âœ… Loaded {len(datasets_list)}/{len(file_paths)} files successfully")
    return datasets_list


def get_cache_version_key(tokenizer, additional_info: str = "") -> str:
    """
    í† í¬ë‚˜ì´ì € ë²„ì „ì„ í¬í•¨í•œ ìºì‹œ ë²„ì „ í‚¤ ìƒì„±

    Args:
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        additional_info: ì¶”ê°€ ì •ë³´ (ì˜ˆ: ì„¤ì •ê°’)

    Returns:
        ìºì‹œ ë²„ì „ í‚¤ (8ìë¦¬ í•´ì‹œ)
    """
    # í† í¬ë‚˜ì´ì € ë²„ì „ ì •ë³´ ìˆ˜ì§‘
    version_info = []

    # 1. í† í¬ë‚˜ì´ì € vocab í¬ê¸°
    version_info.append(f"vocab_{tokenizer.vocab_size}")

    # 2. í† í¬ë‚˜ì´ì € íƒ€ì…
    tokenizer_type = type(tokenizer).__name__
    version_info.append(f"type_{tokenizer_type}")

    # 3. íŠ¹ìˆ˜ í† í°
    special_tokens = {
        'bos': tokenizer.bos_token_id,
        'eos': tokenizer.eos_token_id,
        'pad': tokenizer.pad_token_id,
        'unk': tokenizer.unk_token_id,
    }
    version_info.append(f"tokens_{special_tokens}")

    # 4. ì¶”ê°€ ì •ë³´
    if additional_info:
        version_info.append(additional_info)

    # í•´ì‹œ ìƒì„±
    version_string = "_".join(str(v) for v in version_info)
    cache_version = hashlib.md5(version_string.encode()).hexdigest()[:8]

    return cache_version


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID

    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np

    # 1. ì´ ê¸¸ì´ ì¶”ì • (over-estimate to avoid reallocation)
    estimated_len = sum(len(seq["input_ids"]) + 1 for seq in tokenized_sequences)
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences (~{estimated_len:,} tokens)")

    # 2. í•œ ë²ˆì˜ ë£¨í”„ë¡œ numpy ë°°ì—´ êµ¬ì¶• (ìµœì í™”)
    all_tokens = np.empty(estimated_len, dtype=np.int32)
    offset = 0

    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)

        if seq_len == 0:
            continue

        # ì‹œí€€ìŠ¤ ë³µì‚¬ ë° EOS ì¶”ê°€ë¥¼ í•œ ë²ˆì—
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len

        # EOS ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°ë§Œ)
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1

    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]

    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í• 
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []

    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk_len = end - start

        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë²„ë¦¼
        if chunk_len < MIN_CHUNK_LENGTH:
            logger.info(f"  Dropping short final chunk of {chunk_len} tokens")
            continue

        # numpy ë°°ì—´ì„ ì§ì ‘ ì‚¬ìš© (ë©”ëª¨ë¦¬ ë³µì‚¬ ìµœì†Œí™”)
        chunks.append({
            "input_ids": all_tokens[start:end].copy(),  # numpy array
            "attention_mask": np.ones(chunk_len, dtype=np.int8),  # int8ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        })

    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    return chunks


# ============================================================================
# Optimized Tokenization Function
# ============================================================================

def tokenize_dataset(
    dataset,
    tokenizer,
    text_column: str = "text",
    max_seq_length: int = 2048,
    packing: bool = False,
    num_proc: int = None,
):
    """
    datasets.map() ê¸°ë°˜ í† í¬ë‚˜ì´ì§• (ì•ˆì •ì )

    Args:
        dataset: HuggingFace Dataset ê°ì²´
        tokenizer: í† í¬ë‚˜ì´ì €
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        packing: Trueë©´ truncation ì—†ì´ í† í°í™”
        num_proc: í”„ë¡œì„¸ìŠ¤ ìˆ˜ (Noneì´ë©´ 1)

    Returns:
        í† í°í™”ëœ Dataset ê°ì²´
    """
    total_samples = len(dataset)

    # num_proc ì„¤ì •
    if num_proc is None:
        num_proc = 4  # 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬

    batch_size = 10000

    logger.info(f"ğŸ”¤ Tokenization config:")
    logger.info(f"   Samples: {total_samples:,}")
    logger.info(f"   Processes: {num_proc}")
    logger.info(f"   Batch size: {batch_size:,}")
    logger.info(f"   Mode: {'packing' if packing else 'truncation'}")

    start_time = time.time()

    # í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
    def batch_tokenize(examples):
        if packing:
            return tokenizer(
                examples[text_column],
                truncation=False,
                padding=False,
                add_special_tokens=True,
            )
        else:
            return tokenizer(
                examples[text_column],
                truncation=True,
                max_length=max_seq_length,
                padding=False,
                add_special_tokens=True,
            )

    # datasets.map() ì‚¬ìš©
    tokenized = dataset.map(
        batch_tokenize,
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        remove_columns=dataset.column_names,
        load_from_cache_file=False,
        desc=f"Tokenizing",
    )

    elapsed = time.time() - start_time
    speed = total_samples / elapsed if elapsed > 0 else 0
    logger.info(f"âœ… Tokenization completed:")
    logger.info(f"   Time: {elapsed/60:.1f} min")
    logger.info(f"   Speed: {speed:,.0f} samples/sec")
    logger.info(f"   Output: {len(tokenized):,} samples")

    return tokenized


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None):
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"

    DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    # DDP í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    ddp_info = get_ddp_info()
    is_distributed = ddp_info['is_distributed']
    is_main_process = ddp_info['is_main_process']
    current_rank = ddp_info['rank']
    
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ - ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    load_kwargs = {
        "keep_in_memory": False,  # ë””ìŠ¤í¬ì— ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ë¡œ ìœ ì§€
    }
    if dataset_config:
        load_kwargs["name"] = dataset_config
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
    # ë‹¤ë¥¸ rankë“¤ì€ ìµœì¢… ë³€í™˜ ê²°ê³¼ë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ê²½ë¡œ ìƒì„±
        cache_key = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        dataset_save_path = create_cache_path(cache_key, "_final")
        filter_marker_path = Path(str(dataset_save_path).replace("_final", ".filtered.marker"))

        # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ëª¨ë“  rankê°€ ë¡œë“œ (ì¬ì‹œì‘ ì‹œ ì•ˆì „)
        if dataset_save_path.exists() and filter_marker_path.exists():
            logger.info(f"    [Rank {current_rank}] âœ… Using existing processed dataset from: {dataset_save_path}")
            from datasets import Dataset
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            logger.info(f"    [Rank {current_rank}] Loaded {len(converted):,} samples in {load_time:.1f}s")

            # barrier ë™ê¸°í™”
            ddp_barrier()

            # ë³€í™˜ ê²°ê³¼ ë°˜í™˜ (ë‚˜ë¨¸ì§€ ë¡œì§ ê±´ë„ˆë›°ê¸°)
            return converted

        # barrier ë™ê¸°í™”
        ddp_barrier()
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
            logger.info(f"    [Rank 0] Downloading dataset...")
            if dataset_config:
                logger.info(f"    Config: {dataset_config}")
            raw_dataset = load_dataset(dataset_name, **load_kwargs)
            logger.info(f"    [Rank 0] Dataset download completed")
            
            # train split ì‚¬ìš©
            train_data = raw_dataset.get("train", raw_dataset)
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚˜ì¤‘ì— ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì•„ë¬´ê²ƒë„ ì•ˆ í•¨)
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete processing...")
            train_data = None  # ë‚˜ì¤‘ì— ìºì‹œì—ì„œ ë¡œë“œ
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        if dataset_config:
            logger.info(f"    Config: {dataset_config}")
        raw_dataset = load_dataset(dataset_name, **load_kwargs)
        
        # train split ì‚¬ìš©
        train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    env_config = get_tokenization_env_config()
    dataset_num_proc = env_config['num_proc']
    dataset_batch_size = env_config['batch_size']
    dataset_writer_batch_size = env_config['writer_batch_size']

    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë³€í™˜í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ìºì‹œë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ì™„ë£Œ ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
        cache_key = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        cache_marker = Path(str(create_cache_path(cache_key, "")).replace(cache_key[:16], f".{cache_key[:16]}_converted.marker"))
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë³€í™˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ ë¹ ë¥´ê²Œ)
            logger.info(f"    [Rank 0] Converting dataset with {dataset_num_proc} processes "
                       f"(batch_size={dataset_batch_size}, writer_batch_size={dataset_writer_batch_size})...")
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=dataset_batch_size,
                num_proc=dataset_num_proc,
                remove_columns=train_data.column_names,
                load_from_cache_file=True,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,  # ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš©
                desc=f"Converting {dataset_name}",
            )

            # dataset.map() í›„ TOKENIZERS_PARALLELISM ë³µì›
            os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"

            # ë³€í™˜ ì™„ë£Œ ë§ˆì»¤ ìƒì„± (filter ì „ì—!)
            cache_marker.parent.mkdir(parents=True, exist_ok=True)
            cache_marker.touch()
            logger.info(f"    [Rank 0] Created conversion marker: {cache_marker}")

            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ)
            filter_num_proc = min(dataset_num_proc // FILTER_NUM_PROC_DIVISOR, MAX_FILTER_NUM_PROC)
            logger.info(f"    [Rank 0] Filtering empty texts with {filter_num_proc} processes...")
            converted = converted.filter(
                lambda x: len(x["text"]) > 0,
                num_proc=filter_num_proc,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,
                load_from_cache_file=True,
            )

            # filter() í›„ TOKENIZERS_PARALLELISM ë³µì›
            os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"

            logger.info(f"    [Rank 0] Conversion completed: {len(converted):,} samples")

            # ìµœì¢… ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë‹¤ë¥¸ rankë“¤ì´ ì•ˆì „í•˜ê²Œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
            if not dataset_save_path.exists():
                logger.info(f"    [Rank 0] Saving final dataset to: {dataset_save_path}")
                save_start = time.time()
                num_shards = get_optimal_num_shards(len(converted), os.cpu_count() or 8)
                converted.save_to_disk(
                    str(dataset_save_path),
                    num_shards=num_shards,
                )
                save_time = time.time() - save_start
                logger.info(f"    [Rank 0] Dataset saved in {save_time:.1f}s (shards={num_shards})")
            else:
                logger.info(f"    [Rank 0] Dataset already saved at: {dataset_save_path}")

            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ìƒì„±
            filter_marker_path.touch()
            logger.info(f"    [Rank 0] Created filter marker: {filter_marker_path}")

            # ë³€í™˜ ì™„ë£Œ í›„ barrier
            ddp_barrier()
                
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” í•„í„° ë§ˆì»¤ ëŒ€ê¸° í›„ ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ!
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete all processing...")

            if not wait_for_marker(filter_marker_path, DATASET_PROCESSING_TIMEOUT, CHECK_INTERVAL, current_rank):
                raise TimeoutError(f"Rank {current_rank}: Dataset processing timeout after {DATASET_PROCESSING_TIMEOUT}s")

            logger.info(f"    [Rank {current_rank}] Processing complete, loading final result from cache...")

            # barrier ë™ê¸°í™”
            ddp_barrier()

            # rank 0ì´ ì €ì¥í•œ ìµœì¢… ë°ì´í„°ì…‹ì„ ì§ì ‘ ë¡œë“œ
            logger.info(f"    [Rank {current_rank}] Loading final dataset from: {dataset_save_path}")

            # íŒŒì¼ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì§§ì€ ëŒ€ê¸° (íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™”)
            for attempt in range(60):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
                if dataset_save_path.exists() and (dataset_save_path / "dataset_info.json").exists():
                    break
                time.sleep(0.5)
            else:
                logger.warning(f"    [Rank {current_rank}] Dataset files not fully ready, proceeding anyway...")

            from datasets import Dataset
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start

            logger.info(f"    [Rank {current_rank}] Loaded from disk in {load_time:.1f}s: {len(converted):,} samples")
            
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        logger.info(f"    Converting dataset with {dataset_num_proc} processes...")
        converted = train_data.map(
            convert_batch,
            batched=True,
            batch_size=dataset_batch_size,
            num_proc=dataset_num_proc,
            remove_columns=train_data.column_names,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
            desc=f"Converting {dataset_name}",
        )

        # dataset.map() í›„ TOKENIZERS_PARALLELISM ë³µì›
        os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"

        logger.info(f"    Filtering empty texts...")
        filter_num_proc = min(dataset_num_proc // FILTER_NUM_PROC_DIVISOR, MAX_FILTER_NUM_PROC)
        converted = converted.filter(
            lambda x: len(x["text"]) > 0,
            num_proc=filter_num_proc,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
        )

        # filter() í›„ TOKENIZERS_PARALLELISM ë³µì›
        os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"
    
    # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ í”¼í•˜ê³  Datasetì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    logger.info(f"    â†’ {len(converted):,} samples")
    return converted  # Dataset ê°ì²´ ë°˜í™˜


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    from datasets import Dataset, concatenate_datasets
    
    datasets_list = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬)
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]

        # íŒŒì¼ ë³‘ë ¬ ë¡œë”© ì‚¬ìš©
        loaded_datasets = load_files_parallel(train_files, max_workers=8)
        datasets_list.extend(loaded_datasets)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ (Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ)
            if isinstance(ds_data, Dataset):
                datasets_list.append(ds_data)
            else:
                # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë§Œ ë³€í™˜
                datasets_list.append(Dataset.from_list(ds_data))
    
    if not datasets_list:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    # ì—¬ëŸ¬ Datasetì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©
    if len(datasets_list) == 1:
        combined_dataset = datasets_list[0]
    else:
        logger.info(f"  Concatenating {len(datasets_list)} datasets...")
        combined_dataset = concatenate_datasets(datasets_list)
    
    logger.info(f"  Total: {len(combined_dataset):,} samples")
    
    dataset = {"train": combined_dataset}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Foundation Model pretrainìš©)
    
    íŠ¹ìˆ˜ í† í° ì—†ì´ ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ì´ê²ƒì€ Next Token Predictionì„ ìœ„í•œ Foundation Model í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"instruction": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"messages": [...]}: ëª¨ë“  ë©”ì‹œì§€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"conversations": [...]}: ëª¨ë“  ëŒ€í™” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    """
    # ì•ˆì „í•œ ë¬¸ìì—´ ì¶”ì¶œ í•¨ìˆ˜
    def safe_str(val) -> str:
        return (val or "").strip()
    
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item and item["text"]:
        return item["text"]
    
    # input/output í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "output" in item:
        inp = safe_str(item["input"])
        out = safe_str(item["output"])
        if not out:
            return None
        return f"{inp}\n\n{out}" if inp else out
    
    # instruction/output í¬ë§· (Alpaca) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "instruction" in item and "output" in item:
        inst = safe_str(item["instruction"])
        out = safe_str(item["output"])
        # input í•„ë“œë„ ìˆìœ¼ë©´ í•©ì¹¨
        inp = safe_str(item.get("input"))
        if inp:
            inst = f"{inst}\n{inp}" if inst else inp
        if not inst and not out:
            return None
        return f"{inst}\n\n{out}" if inst else out
    
    # messages í¬ë§· (OpenAI Chat) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "messages" in item and item["messages"]:
        texts = []
        for msg in item["messages"]:
            if msg:
                content = safe_str(msg.get("content"))
                if content:
                    texts.append(content)
        return "\n\n".join(texts) if texts else None
    
    # conversations í¬ë§· (ShareGPT) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "conversations" in item and item["conversations"]:
        texts = []
        for conv in item["conversations"]:
            if conv:
                value = safe_str(conv.get("value"))
                if value:
                    texts.append(value)
        return "\n\n".join(texts) if texts else None
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "content" in item:
        parts = []
        inp = safe_str(item.get("input"))
        reasoning = safe_str(item.get("reasoning_content"))
        content = safe_str(item.get("content"))
        if inp:
            parts.append(inp)
        if reasoning:
            parts.append(reasoning)
        if content:
            parts.append(content)
        return "\n\n".join(parts) if parts else None
    
    # prompt/response í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "response" in item:
        prompt = safe_str(item["prompt"])
        response = safe_str(item["response"])
        if not response:
            return None
        return f"{prompt}\n\n{response}" if prompt else response
    
    # question/answer í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "question" in item and "answer" in item:
        question = safe_str(item["question"])
        answer = safe_str(item["answer"])
        if not answer:
            return None
        return f"{question}\n\n{answer}" if question else answer
    
    # prompt/completion í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "completion" in item:
        prompt = safe_str(item["prompt"])
        completion = safe_str(item["completion"])
        if not completion:
            return None
        return f"{prompt}\n\n{completion}" if prompt else completion
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    import time
    tok_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        use_fast=True,  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì € ê°•ì œ ì‚¬ìš©
    )
    tok_time = time.time() - tok_start
    logger.info(f"âœ“ Tokenizer loaded in {tok_time:.1f}s")

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        logger.info(f"  (This may take 20-30s for 8 GPUs...)")
        model_start = time.time()
        model = MoaiForCausalLM.from_pretrained(pretrained_model, dtype=dtype)
        model_time = time.time() - model_start
        logger.info(f"âœ“ Model loaded in {model_time:.1f}s")
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

# ============================================================================
# Data Source Preparation
# ============================================================================
def prepare_data_sources(args) -> list:
    """
    argsì—ì„œ ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

    Args:
        args: í•™ìŠµ ì¸ì

    Returns:
        [("hf", "dataset_name"), ("file", "path.jsonl"), ...]
    """
    all_sources = []

    if args.dataset:
        for ds in args.dataset:
            all_sources.append(("hf", ds))

    if args.train_file:
        for f in args.train_file:
            all_sources.append(("file", f))

    return all_sources


def calculate_cache_paths(source: tuple, tokenizer, args, idx: int) -> tuple:
    """
    ìºì‹œ ê²½ë¡œ ê³„ì‚° (ëª¨ë“  rankì—ì„œ ë™ì¼í•˜ê²Œ ê³„ì‚°)

    Args:
        source: ("hf", "dataset_name") ë˜ëŠ” ("file", "path")
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì
        idx: ì†ŒìŠ¤ ì¸ë±ìŠ¤ (ìºì‹œ í‚¤ ìƒì„±ìš©)

    Returns:
        (src_name, tokenized_cache_path, tokenized_marker)
    """
    src_type, src_name = source
    cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))

    cache_version = get_cache_version_key(
        tokenizer,
        additional_info=f"packing_{args.packing}_maxlen_{args.max_seq_length}_seq_{idx}"
    )
    dataset_hash = hashlib.md5(f"{src_name}_{cache_version}".encode()).hexdigest()[:16]
    tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
    tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

    return src_name, tokenized_cache_path, tokenized_marker


def load_dataset_info_from_cache(
    source: tuple,
    tokenizer,
    args,
    idx: int,
) -> dict:
    """
    ìºì‹œì—ì„œ ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ (Non-main ranksìš©)

    Args:
        source: ("hf", "dataset_name") ë˜ëŠ” ("file", "path")
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì
        idx: ì†ŒìŠ¤ ì¸ë±ìŠ¤

    Returns:
        {'name': str, 'cache_path': Path, 'num_samples': int}
    """
    import gc
    from datasets import Dataset as HFDataset

    ddp_info = get_ddp_info()
    rank = ddp_info['rank']

    # ìºì‹œ ê²½ë¡œ ê³„ì‚°
    src_name, tokenized_cache_path, tokenized_marker = calculate_cache_paths(
        source, tokenizer, args, idx
    )

    # ë§ˆì»¤ íŒŒì¼ ëŒ€ê¸°
    logger.info(f"[Rank {rank}] Waiting for tokenization to complete...")
    if not wait_for_marker(tokenized_marker, TOKENIZATION_TIMEOUT, CHECK_INTERVAL, rank):
        raise TimeoutError(f"Rank {rank}: Tokenization timeout after {TOKENIZATION_TIMEOUT}s")

    logger.info(f"[Rank {rank}] âœ… Tokenization completed, loading info...")

    # ìƒ˜í”Œ ìˆ˜ í™•ì¸ì„ ìœ„í•´ dataset ë¡œë“œ
    tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
    num_samples = len(tokenized_dataset)
    del tokenized_dataset
    gc.collect()

    return {
        'name': src_name,
        'cache_path': tokenized_cache_path,
        'num_samples': num_samples,
    }


def tokenize_single_source(
    source: tuple,
    tokenizer,
    args,
    idx: int,
) -> dict:
    """
    ë‹¨ì¼ ë°ì´í„° ì†ŒìŠ¤ í† í¬ë‚˜ì´ì§• (Rank 0 ì „ìš©)

    Args:
        source: ("hf", "dataset_name") ë˜ëŠ” ("file", "path")
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì
        idx: ì†ŒìŠ¤ ì¸ë±ìŠ¤ (ìºì‹œ í‚¤ ìƒì„±ìš©)

    Returns:
        {'name': str, 'cache_path': Path, 'num_samples': int}
    """
    import gc
    from datasets import Dataset as HFDataset

    src_type, src_name = source

    # ìºì‹œ ê²½ë¡œ ê³„ì‚°
    src_name, tokenized_cache_path, tokenized_marker = calculate_cache_paths(
        source, tokenizer, args, idx
    )

    logger.info(f"ğŸ“¦ [{idx+1}] Dataset: {src_name}")

    # ìºì‹œ í™•ì¸
    if tokenized_cache_path.exists() and tokenized_marker.exists():
        logger.info(f"  âœ… Loading cached tokenized dataset")
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        num_samples = len(tokenized_dataset)
        logger.info(f"  âœ… Loaded {num_samples:,} samples from cache")
    else:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        logger.info(f"  ğŸ“š Loading dataset...")
        if src_type == "hf":
            dataset, text_column = load_pretrain_dataset(
                dataset_names=[src_name],
                dataset_config=args.dataset_config if idx == 0 else None,
                train_files=None,
                text_column=args.text_column,
            )
        else:
            dataset, text_column = load_pretrain_dataset(
                dataset_names=None,
                dataset_config=None,
                train_files=[src_name],
                text_column=args.text_column,
            )

        # í† í¬ë‚˜ì´ì§•
        logger.info(f"  ğŸ”¤ Tokenizing dataset...")
        tokenized_ds = tokenize_dataset(
            dataset=dataset["train"],
            tokenizer=tokenizer,
            text_column=text_column,
            max_seq_length=args.max_seq_length,
            packing=args.packing,
        )

        # Packing (ì„ íƒì ) - PyArrow ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ (íŒŒì¼ í˜ì´ì§•)
        if args.packing:
            import tempfile
            import shutil

            logger.info(f"  ğŸ“¦ Packing sequences (PyArrow streaming for memory efficiency)...")

            total_samples = len(tokenized_ds)
            logger.info(f"     Total samples: {total_samples:,}")

            # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ì— Arrow í¬ë§·ìœ¼ë¡œ ì €ì¥ (ë””ìŠ¤í¬ í˜ì´ì§• ì‹œì‘)
            temp_dir = Path(tempfile.mkdtemp(prefix="moai_packing_"))
            try:
                logger.info(f"     Saving to temporary Arrow files...")
                tokenized_ds.save_to_disk(str(temp_dir / "tokenized"))
                del tokenized_ds
                gc.collect()

                # 2. ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ ë‹¨ìœ„ packing
                STREAM_BATCH_SIZE = 500000  # 50ë§Œ ìƒ˜í”Œì”© ìŠ¤íŠ¸ë¦¬ë°
                all_packed_chunks = []

                # Arrow íŒŒì¼ì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ë¡œë“œ
                from datasets import load_from_disk
                dataset_on_disk = load_from_disk(str(temp_dir / "tokenized"))

                num_batches = (total_samples + STREAM_BATCH_SIZE - 1) // STREAM_BATCH_SIZE
                logger.info(f"     Processing {num_batches} batches of {STREAM_BATCH_SIZE:,} samples each")

                for batch_idx in range(num_batches):
                    start_idx = batch_idx * STREAM_BATCH_SIZE
                    end_idx = min(start_idx + STREAM_BATCH_SIZE, total_samples)

                    logger.info(f"     Batch {batch_idx+1}/{num_batches}: Loading {start_idx:,} - {end_idx:,}")

                    # Arrowì—ì„œ ë°°ì¹˜ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                    batch_data = dataset_on_disk.select(range(start_idx, end_idx))
                    tokenized_list = [{"input_ids": ids} for ids in batch_data["input_ids"]]
                    del batch_data
                    gc.collect()

                    logger.info(f"     Batch {batch_idx+1}/{num_batches}: Packing...")
                    packed_batch = concatenate_sequences(
                        tokenized_sequences=tokenized_list,
                        max_seq_length=args.max_seq_length,
                        eos_token_id=tokenizer.eos_token_id,
                    )
                    del tokenized_list
                    gc.collect()

                    all_packed_chunks.extend(packed_batch)
                    del packed_batch
                    gc.collect()

                    logger.info(f"     âœ“ Batch {batch_idx+1}/{num_batches} complete ({len(all_packed_chunks):,} chunks so far)")

                del dataset_on_disk
                gc.collect()

                logger.info(f"  âœ“ Total packed chunks: {len(all_packed_chunks):,}")
                tokenized_dataset = HFDataset.from_list(all_packed_chunks)
                del all_packed_chunks
                gc.collect()

            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                logger.info(f"     Cleaning up temporary files...")
                shutil.rmtree(temp_dir, ignore_errors=True)
        else:
            tokenized_dataset = tokenized_ds

        # ì €ì¥
        logger.info(f"  ğŸ’¾ Saving tokenized dataset...")
        num_shards = get_optimal_num_shards(len(tokenized_dataset), os.cpu_count() or 8)
        tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=num_shards)
        tokenized_marker.touch()
        num_samples = len(tokenized_dataset)
        logger.info(f"  âœ… Tokenized: {num_samples:,} samples (shards={num_shards})")

        # ë©”ëª¨ë¦¬ í•´ì œ
        del dataset
        del tokenized_dataset
        gc.collect()

    return {
        'name': src_name,
        'cache_path': tokenized_cache_path,
        'num_samples': num_samples,
    }


# ============================================================================
# Train Function (ë‹¨ì¼ ë°ì´í„°ì…‹ í•™ìŠµ)
# ============================================================================
def train_single_dataset(
    args,
    dataset_info: dict,
    tokenizer,
    pretrained_model_path: str = None,
    is_first_stage: bool = True,
    stage_name: str = None,
):
    """
    ë‹¨ì¼ ë°ì´í„°ì…‹ì— ëŒ€í•œ í•™ìŠµ ìˆ˜í–‰

    Args:
        args: í•™ìŠµ ì¸ì
        dataset_info: {'name': str, 'cache_path': Path, 'num_samples': int}
        tokenizer: í† í¬ë‚˜ì´ì €
        pretrained_model_path: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê²½ë¡œ (Sequential modeì—ì„œ ì´ì „ stage ì²´í¬í¬ì¸íŠ¸)
        is_first_stage: ì²« ë²ˆì§¸ stageì¸ì§€ ì—¬ë¶€ (warmup_steps ê²°ì •)
        stage_name: Stage ì´ë¦„ (ë¡œê¹…ìš©)

    Returns:
        checkpoint_path: ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    """
    from datasets import Dataset as HFDataset
    import gc

    ddp_info = get_ddp_info()
    is_main_process = ddp_info['is_main_process']

    if is_main_process:
        logger.info("="*80)
        if stage_name:
            logger.info(f"ğŸš€ Training: {stage_name}")
        else:
            logger.info(f"ğŸš€ Training: {dataset_info['name']}")
        logger.info(f"   Samples: {dataset_info['num_samples']:,}")
        logger.info("="*80)

    # ============================================================================
    # 1. ëª¨ë¸ ë¡œë“œ
    # ============================================================================
    if is_main_process:
        logger.info(f"â³ Loading model from: {pretrained_model_path or 'scratch'}")

    model, _ = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=pretrained_model_path,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )
    if is_main_process:
        logger.info(f"âœ“ Model loaded")

    # ============================================================================
    # 2. í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ ë¡œë“œ (ëª¨ë“  rank)
    # ============================================================================
    if is_main_process:
        logger.info(f"ğŸ“¥ Loading tokenized dataset from cache...")

    tokenized_dataset = HFDataset.load_from_disk(str(dataset_info['cache_path']))

    if is_main_process:
        logger.info(f"âœ“ Loaded {len(tokenized_dataset):,} samples")

    # ============================================================================
    # 3. Data Collator
    # ============================================================================
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    # ============================================================================
    # 4. Training Arguments
    # ============================================================================
    output_dir = args.output_dir if stage_name is None else f"{args.output_dir}/{stage_name}"
    optimal_prefetch = get_optimal_prefetch_factor(batch_size=args.batch_size)

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=WARMUP_STEPS_FIRST_STAGE if is_first_stage else WARMUP_STEPS_RESUME,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=2,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to=["wandb"] if args.use_wandb else ["tensorboard"],
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        save_safetensors=True,
        # I/O ìµœì í™”
        dataloader_pin_memory=True,
        dataloader_prefetch_factor=optimal_prefetch,
        dataloader_persistent_workers=True,
        dataloader_drop_last=True,
        # ì˜µí‹°ë§ˆì´ì € ë° ì •ë°€ë„
        optim="adamw_torch_fused",
        ddp_find_unused_parameters=False,
        tf32=True,
        # ë°°ì¹˜ ìµœì í™”
        group_by_length=not getattr(args, 'packing', False),
        max_grad_norm=1.0,
        gradient_checkpointing_kwargs={"use_reentrant": False} if args.gradient_checkpointing else None,
    )

    # ============================================================================
    # 5. Trainer ìƒì„± ë° í•™ìŠµ
    # ============================================================================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    if is_main_process:
        logger.info(f"ğŸƒ Starting training...")
    trainer.train()

    # ============================================================================
    # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    # ============================================================================
    checkpoint_path = f"{output_dir}/checkpoint"
    trainer.save_model(checkpoint_path)

    # DDP barrier
    if torch.distributed.is_initialized():
        torch.distributed.barrier()

    # dtype ìœ ì§€í•˜ë©° ì €ì¥ (rank 0ë§Œ)
    if is_main_process:
        model_dtype = next(model.parameters()).dtype
        if model_dtype in (torch.bfloat16, torch.float16):
            logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format...")
            model.save_pretrained(checkpoint_path, torch_dtype=model_dtype, safe_serialization=True)
        logger.info(f"âœ… Training completed: {checkpoint_path}")

    # ============================================================================
    # 7. ë©”ëª¨ë¦¬ í•´ì œ
    # ============================================================================
    del model
    del tokenized_dataset
    del trainer
    gc.collect()

    try:
        torch.cuda.empty_cache()
    except:
        pass

    # DDP barrier
    if torch.distributed.is_initialized():
        torch.distributed.barrier()

    return checkpoint_path


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # í•„ìˆ˜ ì¸ì
    parser.add_argument("--mode", type=str, required=True, choices=["pretrain", "sft"], help="Training mode")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Path to tokenizer")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for model checkpoints")

    # ë°ì´í„°ì…‹ (ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë˜ëŠ” ì—¬ëŸ¬ íŒŒì¼ ì§€ì›)
    parser.add_argument("--dataset", type=str, nargs="*", help="HuggingFace dataset names")
    parser.add_argument("--dataset_config", type=str, default=None, help="Dataset configuration name")
    parser.add_argument("--train_file", type=str, nargs="*", help="Training data files (JSONL, JSON, Parquet, etc.)")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name for dataset")

    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_config", type=str, help="Model config file (JSON)")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )
    
    # Tokenize only ëª¨ë“œ (DDP ì „ì— í† í°í™”ë§Œ ìˆ˜í–‰)
    parser.add_argument(
        "--tokenize_only",
        action="store_true",
        help="Only tokenize datasets and exit (no training). "
             "Use this to pre-tokenize before running torchrun."
    )

    # Skip tokenization ëª¨ë“œ (ì´ë¯¸ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©)
    parser.add_argument(
        "--skip_tokenization",
        action="store_true",
        help="Skip tokenization and load pre-tokenized datasets from cache. "
             "Use this when you've already run tokenize_datasets.py."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=48, help="Number of processes for tokenization (default: 48 for high-performance CPUs)")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )
    
    # Logging ì˜µì…˜
    parser.add_argument(
        "--use_wandb",
        action="store_true",
        help="Use Weights & Biases for logging (default: tensorboard)"
    )
    parser.add_argument(
        "--wandb_project",
        type=str,
        default="moai-llm",
        help="W&B project name (default: moai-llm)"
    )
    parser.add_argument(
        "--wandb_run_name",
        type=str,
        default=None,
        help="W&B run name (default: auto-generated)"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # DDP í™˜ê²½ ì •ë³´
    ddp_info = get_ddp_info()
    rank = ddp_info['rank']
    world_size = ddp_info['world_size']
    is_main_process = ddp_info['is_main_process']

    if is_main_process:
        logger.info("="*80)
        logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
        logger.info(f"ğŸŒ Environment: {world_size} GPU(s), Rank {rank}")
        logger.info("="*80)

    # ============================================================================
    # STEP 1: í† í¬ë‚˜ì´ì € ë¡œë“œ
    # ============================================================================
    if is_main_process:
        logger.info("ğŸ“ Loading tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer_path,
        use_fast=True,
    )

    # Fast Tokenizer ê°•ì œ ì²´í¬
    if not tokenizer.is_fast:
        raise ValueError(
            f"âŒ Fast Tokenizer not available! "
            f"Current tokenizer: {type(tokenizer).__name__}\n"
            f"Please ensure you're using a tokenizer that supports Fast mode (Rust-based)."
        )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    if is_main_process:
        logger.info("âœ… Using Fast Tokenizer (Rust-based)")
        logger.info(f"   Tokenizer type: {type(tokenizer).__name__}")

        # Warmup: Rust í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        logger.info("   Warming up tokenizer...")
        warmup_start = time.time()
        for _ in range(WARMUP_TEXT_COUNT):
            _ = tokenizer(WARMUP_TEXT_PATTERN, truncation=False, padding=False)
        warmup_time = time.time() - warmup_start
        logger.info(f"   Warmup completed in {warmup_time:.2f}s")

    # ============================================================================
    # STEP 2: ë°ì´í„° ì†ŒìŠ¤ ì¤€ë¹„
    # ============================================================================
    all_sources = prepare_data_sources(args)

    if is_main_process:
        logger.info(f"ğŸ“‹ Total datasets: {len(all_sources)}")
        for i, (src_type, src_name) in enumerate(all_sources):
            logger.info(f"  {i+1}. [{src_type}] {src_name}")

    # Tokenize-only ëª¨ë“œ ì²´í¬ (early return)
    if args.tokenize_only:
        if is_main_process:
            logger.info("="*80)
            logger.info("ğŸ”¥ Tokenize-Only Mode")
            logger.info("="*80)

        # ëª¨ë“  ì†ŒìŠ¤ í† í¬ë‚˜ì´ì§•
        for idx, source in enumerate(all_sources):
            # Rank 0ë§Œ í† í¬ë‚˜ì´ì§• ì‹¤í–‰
            if is_main_process:
                dataset_info = tokenize_single_source(source, tokenizer, args, idx)

            # ëª¨ë“  rank ë™ê¸°í™”
            ddp_barrier()

            # Non-main ranksëŠ” ìºì‹œì—ì„œ ì •ë³´ ë¡œë“œ
            if not is_main_process:
                dataset_info = load_dataset_info_from_cache(source, tokenizer, args, idx)

        if is_main_process:
            logger.info("="*80)
            logger.info("âœ… All datasets tokenized!")
            logger.info("="*80)
        return

    # ============================================================================
    # STEP 3: W&B ì´ˆê¸°í™” (ì„ íƒì )
    # ============================================================================
    if args.use_wandb:
        try:
            import wandb
            if is_main_process:
                wandb.init(
                    project=args.wandb_project,
                    name=args.wandb_run_name,
                    config=vars(args),
                )
                logger.info(f"ğŸ“Š W&B initialized: {args.wandb_project}")
        except ImportError:
            logger.warning("âš ï¸ wandb not installed")
            args.use_wandb = False

    # ============================================================================
    # STEP 4: ê° ì†ŒìŠ¤ë§ˆë‹¤ í† í¬ë‚˜ì´ì§• â†’ í•™ìŠµ ë°˜ë³µ
    # ============================================================================
    if is_main_process:
        logger.info("="*80)
        logger.info("ğŸ¯ Starting Training Pipeline")
        logger.info(f"   Total stages: {len(all_sources)}")
        logger.info("="*80)

    current_checkpoint = args.pretrained_model

    for idx, source in enumerate(all_sources):
        if is_main_process:
            logger.info("")
            logger.info("="*80)
            logger.info(f"ğŸ“Š Stage {idx+1}/{len(all_sources)}: {source[1]}")
            logger.info("="*80)

        # 1. ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ (í† í¬ë‚˜ì´ì§• ë˜ëŠ” ìºì‹œ)
        if args.skip_tokenization:
            # Skip tokenization ëª¨ë“œ: ëª¨ë“  rankê°€ ìºì‹œì—ì„œ ì§ì ‘ ë¡œë“œ
            if is_main_process:
                logger.info("ğŸ“¥ Step 1: Loading from cache (skip_tokenization=True)...")

            # ìºì‹œì—ì„œ ì •ë³´ ë¡œë“œ (barrier ì—†ì´ ê° rankê°€ ë…ë¦½ì ìœ¼ë¡œ ë¡œë“œ)
            src_name, tokenized_cache_path, tokenized_marker = calculate_cache_paths(
                source, tokenizer, args, idx
            )

            # ë§ˆì»¤ íŒŒì¼ í™•ì¸
            if not tokenized_marker.exists():
                if is_main_process:
                    logger.error(f"âŒ Tokenized cache not found for {src_name}")
                    logger.error(f"   Expected marker: {tokenized_marker}")
                    logger.error(f"   Please run tokenize_datasets.py first!")
                raise FileNotFoundError(f"Tokenized cache not found for {src_name}")

            # ìƒ˜í”Œ ìˆ˜ í™•ì¸ì„ ìœ„í•´ dataset ë¡œë“œ
            from datasets import Dataset as HFDataset
            import gc
            tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
            num_samples = len(tokenized_dataset)
            del tokenized_dataset
            gc.collect()

            dataset_info = {
                'name': src_name,
                'cache_path': tokenized_cache_path,
                'num_samples': num_samples,
            }

            if is_main_process:
                logger.info(f"âœ… Loaded from cache: {num_samples:,} samples")
        else:
            # ì¼ë°˜ ëª¨ë“œ: Rank 0ì´ í† í¬ë‚˜ì´ì§•, ë‹¤ë¥¸ ranksëŠ” ëŒ€ê¸° í›„ ë¡œë“œ
            if is_main_process:
                logger.info("ğŸ”¤ Step 1: Tokenizing...")
                dataset_info = tokenize_single_source(source, tokenizer, args, idx)

            # Barrier (ëª¨ë“  rank ë™ê¸°í™”)
            if is_main_process:
                logger.info("â³ Step 2: Synchronizing all ranks...")
            ddp_barrier()
            if is_main_process:
                logger.info("âœ… All ranks synchronized!")

            # Non-main ranksëŠ” ìºì‹œì—ì„œ ì •ë³´ ë¡œë“œ
            if not is_main_process:
                dataset_info = load_dataset_info_from_cache(source, tokenizer, args, idx)

        # 2. í•™ìŠµ (ëª¨ë“  rank)
        if is_main_process:
            step_num = 2 if args.skip_tokenization else 4
            logger.info(f"ğŸ‹ï¸ Step {step_num}: Training...")
        checkpoint_path = train_single_dataset(
            args=args,
            dataset_info=dataset_info,
            tokenizer=tokenizer,
            pretrained_model_path=current_checkpoint,
            is_first_stage=(idx == 0),
            stage_name=f"stage_{idx+1}",
        )

        # ë‹¤ìŒ stageë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path

    # ============================================================================
    # STEP 5: ìµœì¢… ì™„ë£Œ
    # ============================================================================
    if is_main_process:
        logger.info("")
        logger.info("="*80)
        logger.info("ğŸ‰ All training completed!")
        logger.info(f"ğŸ“ Final model: {current_checkpoint}")
        logger.info(f"ğŸ“Š Trained on {len(all_sources)} datasets")
        logger.info("="*80)


if __name__ == "__main__":
    # ê°€ì¥ ë¨¼ì € ì¶œë ¥ (torchrunì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œëŒ€ë¡œ ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸)
    import sys
    import os
    
    # ì¦‰ì‹œ ì¶œë ¥
    rank = int(os.environ.get(ENV_RANK, -1))
    print(f"[INIT] Rank {rank}: Python script started!", flush=True)

    world_size = int(os.environ.get(ENV_WORLD_SIZE, 1))
    
    if rank == 0:
        print("="*80, flush=True)
        print("ğŸš€ MOAI-LLM Training Starting...", flush=True)
        print(f"ğŸŒ World size: {world_size} GPUs", flush=True)
        print("="*80, flush=True)

    main()

