"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

import argparse
import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import json

import torch
from transformers import (
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from datasets import load_dataset

from moai_llm.config import MoaiConfig
from moai_llm.modeling.model import MoaiForCausalLM

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def load_pretrain_dataset(
    dataset_name: Optional[str] = None,
    dataset_config: Optional[str] = None,
    train_file: Optional[str] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ

    Args:
        dataset_name: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ (ì˜ˆ: "wikipedia")
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì˜ˆ: "20220301.en")
        train_file: ë¡œì»¬ txt íŒŒì¼ ê²½ë¡œ
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    """
    logger.info("ğŸ“š Loading pretrain dataset...")

    if train_file:
        # ë¡œì»¬ txt íŒŒì¼
        logger.info(f"  From local file: {train_file}")
        dataset = load_dataset("text", data_files={"train": train_file})
        text_column = "text"
    elif dataset_name:
        # HuggingFace ë°ì´í„°ì…‹
        logger.info(f"  From HuggingFace: {dataset_name}")
        if dataset_config:
            logger.info(f"  Config: {dataset_config}")
            dataset = load_dataset(dataset_name, dataset_config, trust_remote_code=True)
        else:
            dataset = load_dataset(dataset_name, trust_remote_code=True)
    else:
        raise ValueError("Either dataset_name or train_file must be provided")

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def load_sft_dataset(
    dataset_name: Optional[str] = None,
    train_file: Optional[str] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")

    if train_file:
        # ë¡œì»¬ JSON íŒŒì¼
        logger.info(f"  From local file: {train_file}")
        with open(train_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # í¬ë§· ìë™ ê°ì§€ ë° ë³€í™˜
        formatted_data = []

        for item in data:
            # input/output í¬ë§· (BCCard ë“±)
            if "input" in item and "output" in item:
                text = f"<|im_start|>user\n{item['input']}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
                formatted_data.append({"text": text})

            # Alpaca í¬ë§·
            elif "instruction" in item and "output" in item:
                text = f"<|im_start|>user\n{item['instruction']}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
                formatted_data.append({"text": text})

            # Chat í¬ë§·
            elif "messages" in item:
                text = ""
                for msg in item["messages"]:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
                formatted_data.append({"text": text})

            # ShareGPT í¬ë§·
            elif "conversations" in item:
                text = ""
                for conv in item["conversations"]:
                    role = "user" if conv["from"] == "human" else "assistant"
                    text += f"<|im_start|>{role}\n{conv['value']}<|im_end|>\n"
                formatted_data.append({"text": text})

        # Datasetìœ¼ë¡œ ë³€í™˜
        from datasets import Dataset
        dataset = {"train": Dataset.from_list(formatted_data)}

    elif dataset_name:
        # HuggingFace ë°ì´í„°ì…‹ (ìë™ ë³€í™˜)
        logger.info(f"  From HuggingFace: {dataset_name}")
        raw_dataset = load_dataset(dataset_name, trust_remote_code=True)

        # í¬ë§· ë³€í™˜
        formatted_data = []
        for item in raw_dataset["train"]:
            # input/output í¬ë§·
            if "input" in item and "output" in item:
                text = f"<|im_start|>user\n{item['input']}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
                formatted_data.append({"text": text})

            # instruction/output í¬ë§·
            elif "instruction" in item and "output" in item:
                text = f"<|im_start|>user\n{item['instruction']}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{item['output']}<|im_end|>"
                formatted_data.append({"text": text})

            # messages í¬ë§·
            elif "messages" in item:
                text = ""
                for msg in item["messages"]:
                    role = msg["role"]
                    content = msg["content"]
                    text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
                formatted_data.append({"text": text})

            # conversations í¬ë§·
            elif "conversations" in item:
                text = ""
                for conv in item["conversations"]:
                    role = "user" if conv["from"] == "human" else "assistant"
                    text += f"<|im_start|>{role}\n{conv['value']}<|im_end|>\n"
                formatted_data.append({"text": text})

        # Datasetìœ¼ë¡œ ë³€í™˜
        from datasets import Dataset
        dataset = {"train": Dataset.from_list(formatted_data)}
    else:
        raise ValueError("Either dataset_name or train_file must be provided")

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        model = MoaiForCausalLM.from_pretrained(pretrained_model)
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        model = MoaiForCausalLM(config)

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""

    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info("="*80)

    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
    )

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    if args.mode == "pretrain":
        dataset, text_column = load_pretrain_dataset(
            dataset_name=args.dataset,
            dataset_config=args.dataset_config,
            train_file=args.train_file,
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_name=args.dataset,
            train_file=args.train_file,
        )

    # 3. í† í°í™”
    logger.info("ğŸ”¤ Tokenizing dataset...")

    def tokenize_function(examples):
        return tokenizer(
            examples[text_column],
            truncation=True,
            max_length=args.max_seq_length,
            padding=False,
            return_special_tokens_mask=True,
        )

    tokenized_dataset = dataset["train"].map(
        tokenize_function,
        batched=True,
        num_proc=args.num_proc,
        remove_columns=dataset["train"].column_names,
        desc="Tokenizing",
    )

    logger.info(f"âœ“ Tokenized {len(tokenized_dataset)} samples")

    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to="none",
        max_steps=args.max_steps if args.max_steps > 0 else -1,
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train()

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))

    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info("="*80)


# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # ëª¨ë“œ
    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["pretrain", "sft"],
        help="Training mode: pretrain or sft"
    )

    # ë°ì´í„°
    parser.add_argument("--dataset", type=str, help="HuggingFace dataset name")
    parser.add_argument("--dataset_config", type=str, help="Dataset config/subset")
    parser.add_argument("--train_file", type=str, help="Local train file (txt or json)")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")

    # ëª¨ë¸
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/",
        help="Tokenizer path"
    )
    parser.add_argument("--model_config", type=str, help="Model config JSON file")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path (for SFT)")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size per device")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=4, help="Number of processes for tokenization")
    parser.add_argument("--dataloader_num_workers", type=int, default=2)

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    main()
