"""
MOAI-LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í†µí•© ë²„ì „)

ì‚¬ì „í•™ìŠµê³¼ SFTë¥¼ í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:

python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --model_config configs/model_config_2b.json \
    --output_dir outputs/pretrain-korean-2b \
    --batch_size 4 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-6 \
    --max_seq_length 2048 \
    --bf16 \
    --gradient_checkpointing

    
    # ì‚¬ì „í•™ìŠµ - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode pretrain \
        --dataset wikipedia \
        --dataset_config 20220301.en \
        --output_dir outputs/pretrain

    # ì‚¬ì „í•™ìŠµ - ë¡œì»¬ txt íŒŒì¼
    python train.py \
        --mode pretrain \
        --train_file data/pretrain/train.txt \
        --output_dir outputs/pretrain

    # SFT - HuggingFace ë°ì´í„°ì…‹
    python train.py \
        --mode sft \
        --dataset tatsu-lab/alpaca \
        --output_dir outputs/sft

    # SFT - ë¡œì»¬ JSON íŒŒì¼
    python train.py \
        --mode sft \
        --train_file data/sft/alpaca.json \
        --output_dir outputs/sft
"""

# ============================================================================
# Constants and Configuration
# ============================================================================

# Timeout settings (seconds)
IMPORT_SYNC_TIMEOUT = 300  # 5 minutes for import synchronization
TOKENIZATION_TIMEOUT = 7200  # 2 hours for tokenization
DATASET_PROCESSING_TIMEOUT = 3600  # 1 hour for dataset processing
CHECK_INTERVAL = 5  # seconds between checks

# Dataset size thresholds
DATASET_SIZE_LARGE = 5_000_000  # 5M+ samples
DATASET_SIZE_MEDIUM = 1_000_000  # 1M-5M samples

# Tokenization settings
MIN_CHUNK_LENGTH = 128  # Minimum tokens per chunk
WARMUP_STEPS_FIRST_STAGE = 2000  # For first training stage
WARMUP_STEPS_RESUME = 100  # For resumed training

# Default batch sizes (Rust Fast Tokenizer ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ìµœì í™”)
BATCH_SIZE_LARGE_DATASET = 5000  # ëŒ€ê·œëª¨: Rust ì„±ëŠ¥ ìµœëŒ€ í™œìš©
BATCH_SIZE_DEFAULT = 10000  # ê¸°ë³¸: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ í° ë°°ì¹˜
WRITER_BATCH_SIZE = 50000  # ë””ìŠ¤í¬ ì“°ê¸° ë°°ì¹˜

# Default process counts (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ë©”ëª¨ë¦¬ ì•ˆì •ì„± + Rust ì†ë„)
DEFAULT_NUM_PROC = 1  # Rust Fast TokenizerëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì¥ ë¹ ë¦„
FILTER_NUM_PROC_DIVISOR = 1  # í•„í„°ë§ë„ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
MAX_FILTER_NUM_PROC = 1  # ë©”ëª¨ë¦¬ ì•ˆì •ì„±

# Performance settings
ESTIMATED_TOKENIZATION_SPEED = 5000  # samples/sec (Rust Fast Tokenizer ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)
WARMUP_TEXT_PATTERN = "Hello world " * 100
WARMUP_TEXT_COUNT = 10

# Environment variable keys
ENV_RANK = "RANK"
ENV_WORLD_SIZE = "WORLD_SIZE"
ENV_LOCAL_RANK = "LOCAL_RANK"
ENV_HF_HOME = "HF_HOME"
ENV_XDG_CACHE_HOME = "XDG_CACHE_HOME"
ENV_DATASET_NUM_PROC = "DATASET_NUM_PROC"
ENV_DATASET_BATCH_SIZE = "DATASET_BATCH_SIZE"
ENV_DATASET_WRITER_BATCH_SIZE = "DATASET_WRITER_BATCH_SIZE"
ENV_TOKENIZERS_PARALLELISM = "TOKENIZERS_PARALLELISM"


# ============================================================================
# Early initialization
# ============================================================================

import os
import sys
import time as time_module
from pathlib import Path as PathType

# Check rank early
rank = int(os.environ.get(ENV_RANK, 0))
world_size = int(os.environ.get(ENV_WORLD_SIZE, 1))
is_main = (rank == 0)

# ë™ê¸°í™” ë§ˆì»¤ íŒŒì¼
import_marker = PathType("/tmp/.moai_import_done")


def _import_all_modules():
    """ê³µí†µ import ë¡œì§ (ì¤‘ë³µ ì œê±°)"""
    import argparse
    import hashlib
    import time
    import gc
    import logging
    from pathlib import Path
    from typing import Optional, Dict, Any

    try:
        import orjson as json
    except ImportError:
        import json

    try:
        import psutil
    except ImportError:
        psutil = None

    import torch
    from transformers import (
        AutoTokenizer,
        Trainer,
        TrainingArguments,
        DataCollatorForLanguageModeling,
    )
    from datasets import load_dataset, disable_caching
    import datasets
    datasets.config.IN_MEMORY_MAX_SIZE = 0
    from moai_llm.config import MoaiConfig
    from moai_llm.modeling.model import MoaiForCausalLM

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
    logger = logging.getLogger(__name__)

    # Return all imports as a dict for global namespace injection
    return {
        'argparse': argparse, 'hashlib': hashlib, 'time': time, 'gc': gc,
        'logging': logging, 'Path': Path, 'Optional': Optional, 'Dict': Dict, 'Any': Any,
        'json': json, 'psutil': psutil, 'torch': torch,
        'AutoTokenizer': AutoTokenizer, 'Trainer': Trainer,
        'TrainingArguments': TrainingArguments,
        'DataCollatorForLanguageModeling': DataCollatorForLanguageModeling,
        'load_dataset': load_dataset, 'disable_caching': disable_caching,
        'datasets': datasets, 'MoaiConfig': MoaiConfig,
        'MoaiForCausalLM': MoaiForCausalLM, 'logger': logger,
    }


# Import synchronization
if is_main:
    # Rank 0: ë¨¼ì € import
    print(f"[IMPORT] Rank 0: Importing modules (world_size={world_size})...", flush=True)

    # ì´ì „ ë§ˆì»¤ ì œê±°
    if import_marker.exists():
        import_marker.unlink()

    _modules = _import_all_modules()
    globals().update(_modules)

    # ë§ˆì»¤ ìƒì„± (ë‹¤ë¥¸ rankë“¤ì´ import ì‹œì‘ ê°€ëŠ¥)
    import_marker.touch()
    print(f"[IMPORT] Rank 0: âœ… All modules imported!", flush=True)
else:
    # ë‹¤ë¥¸ rankë“¤: ë§ˆì»¤ ëŒ€ê¸°
    print(f"[IMPORT] Rank {rank}: Waiting for rank 0...", flush=True)

    waited = 0
    while not import_marker.exists() and waited < IMPORT_SYNC_TIMEOUT:
        time_module.sleep(0.5)
        waited += 0.5

    if not import_marker.exists():
        print(f"[IMPORT] Rank {rank}: Timeout waiting for rank 0!", flush=True)
        sys.exit(1)

    # ì´ì œ ì•ˆì „í•˜ê²Œ import
    _modules = _import_all_modules()
    globals().update(_modules)

    print(f"[IMPORT] Rank {rank}: âœ… Modules imported!", flush=True)


# ============================================================================
# Utility Functions
# ============================================================================

def get_ddp_info() -> Dict[str, Any]:
    """
    DDP í™˜ê²½ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict: rank, world_size, is_distributed, is_main_process í¬í•¨
    """
    rank = int(os.environ.get(ENV_RANK, -1))
    world_size = int(os.environ.get(ENV_WORLD_SIZE, -1))

    is_distributed = rank >= 0 and world_size > 1

    # torch.distributedë¡œ ë‹¤ì‹œ í™•ì¸
    if not is_distributed and torch.distributed.is_available():
        try:
            if torch.distributed.is_initialized():
                rank = torch.distributed.get_rank()
                world_size = torch.distributed.get_world_size()
                is_distributed = True
        except (RuntimeError, ValueError, AttributeError):
            pass

    return {
        'rank': rank if rank >= 0 else 0,
        'world_size': world_size if world_size > 0 else 1,
        'is_distributed': is_distributed,
        'is_main_process': rank == 0 if rank >= 0 else True,
    }


def ddp_barrier():
    """ì•ˆì „í•œ DDP barrier í˜¸ì¶œ"""
    try:
        if torch.distributed.is_initialized():
            torch.distributed.barrier()
    except (RuntimeError, ValueError, AttributeError):
        pass


def get_cache_home() -> str:
    """ìºì‹œ í™ˆ ë””ë ‰í† ë¦¬ ë°˜í™˜"""
    return os.environ.get(
        ENV_HF_HOME,
        os.environ.get(ENV_XDG_CACHE_HOME, os.path.expanduser("~/.cache/huggingface"))
    )


def create_cache_path(name: str, suffix: str = "") -> Path:
    """
    ìºì‹œ ê²½ë¡œ ìƒì„±

    Args:
        name: ë°ì´í„°ì…‹ ì´ë¦„ ë˜ëŠ” ì‹ë³„ì
        suffix: ê²½ë¡œ ì ‘ë¯¸ì‚¬ (ì˜ˆ: "_tokenized", "_final")

    Returns:
        Path: ìºì‹œ ê²½ë¡œ
    """
    cache_hash = hashlib.md5(name.encode()).hexdigest()[:16]
    cache_home = get_cache_home()
    return Path(cache_home) / "datasets" / f"{cache_hash}{suffix}"


def wait_for_marker(marker_path: Path, timeout: int = TOKENIZATION_TIMEOUT,
                   check_interval: int = CHECK_INTERVAL, rank: int = 0) -> bool:
    """
    ë§ˆì»¤ íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°

    Args:
        marker_path: ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ
        timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        check_interval: ì²´í¬ ê°„ê²© (ì´ˆ)
        rank: í˜„ì¬ rank (ë¡œê¹…ìš©)

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    import time
    waited = 0
    while not marker_path.exists() and waited < timeout:
        time.sleep(check_interval)
        waited += check_interval
        if waited % 60 == 0:  # 1ë¶„ë§ˆë‹¤ ë¡œê·¸
            logger.info(f"[Rank {rank}] Still waiting... ({waited}s elapsed)")

    return marker_path.exists()


def log_with_rank(msg: str, rank: int = None, is_main: bool = None):
    """
    Rank ì •ë³´ì™€ í•¨ê»˜ ë¡œê¹…

    Args:
        msg: ë¡œê·¸ ë©”ì‹œì§€
        rank: Rank ë²ˆí˜¸ (Noneì´ë©´ ìë™ ê°ì§€)
        is_main: Main process ì—¬ë¶€ (Noneì´ë©´ ìë™ ê°ì§€)
    """
    if rank is None or is_main is None:
        ddp_info = get_ddp_info()
        rank = ddp_info['rank']
        is_main = ddp_info['is_main_process']

    if is_main:
        logger.info(msg)
    else:
        logger.info(f"[Rank {rank}] {msg}")


def calculate_optimal_num_proc(total_samples: int, cpu_count: int, available_memory: int = None) -> int:
    """
    CPU, ë©”ëª¨ë¦¬, ë°ì´í„° í¬ê¸°ë¥¼ ê³ ë ¤í•œ ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³„ì‚°

    Args:
        total_samples: ì´ ìƒ˜í”Œ ìˆ˜
        cpu_count: CPU ì½”ì–´ ìˆ˜
        available_memory: ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (bytes), Noneì´ë©´ ìë™ ê°ì§€

    Returns:
        ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜
    """
    # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì œí•œ
    if available_memory is None:
        if psutil:
            available_memory = psutil.virtual_memory().available
        else:
            available_memory = 16 * 1024**3  # 16GB ê¸°ë³¸ê°’

    # Rust Fast TokenizerëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì¥ íš¨ìœ¨ì 
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë²„í—¤ë“œ > Rust ë³‘ë ¬ ì²˜ë¦¬ ì´ì 
    # ë©”ëª¨ë¦¬ ì•ˆì •ì„±ë„ ìµœê³ 
    return 1


def get_tokenization_env_config() -> Dict[str, int]:
    """í† í¬ë‚˜ì´ì œì´ì…˜ ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë°˜í™˜"""
    cpu_count = os.cpu_count() or 8
    return {
        'num_proc': int(os.getenv(ENV_DATASET_NUM_PROC, min(DEFAULT_NUM_PROC, cpu_count))),
        'batch_size': int(os.getenv(ENV_DATASET_BATCH_SIZE, BATCH_SIZE_DEFAULT)),
        'writer_batch_size': int(os.getenv(ENV_DATASET_WRITER_BATCH_SIZE, WRITER_BATCH_SIZE)),
    }


def get_optimal_num_shards(dataset_size: int, cpu_count: int) -> int:
    """
    ë°ì´í„°ì…‹ í¬ê¸°ì™€ CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìµœì  shard ìˆ˜ ê³„ì‚°

    Args:
        dataset_size: ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜
        cpu_count: CPU ì½”ì–´ ìˆ˜

    Returns:
        ìµœì  shard ìˆ˜ (8-64 ì‚¬ì´)
    """
    # CPU ì½”ì–´ì˜ ì ˆë°˜ì„ ê¸°ì¤€ìœ¼ë¡œ, ìµœì†Œ 8, ìµœëŒ€ 64
    base_shards = max(8, cpu_count // 2)

    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì€ ë” ë§ì€ shard ì‚¬ìš©
    if dataset_size > DATASET_SIZE_LARGE:
        return min(64, base_shards * 2)
    elif dataset_size > DATASET_SIZE_MEDIUM:
        return min(48, base_shards)
    else:
        return min(32, base_shards)


def get_optimal_prefetch_factor(gpu_memory_gb: float = None, batch_size: int = 4) -> int:
    """
    GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  prefetch factor ê³„ì‚°

    Args:
        gpu_memory_gb: GPU ë©”ëª¨ë¦¬ í¬ê¸° (GB), Noneì´ë©´ ìë™ ê°ì§€
        batch_size: ë°°ì¹˜ í¬ê¸°

    Returns:
        ìµœì  prefetch factor (2-8 ì‚¬ì´)
    """
    if gpu_memory_gb is None:
        try:
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            else:
                gpu_memory_gb = 16  # CPU ëª¨ë“œ ê¸°ë³¸ê°’
        except:
            gpu_memory_gb = 16

    # GPU ë©”ëª¨ë¦¬ í¬ê¸°ì— ë”°ë¥¸ ìµœì  prefetch
    # 40GB+ GPU (A100): 8, 24GB (RTX 3090/4090): 6, 16GB: 4, ê·¸ ì™¸: 2
    if gpu_memory_gb >= 40:
        return 8
    elif gpu_memory_gb >= 24:
        return 6
    elif gpu_memory_gb >= 16:
        return 4
    else:
        return 2


def load_files_parallel(file_paths: list, max_workers: int = 8) -> list:
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¡œë“œ

    Args:
        file_paths: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ worker ìˆ˜

    Returns:
        ë¡œë“œëœ Dataset ë¦¬ìŠ¤íŠ¸
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datasets import Dataset

    if not file_paths:
        return []

    # ë‹¨ì¼ íŒŒì¼ì€ ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆí•„ìš”
    if len(file_paths) == 1:
        logger.info(f"  Loading file: {file_paths[0]}")
        file_data = _load_single_file(file_paths[0])
        logger.info(f"    â†’ {len(file_data):,} samples")
        return [Dataset.from_list(file_data)]

    # ë³‘ë ¬ ë¡œë”©
    logger.info(f"ğŸš€ Loading {len(file_paths)} files in parallel (workers={max_workers})...")
    datasets_list = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # íŒŒì¼ ë¡œë”© ì‘ì—… ì œì¶œ
        future_to_file = {executor.submit(_load_single_file, f): f for f in file_paths}

        # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                file_data = future.result()
                logger.info(f"  âœ“ Loaded {file_path}: {len(file_data):,} samples")
                datasets_list.append(Dataset.from_list(file_data))
            except Exception as e:
                logger.error(f"  âœ— Failed to load {file_path}: {e}")

    logger.info(f"âœ… Loaded {len(datasets_list)}/{len(file_paths)} files successfully")
    return datasets_list


def get_cache_version_key(tokenizer, additional_info: str = "") -> str:
    """
    í† í¬ë‚˜ì´ì € ë²„ì „ì„ í¬í•¨í•œ ìºì‹œ ë²„ì „ í‚¤ ìƒì„±

    Args:
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        additional_info: ì¶”ê°€ ì •ë³´ (ì˜ˆ: ì„¤ì •ê°’)

    Returns:
        ìºì‹œ ë²„ì „ í‚¤ (8ìë¦¬ í•´ì‹œ)
    """
    # í† í¬ë‚˜ì´ì € ë²„ì „ ì •ë³´ ìˆ˜ì§‘
    version_info = []

    # 1. í† í¬ë‚˜ì´ì € vocab í¬ê¸°
    version_info.append(f"vocab_{tokenizer.vocab_size}")

    # 2. í† í¬ë‚˜ì´ì € íƒ€ì…
    tokenizer_type = type(tokenizer).__name__
    version_info.append(f"type_{tokenizer_type}")

    # 3. íŠ¹ìˆ˜ í† í°
    special_tokens = {
        'bos': tokenizer.bos_token_id,
        'eos': tokenizer.eos_token_id,
        'pad': tokenizer.pad_token_id,
        'unk': tokenizer.unk_token_id,
    }
    version_info.append(f"tokens_{special_tokens}")

    # 4. ì¶”ê°€ ì •ë³´
    if additional_info:
        version_info.append(additional_info)

    # í•´ì‹œ ìƒì„±
    version_string = "_".join(str(v) for v in version_info)
    cache_version = hashlib.md5(version_string.encode()).hexdigest()[:8]

    return cache_version


# ============================================================================
# Sequence Concatenation for Pretraining
# ============================================================================

def concatenate_sequences(
    tokenized_sequences: list,
    max_seq_length: int,
    eos_token_id: int,
) -> list:
    """
    ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ max_seq_length ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    ê° ì›ë³¸ ì‹œí€€ìŠ¤ ëì— EOS í† í°ì„ ì‚½ì…í•˜ì—¬ ë¬¸ì„œ ê²½ê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

    Args:
        tokenized_sequences: í† í°í™”ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê°ê° input_ids í¬í•¨)
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        eos_token_id: EOS í† í° ID

    Returns:
        ì—°ê²° í›„ max_seq_lengthë¡œ ë¶„í• ëœ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    import numpy as np

    # 1. ì´ ê¸¸ì´ ì¶”ì • (over-estimate to avoid reallocation)
    estimated_len = sum(len(seq["input_ids"]) + 1 for seq in tokenized_sequences)
    logger.info(f"ğŸ“¦ Concatenating {len(tokenized_sequences):,} sequences (~{estimated_len:,} tokens)")

    # 2. í•œ ë²ˆì˜ ë£¨í”„ë¡œ numpy ë°°ì—´ êµ¬ì¶• (ìµœì í™”)
    all_tokens = np.empty(estimated_len, dtype=np.int32)
    offset = 0

    for seq in tokenized_sequences:
        input_ids = seq["input_ids"]
        seq_len = len(input_ids)

        if seq_len == 0:
            continue

        # ì‹œí€€ìŠ¤ ë³µì‚¬ ë° EOS ì¶”ê°€ë¥¼ í•œ ë²ˆì—
        all_tokens[offset:offset + seq_len] = input_ids
        offset += seq_len

        # EOS ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°ë§Œ)
        if input_ids[-1] != eos_token_id:
            all_tokens[offset] = eos_token_id
            offset += 1

    # ì‹¤ì œ ì‚¬ìš©ëœ ê¸¸ì´ë¡œ ìë¥´ê¸°
    all_tokens = all_tokens[:offset]

    # 3. max_seq_length ì²­í¬ë¡œ ë¶„í• 
    num_chunks = (len(all_tokens) + max_seq_length - 1) // max_seq_length
    chunks = []

    for i in range(num_chunks):
        start = i * max_seq_length
        end = min(start + max_seq_length, len(all_tokens))
        chunk_len = end - start

        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë²„ë¦¼
        if chunk_len < MIN_CHUNK_LENGTH:
            logger.info(f"  Dropping short final chunk of {chunk_len} tokens")
            continue

        # numpy ë°°ì—´ì„ ì§ì ‘ ì‚¬ìš© (ë©”ëª¨ë¦¬ ë³µì‚¬ ìµœì†Œí™”)
        chunks.append({
            "input_ids": all_tokens[start:end].copy(),  # numpy array
            "attention_mask": np.ones(chunk_len, dtype=np.int8),  # int8ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        })

    logger.info(f"âœ“ Created {len(chunks):,} chunks of max {max_seq_length} tokens each")
    return chunks


# ============================================================================
# Optimized Tokenization Function
# ============================================================================

def tokenize_all_datasets(
    all_sources: list,
    tokenizer,
    args,
) -> list:
    """
    ëª¨ë“  ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì§•í•˜ê³  ìºì‹œì— ì €ì¥

    Args:
        all_sources: [(source_type, source_name), ...] ë¦¬ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì

    Returns:
        [{'name': str, 'cache_path': Path, 'num_samples': int}, ...]
    """
    import gc
    from datasets import Dataset as HFDataset

    rank = int(os.environ.get("RANK", 0))
    is_main_process = rank == 0

    tokenized_datasets_info = []
    cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))

    if is_main_process:
        logger.info("="*80)
        logger.info("âš¡ Pre-tokenizing all datasets (Rank 0 only)")
        logger.info("="*80)

        for idx, (src_type, src_name) in enumerate(all_sources):
            logger.info(f"")
            logger.info(f"ğŸ“¦ [{idx+1}/{len(all_sources)}] Dataset: {src_name}")

            # ë°ì´í„°ì…‹ ë¡œë“œ
            logger.info(f"  Loading dataset...")
            if src_type == "hf":
                dataset, text_column = load_pretrain_dataset(
                    dataset_names=[src_name],
                    dataset_config=args.dataset_config if idx == 0 else None,
                    train_files=None,
                    text_column=args.text_column,
                )
            else:
                dataset, text_column = load_pretrain_dataset(
                    dataset_names=None,
                    dataset_config=None,
                    train_files=[src_name],
                    text_column=args.text_column,
                )

            # ìºì‹œ ê²½ë¡œ ì„¤ì • (í† í¬ë‚˜ì´ì € ë²„ì „ í¬í•¨)
            cache_version = get_cache_version_key(
                tokenizer,
                additional_info=f"packing_{args.packing}_maxlen_{args.max_seq_length}_seq_{idx}"
            )
            dataset_hash = hashlib.md5(f"{src_name}_{cache_version}".encode()).hexdigest()[:16]
            tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
            tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

            if tokenized_cache_path.exists() and tokenized_marker.exists():
                logger.info(f"  âœ… Loading cached tokenized dataset")
                tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
                logger.info(f"  âœ… Loaded {len(tokenized_dataset):,} samples")
            else:
                logger.info(f"  ğŸ”¤ Tokenizing dataset...")

                tokenized_ds = tokenize_dataset(
                    dataset=dataset["train"],
                    tokenizer=tokenizer,
                    text_column=text_column,
                    max_seq_length=args.max_seq_length,
                    packing=args.packing,
                )

                if args.packing:
                    logger.info(f"  ğŸ“¦ Packing sequences...")
                    tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
                    del tokenized_ds
                    gc.collect()

                    concatenated_chunks = concatenate_sequences(
                        tokenized_sequences=tokenized_list,
                        max_seq_length=args.max_seq_length,
                        eos_token_id=tokenizer.eos_token_id,
                    )
                    del tokenized_list
                    gc.collect()

                    tokenized_dataset = HFDataset.from_list(concatenated_chunks)
                    del concatenated_chunks
                    gc.collect()
                else:
                    tokenized_dataset = tokenized_ds

                logger.info(f"  ğŸ’¾ Saving tokenized dataset...")
                num_shards = get_optimal_num_shards(len(tokenized_dataset), os.cpu_count() or 8)
                tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=num_shards)
                tokenized_marker.touch()
                logger.info(f"  âœ… Tokenized: {len(tokenized_dataset):,} samples (shards={num_shards})")

            tokenized_datasets_info.append({
                'name': src_name,
                'cache_path': tokenized_cache_path,
                'num_samples': len(tokenized_dataset),
            })

            del dataset
            del tokenized_dataset
            gc.collect()

        logger.info("="*80)
        logger.info("âœ… All datasets pre-tokenized!")
        logger.info("="*80)
    else:
        # ë‹¤ë¥¸ Rankë“¤ì€ ë§ˆì§€ë§‰ ë§ˆì»¤ ëŒ€ê¸°
        logger.info(f"[Rank {rank}] Waiting for rank 0 to complete tokenization...")

        import time as time_module
        last_src_name = all_sources[-1][1]
        last_idx = len(all_sources) - 1
        cache_version = get_cache_version_key(
            tokenizer,
            additional_info=f"packing_{args.packing}_maxlen_{args.max_seq_length}_seq_{last_idx}"
        )
        dataset_hash = hashlib.md5(f"{last_src_name}_{cache_version}".encode()).hexdigest()[:16]
        last_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

        max_wait = 7200
        waited = 0
        while not last_marker.exists() and waited < max_wait:
            time_module.sleep(10)
            waited += 10
            if waited % 60 == 0:
                logger.info(f"[Rank {rank}] Still waiting... ({waited}s)")

        if not last_marker.exists():
            raise TimeoutError(f"Rank {rank}: Tokenizing timeout after {max_wait}s")

        logger.info(f"[Rank {rank}] âœ… Loading tokenized datasets info...")

        for idx, (src_type, src_name) in enumerate(all_sources):
            cache_version = get_cache_version_key(
                tokenizer,
                additional_info=f"packing_{args.packing}_maxlen_{args.max_seq_length}_seq_{idx}"
            )
            dataset_hash = hashlib.md5(f"{src_name}_{cache_version}".encode()).hexdigest()[:16]
            tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"

            tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
            tokenized_datasets_info.append({
                'name': src_name,
                'cache_path': tokenized_cache_path,
                'num_samples': len(tokenized_dataset),
            })
            del tokenized_dataset
            gc.collect()

        logger.info(f"[Rank {rank}] âœ… All dataset info loaded!")

    return tokenized_datasets_info


def tokenize_dataset(
    dataset,
    tokenizer,
    text_column: str = "text",
    max_seq_length: int = 2048,
    packing: bool = False,
    num_proc: int = None,
):
    """
    ìµœì í™”ëœ í† í°í™” í•¨ìˆ˜ (ëª¨ë“  ì½”ë“œ ê²½ë¡œì—ì„œ ê³µìœ )

    í•µì‹¬ ìµœì í™”:
    - TOKENIZERS_PARALLELISM=false + num_proc=N (ë©€í‹°í”„ë¡œì„¸ì‹±)
    - ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ Fast Tokenizer ì‹¤í–‰ = ìµœëŒ€ ë³‘ë ¬í™”
    - batch_size ìë™ ì¡°ì • (IPC ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)

    Args:
        dataset: HuggingFace Dataset ê°ì²´
        tokenizer: í† í¬ë‚˜ì´ì € (Fast Tokenizer ê¶Œì¥)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
        max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        packing: Trueë©´ truncation ì—†ì´ í† í°í™” (ë‚˜ì¤‘ì— concatenate)
        num_proc: í”„ë¡œì„¸ìŠ¤ ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)

    Returns:
        í† í°í™”ëœ Dataset ê°ì²´
    """
    import multiprocessing

    total_samples = len(dataset)
    cpu_count = multiprocessing.cpu_count()

    # ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ìë™ ê²°ì •
    if num_proc is None:
        env_num_proc = os.getenv(ENV_DATASET_NUM_PROC)
        if env_num_proc:
            num_proc = int(env_num_proc)
        else:
            # CPU, ë©”ëª¨ë¦¬, ë°ì´í„° í¬ê¸°ë¥¼ ê³ ë ¤í•œ ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³„ì‚°
            num_proc = calculate_optimal_num_proc(total_samples, cpu_count)

    # ë°°ì¹˜ í¬ê¸°ë„ ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ì¡°ì ˆ
    batch_size = BATCH_SIZE_LARGE_DATASET if total_samples > DATASET_SIZE_LARGE else BATCH_SIZE_DEFAULT
    writer_batch_size = WRITER_BATCH_SIZE

    # TOKENIZERS_PARALLELISM ì„¤ì •
    # num_proc=1: Rust ë‚´ë¶€ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” (true)
    # num_proc>1: Python ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©, Rust ë³‘ë ¬ ë¹„í™œì„±í™” (false)
    if num_proc == 1:
        os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"  # Rust ë©€í‹°ìŠ¤ë ˆë”© í™œì„±í™”!
        logger.info("   TOKENIZERS_PARALLELISM=true (Rust internal parallelism enabled)")
    else:
        os.environ[ENV_TOKENIZERS_PARALLELISM] = "false"  # Python ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
        logger.info("   TOKENIZERS_PARALLELISM=false (Python multiprocessing mode)")

    # Fast Tokenizer í™•ì¸
    if not tokenizer.is_fast:
        logger.warning("âš ï¸ WARNING: Slow tokenizer detected! 10-50x slower expected.")

    # ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
    estimated_speed = num_proc * ESTIMATED_TOKENIZATION_SPEED
    estimated_time = total_samples / estimated_speed / 60

    logger.info(f"ğŸ”¤ Tokenization config:")
    logger.info(f"   Samples: {total_samples:,}")
    logger.info(f"   Processes: {num_proc} (auto-tuned for dataset size)")
    logger.info(f"   Batch size: {batch_size:,}")
    logger.info(f"   Mode: {'packing' if packing else 'truncation'}")
    logger.info(f"   Estimated time: ~{estimated_time:.0f} min")

    start_time = time.time()

    if packing:
        # Packing ëª¨ë“œ: truncation ì—†ì´ í† í°í™” (ë‚˜ì¤‘ì— concatenate)
        def batch_tokenize(examples):
            # datasets.map()ì´ TOKENIZERS_PARALLELISMì„ falseë¡œ ì¬ì„¤ì •í•˜ë¯€ë¡œ
            # ë§¤ ë°°ì¹˜ë§ˆë‹¤ trueë¡œ ì¬ì„¤ì • (Rust ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”)
            os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"
            return tokenizer(
                examples[text_column],
                truncation=False,
                padding=False,
                add_special_tokens=True,
            )
    else:
        # ì¼ë°˜ ëª¨ë“œ: truncation ì ìš©
        def batch_tokenize(examples):
            # datasets.map()ì´ TOKENIZERS_PARALLELISMì„ falseë¡œ ì¬ì„¤ì •í•˜ë¯€ë¡œ
            # ë§¤ ë°°ì¹˜ë§ˆë‹¤ trueë¡œ ì¬ì„¤ì • (Rust ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”)
            os.environ[ENV_TOKENIZERS_PARALLELISM] = "true"
            return tokenizer(
                examples[text_column],
                truncation=True,
                max_length=max_seq_length,
                padding=False,
            )

    tokenized = dataset.map(
        batch_tokenize,
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        remove_columns=dataset.column_names,
        load_from_cache_file=False,
        writer_batch_size=writer_batch_size,
        keep_in_memory=False,
        desc=f"Tokenizing ({num_proc} procs)",
    )

    elapsed = time.time() - start_time
    speed = total_samples / elapsed if elapsed > 0 else 0
    logger.info(f"âœ… Tokenization completed:")
    logger.info(f"   Time: {elapsed/60:.1f} min")
    logger.info(f"   Speed: {speed:,.0f} samples/sec")
    logger.info(f"   Output: {len(tokenized):,} samples")

    return tokenized


def tokenize_non_sequential_dataset(tokenizer, args):
    """
    Non-sequential ëª¨ë“œ: ë‹¨ì¼/ë³‘í•© ë°ì´í„°ì…‹ ë¡œë“œ ë° í† í¬ë‚˜ì´ì§•

    Args:
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì

    Returns:
        tokenized_dataset: í† í¬ë‚˜ì´ì§•ëœ Dataset ê°ì²´
    """
    import gc
    from datasets import Dataset as HFDataset

    ddp_info = get_ddp_info()
    rank = ddp_info['rank']
    is_main_process = ddp_info['is_main_process']

    # ----------------------------------------------------------------
    # 1. ë°ì´í„°ì…‹ ë¡œë“œ
    # ----------------------------------------------------------------
    log_with_rank("ğŸ“š Loading datasets...", rank, is_main_process)
    load_start = time.time()

    if args.mode == "pretrain":
        if is_main_process:
            logger.info(f"[Rank 0] Loading {len(args.dataset) if args.dataset else 0} datasets...")
        dataset, text_column = load_pretrain_dataset(
            dataset_names=args.dataset,
            dataset_config=args.dataset_config,
            train_files=args.train_file,
            text_column=args.text_column,
        )
    else:  # sft
        dataset, text_column = load_sft_dataset(
            dataset_names=args.dataset,
            train_files=args.train_file,
        )

    load_time = time.time() - load_start
    log_with_rank(f"âœ… Dataset loaded in {load_time:.1f}s: {len(dataset['train']):,} samples", rank, is_main_process)

    # ----------------------------------------------------------------
    # 2. í† í¬ë‚˜ì´ì§• ìºì‹œ ê²½ë¡œ ì„¤ì • (í† í¬ë‚˜ì´ì € ë²„ì „ í¬í•¨)
    # ----------------------------------------------------------------
    dataset_names_str = "_".join(args.dataset) if args.dataset else "local"

    # í† í¬ë‚˜ì´ì € ë²„ì „ì„ í¬í•¨í•œ ìºì‹œ í‚¤ ìƒì„±
    cache_version = get_cache_version_key(
        tokenizer,
        additional_info=f"packing_{getattr(args, 'packing', False)}_maxlen_{args.max_seq_length}"
    )
    dataset_cache_key = f"{dataset_names_str}_{cache_version}"

    tokenized_cache_path = create_cache_path(dataset_cache_key, "_tokenized")
    tokenized_marker = Path(str(tokenized_cache_path).replace("_tokenized", ".tokenized.marker"))

    # ----------------------------------------------------------------
    # 3. í† í¬ë‚˜ì´ì§• ìˆ˜í–‰ (Rank 0ë§Œ) ë˜ëŠ” ìºì‹œì—ì„œ ë¡œë“œ
    # ----------------------------------------------------------------
    if is_main_process:
        logger.info("ğŸ”¤ [Rank 0] Tokenizing dataset...")

        # í† í¬ë‚˜ì´ì € ì›Œë°ì—…
        logger.info("ğŸ”¥ Warming up tokenizer...")
        warmup_texts = [WARMUP_TEXT_PATTERN] * WARMUP_TEXT_COUNT
        _ = tokenizer(warmup_texts, truncation=False, padding=False)
        logger.info("âœ… Tokenizer warmed up")

    # ìºì‹œ í™•ì¸ ë° ë¡œë“œ
    if tokenized_cache_path.exists() and tokenized_marker.exists():
        log_with_rank(f"âœ… Loading cached tokenized dataset from: {tokenized_cache_path}", rank, is_main_process)
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        log_with_rank(f"âœ… Loaded {len(tokenized_dataset):,} samples from cache", rank, is_main_process)
    elif is_main_process:
        # Rank 0ë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰
        tokenized_ds = tokenize_dataset(
            dataset=dataset["train"],
            tokenizer=tokenizer,
            text_column=text_column,
            max_seq_length=args.max_seq_length,
            packing=args.packing,
        )

        if args.packing:
            # Packing: concatenate sequences
            logger.info("ğŸ“¦ Packing sequences...")
            tokenized_list = [{"input_ids": ids} for ids in tokenized_ds["input_ids"]]
            del tokenized_ds
            gc.collect()

            concatenated_chunks = concatenate_sequences(
                tokenized_sequences=tokenized_list,
                max_seq_length=args.max_seq_length,
                eos_token_id=tokenizer.eos_token_id,
            )
            del tokenized_list
            gc.collect()

            tokenized_dataset = HFDataset.from_list(concatenated_chunks)
            del concatenated_chunks
            gc.collect()
        else:
            tokenized_dataset = tokenized_ds

        # ìºì‹œ ì €ì¥ (rank 0ë§Œ)
        logger.info(f"ğŸ’¾ [Rank 0] Saving tokenized dataset to: {tokenized_cache_path}")
        num_shards = get_optimal_num_shards(len(tokenized_dataset), os.cpu_count() or 8)
        tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=num_shards)
        tokenized_marker.touch()
        logger.info(f"âœ… [Rank 0] Tokenized and saved: {len(tokenized_dataset):,} samples (shards={num_shards})")
    else:
        # ë‹¤ë¥¸ rankë“¤ì€ ë§ˆì»¤ ëŒ€ê¸° í›„ ë¡œë“œ
        if not wait_for_marker(tokenized_marker, TOKENIZATION_TIMEOUT, CHECK_INTERVAL, rank):
            raise TimeoutError(f"Rank {rank}: Tokenizing timeout after {TOKENIZATION_TIMEOUT}s")

        logger.info(f"ğŸ“¥ [Rank {rank}] Loading tokenized dataset from: {tokenized_cache_path}")
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        logger.info(f"âœ… [Rank {rank}] Loaded {len(tokenized_dataset):,} samples")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del dataset
    gc.collect()

    if is_main_process:
        logger.info(f"âœ… Tokenization complete: {len(tokenized_dataset):,} samples ready for training")
    else:
        logger.info(f"âœ… [Rank {rank}] Ready for training with {len(tokenized_dataset):,} samples")

    return tokenized_dataset


# ============================================================================
# ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³€í™˜
# ============================================================================

def _load_single_file(file_path: str) -> list:
    """ë‹¨ì¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    formatted_data = []
    
    if file_path.endswith('.json') or file_path.endswith('.jsonl'):
        with open(file_path, 'rb') as f:  # Binary mode for orjson
            if file_path.endswith('.jsonl'):
                data = [json.loads(line) for line in f if line.strip()]
            else:
                data = json.loads(f.read())  # orjson uses loads() not load()
        
        for item in data:
            text = _convert_to_text(item)
            if text:
                formatted_data.append({"text": text})
    else:
        # txt íŒŒì¼
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    formatted_data.append({"text": line})
    
    return formatted_data


def _load_hf_dataset(dataset_name: str, dataset_config: Optional[str] = None):
    """
    ë‹¨ì¼ HuggingFace ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜

    ë°ì´í„°ì…‹ ì´ë¦„ì— configë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ:
        - "dataset_name:config_name" í˜•ì‹ ì§€ì›
        - ì˜ˆ: "maywell/korean_textbooks:claude_evol"

    DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    # DDP í™˜ê²½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    ddp_info = get_ddp_info()
    is_distributed = ddp_info['is_distributed']
    is_main_process = ddp_info['is_main_process']
    current_rank = ddp_info['rank']
    
    # dataset_name:config_name í˜•ì‹ íŒŒì‹±
    if ":" in dataset_name:
        dataset_name, config_from_name = dataset_name.split(":", 1)
        if not dataset_config:
            dataset_config = config_from_name
    
    logger.info(f"  Loading HuggingFace: {dataset_name}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ - ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    load_kwargs = {
        "keep_in_memory": False,  # ë””ìŠ¤í¬ì— ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ë¡œ ìœ ì§€
    }
    if dataset_config:
        load_kwargs["name"] = dataset_config
    
    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
    # ë‹¤ë¥¸ rankë“¤ì€ ìµœì¢… ë³€í™˜ ê²°ê³¼ë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ê²½ë¡œ ìƒì„±
        cache_key = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        dataset_save_path = create_cache_path(cache_key, "_final")
        filter_marker_path = Path(str(dataset_save_path).replace("_final", ".filtered.marker"))

        # ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ìˆìœ¼ë©´ ëª¨ë“  rankê°€ ë¡œë“œ (ì¬ì‹œì‘ ì‹œ ì•ˆì „)
        if dataset_save_path.exists() and filter_marker_path.exists():
            logger.info(f"    [Rank {current_rank}] âœ… Using existing processed dataset from: {dataset_save_path}")
            from datasets import Dataset
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start
            logger.info(f"    [Rank {current_rank}] Loaded {len(converted):,} samples in {load_time:.1f}s")

            # barrier ë™ê¸°í™”
            ddp_barrier()

            # ë³€í™˜ ê²°ê³¼ ë°˜í™˜ (ë‚˜ë¨¸ì§€ ë¡œì§ ê±´ë„ˆë›°ê¸°)
            return converted

        # barrier ë™ê¸°í™”
        ddp_barrier()
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
            logger.info(f"    [Rank 0] Downloading dataset...")
            if dataset_config:
                logger.info(f"    Config: {dataset_config}")
            raw_dataset = load_dataset(dataset_name, **load_kwargs)
            logger.info(f"    [Rank 0] Dataset download completed")
            
            # train split ì‚¬ìš©
            train_data = raw_dataset.get("train", raw_dataset)
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë‚˜ì¤‘ì— ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” ì•„ë¬´ê²ƒë„ ì•ˆ í•¨)
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete processing...")
            train_data = None  # ë‚˜ì¤‘ì— ìºì‹œì—ì„œ ë¡œë“œ
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        if dataset_config:
            logger.info(f"    Config: {dataset_config}")
        raw_dataset = load_dataset(dataset_name, **load_kwargs)
        
        # train split ì‚¬ìš©
        train_data = raw_dataset.get("train", raw_dataset)
    
    # dataset.map()ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™˜
    def convert_batch(examples):
        texts = []
        # ê° ì»¬ëŸ¼ì„ ê°œë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±
        keys = list(examples.keys())
        num_examples = len(examples[keys[0]]) if keys else 0
        
        for i in range(num_examples):
            item = {k: examples[k][i] for k in keys}
            text = _convert_to_text(item)
            texts.append(text if text else "")
        
        return {"text": texts}
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ìœ ì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    env_config = get_tokenization_env_config()
    dataset_num_proc = env_config['num_proc']
    dataset_batch_size = env_config['batch_size']
    dataset_writer_batch_size = env_config['writer_batch_size']

    # DDP í™˜ê²½ì—ì„œëŠ” rank 0ë§Œ ë³€í™˜í•˜ê³  ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” ìºì‹œë§Œ ë¡œë“œ
    if is_distributed:
        # ìºì‹œ ì™„ë£Œ ë§ˆì»¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
        cache_key = f"{dataset_name}_{dataset_config}" if dataset_config else dataset_name
        cache_marker = Path(str(create_cache_path(cache_key, "")).replace(cache_key[:16], f".{cache_key[:16]}_converted.marker"))
        
        if is_main_process:
            # rank 0ë§Œ ë°ì´í„°ì…‹ ë³€í™˜ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ë¡œ ë¹ ë¥´ê²Œ)
            logger.info(f"    [Rank 0] Converting dataset with {dataset_num_proc} processes "
                       f"(batch_size={dataset_batch_size}, writer_batch_size={dataset_writer_batch_size})...")
            converted = train_data.map(
                convert_batch,
                batched=True,
                batch_size=dataset_batch_size,
                num_proc=dataset_num_proc,
                remove_columns=train_data.column_names,
                load_from_cache_file=True,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,  # ë©”ëª¨ë¦¬ ë§µ íŒŒì¼ ì‚¬ìš©
                desc=f"Converting {dataset_name}",
            )
            
            # ë³€í™˜ ì™„ë£Œ ë§ˆì»¤ ìƒì„± (filter ì „ì—!)
            cache_marker.parent.mkdir(parents=True, exist_ok=True)
            cache_marker.touch()
            logger.info(f"    [Rank 0] Created conversion marker: {cache_marker}")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ)
            filter_num_proc = min(dataset_num_proc // FILTER_NUM_PROC_DIVISOR, MAX_FILTER_NUM_PROC)
            logger.info(f"    [Rank 0] Filtering empty texts with {filter_num_proc} processes...")
            converted = converted.filter(
                lambda x: len(x["text"]) > 0,
                num_proc=filter_num_proc,
                writer_batch_size=dataset_writer_batch_size,
                keep_in_memory=False,
                load_from_cache_file=True,
            )

            logger.info(f"    [Rank 0] Conversion completed: {len(converted):,} samples")

            # ìµœì¢… ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ë‹¤ë¥¸ rankë“¤ì´ ì•ˆì „í•˜ê²Œ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡)
            if not dataset_save_path.exists():
                logger.info(f"    [Rank 0] Saving final dataset to: {dataset_save_path}")
                save_start = time.time()
                num_shards = get_optimal_num_shards(len(converted), os.cpu_count() or 8)
                converted.save_to_disk(
                    str(dataset_save_path),
                    num_shards=num_shards,
                )
                save_time = time.time() - save_start
                logger.info(f"    [Rank 0] Dataset saved in {save_time:.1f}s (shards={num_shards})")
            else:
                logger.info(f"    [Rank 0] Dataset already saved at: {dataset_save_path}")

            # í•„í„° ì™„ë£Œ ë§ˆì»¤ ìƒì„±
            filter_marker_path.touch()
            logger.info(f"    [Rank 0] Created filter marker: {filter_marker_path}")

            # ë³€í™˜ ì™„ë£Œ í›„ barrier
            ddp_barrier()
                
        else:
            # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ëŠ” í•„í„° ë§ˆì»¤ ëŒ€ê¸° í›„ ìµœì¢… ê²°ê³¼ë§Œ ë¡œë“œ!
            logger.info(f"    [Rank {current_rank}] Waiting for rank 0 to complete all processing...")

            if not wait_for_marker(filter_marker_path, DATASET_PROCESSING_TIMEOUT, CHECK_INTERVAL, current_rank):
                raise TimeoutError(f"Rank {current_rank}: Dataset processing timeout after {DATASET_PROCESSING_TIMEOUT}s")

            logger.info(f"    [Rank {current_rank}] Processing complete, loading final result from cache...")

            # barrier ë™ê¸°í™”
            ddp_barrier()

            # rank 0ì´ ì €ì¥í•œ ìµœì¢… ë°ì´í„°ì…‹ì„ ì§ì ‘ ë¡œë“œ
            logger.info(f"    [Rank {current_rank}] Loading final dataset from: {dataset_save_path}")

            # íŒŒì¼ì´ ì™„ì „íˆ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì§§ì€ ëŒ€ê¸° (íŒŒì¼ ì‹œìŠ¤í…œ ë™ê¸°í™”)
            for attempt in range(60):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
                if dataset_save_path.exists() and (dataset_save_path / "dataset_info.json").exists():
                    break
                time.sleep(0.5)
            else:
                logger.warning(f"    [Rank {current_rank}] Dataset files not fully ready, proceeding anyway...")

            from datasets import Dataset
            load_start = time.time()
            converted = Dataset.load_from_disk(str(dataset_save_path))
            load_time = time.time() - load_start

            logger.info(f"    [Rank {current_rank}] Loaded from disk in {load_time:.1f}s: {len(converted):,} samples")
            
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ í™˜ê²½
        logger.info(f"    Converting dataset with {dataset_num_proc} processes...")
        converted = train_data.map(
            convert_batch,
            batched=True,
            batch_size=dataset_batch_size,
            num_proc=dataset_num_proc,
            remove_columns=train_data.column_names,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
            desc=f"Converting {dataset_name}",
        )
        
        logger.info(f"    Filtering empty texts...")
        filter_num_proc = min(dataset_num_proc // FILTER_NUM_PROC_DIVISOR, MAX_FILTER_NUM_PROC)
        converted = converted.filter(
            lambda x: len(x["text"]) > 0,
            num_proc=filter_num_proc,
            load_from_cache_file=True,
            writer_batch_size=dataset_writer_batch_size,
            keep_in_memory=False,
        )
    
    # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    # ë¦¬ìŠ¤íŠ¸ ë³€í™˜ì„ í”¼í•˜ê³  Datasetì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
    logger.info(f"    â†’ {len(converted):,} samples")
    return converted  # Dataset ê°ì²´ ë°˜í™˜


def load_pretrain_dataset(
    dataset_names: Optional[list] = None,
    dataset_config: Optional[str] = None,
    train_files: Optional[list] = None,
    text_column: str = "text",
):
    """
    ì‚¬ì „í•™ìŠµìš© ë°ì´í„°ì…‹ ë¡œë“œ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    Args:
        dataset_names: HuggingFace ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["wikipedia", "alpaca"])
        dataset_config: ë°ì´í„°ì…‹ ì„¤ì • (ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©)
        train_files: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (txt ë˜ëŠ” json)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„
    
    ì§€ì› í¬ë§·:
        - txt íŒŒì¼: ê° ì¤„ì´ í•˜ë‚˜ì˜ ë¬¸ì„œ
        - json íŒŒì¼: instruction/output, input/output, messages, conversations ë“±
        - HuggingFace ë°ì´í„°ì…‹: ìœ„ í˜•ì‹ ìë™ ê°ì§€
    """
    logger.info("ğŸ“š Loading pretrain dataset...")
    
    from datasets import Dataset, concatenate_datasets
    
    datasets_list = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬)
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]

        # íŒŒì¼ ë³‘ë ¬ ë¡œë”© ì‚¬ìš©
        loaded_datasets = load_files_parallel(train_files, max_workers=8)
        datasets_list.extend(loaded_datasets)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ (Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for i, ds_name in enumerate(dataset_names):
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì—ë§Œ config ì ìš©
            config = dataset_config if i == 0 else None
            ds_data = _load_hf_dataset(ds_name, config)
            # Dataset ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ)
            if isinstance(ds_data, Dataset):
                datasets_list.append(ds_data)
            else:
                # í˜¸í™˜ì„±ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë§Œ ë³€í™˜
                datasets_list.append(Dataset.from_list(ds_data))
    
    if not datasets_list:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    # ì—¬ëŸ¬ Datasetì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©
    if len(datasets_list) == 1:
        combined_dataset = datasets_list[0]
    else:
        logger.info(f"  Concatenating {len(datasets_list)} datasets...")
        combined_dataset = concatenate_datasets(datasets_list)
    
    logger.info(f"  Total: {len(combined_dataset):,} samples")
    
    dataset = {"train": combined_dataset}
    text_column = "text"

    logger.info(f"âœ“ Dataset loaded: {len(dataset['train'])} samples")
    return dataset, text_column


def _convert_to_text(item: dict) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Foundation Model pretrainìš©)
    
    íŠ¹ìˆ˜ í† í° ì—†ì´ ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ì´ê²ƒì€ Next Token Predictionì„ ìœ„í•œ Foundation Model í•™ìŠµ ë°©ì‹ì…ë‹ˆë‹¤.
    
    ì§€ì› í˜•ì‹:
        - {"text": "..."}: ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - {"input": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"instruction": "...", "output": "..."}: ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"messages": [...]}: ëª¨ë“  ë©”ì‹œì§€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        - {"conversations": [...]}: ëª¨ë“  ëŒ€í™” ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    """
    # ì•ˆì „í•œ ë¬¸ìì—´ ì¶”ì¶œ í•¨ìˆ˜
    def safe_str(val) -> str:
        return (val or "").strip()
    
    # text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if "text" in item and item["text"]:
        return item["text"]
    
    # input/output í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "output" in item:
        inp = safe_str(item["input"])
        out = safe_str(item["output"])
        if not out:
            return None
        return f"{inp}\n\n{out}" if inp else out
    
    # instruction/output í¬ë§· (Alpaca) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "instruction" in item and "output" in item:
        inst = safe_str(item["instruction"])
        out = safe_str(item["output"])
        # input í•„ë“œë„ ìˆìœ¼ë©´ í•©ì¹¨
        inp = safe_str(item.get("input"))
        if inp:
            inst = f"{inst}\n{inp}" if inst else inp
        if not inst and not out:
            return None
        return f"{inst}\n\n{out}" if inst else out
    
    # messages í¬ë§· (OpenAI Chat) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "messages" in item and item["messages"]:
        texts = []
        for msg in item["messages"]:
            if msg:
                content = safe_str(msg.get("content"))
                if content:
                    texts.append(content)
        return "\n\n".join(texts) if texts else None
    
    # conversations í¬ë§· (ShareGPT) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "conversations" in item and item["conversations"]:
        texts = []
        for conv in item["conversations"]:
            if conv:
                value = safe_str(conv.get("value"))
                if value:
                    texts.append(value)
        return "\n\n".join(texts) if texts else None
    
    # DeepSeek R1 ìŠ¤íƒ€ì¼ (input/content/reasoning_content) â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "input" in item and "content" in item:
        parts = []
        inp = safe_str(item.get("input"))
        reasoning = safe_str(item.get("reasoning_content"))
        content = safe_str(item.get("content"))
        if inp:
            parts.append(inp)
        if reasoning:
            parts.append(reasoning)
        if content:
            parts.append(content)
        return "\n\n".join(parts) if parts else None
    
    # prompt/response í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "response" in item:
        prompt = safe_str(item["prompt"])
        response = safe_str(item["response"])
        if not response:
            return None
        return f"{prompt}\n\n{response}" if prompt else response
    
    # question/answer í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "question" in item and "answer" in item:
        question = safe_str(item["question"])
        answer = safe_str(item["answer"])
        if not answer:
            return None
        return f"{question}\n\n{answer}" if question else answer
    
    # prompt/completion í¬ë§· â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    if "prompt" in item and "completion" in item:
        prompt = safe_str(item["prompt"])
        completion = safe_str(item["completion"])
        if not completion:
            return None
        return f"{prompt}\n\n{completion}" if prompt else completion
    
    # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹
    logger.warning(f"Unknown format, skipping: {list(item.keys())}")
    return None


def load_sft_dataset(
    dataset_names: Optional[list] = None,
    train_files: Optional[list] = None,
):
    """
    SFTìš© ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§· ë³€í™˜ (ì—¬ëŸ¬ íŒŒì¼/ë°ì´í„°ì…‹ ì§€ì›)

    ì§€ì› í¬ë§·:
    - Alpaca: {"instruction": "...", "output": "..."}
    - Chat: {"messages": [{"role": "user", "content": "..."}]}
    - ShareGPT: {"conversations": [{"from": "human", "value": "..."}]}
    """
    logger.info("ğŸ“š Loading SFT dataset...")
    
    all_data = []

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    if train_files:
        if isinstance(train_files, str):
            train_files = [train_files]
        
        for file_path in train_files:
            logger.info(f"  Loading file: {file_path}")
            file_data = _load_single_file(file_path)
            logger.info(f"    â†’ {len(file_data):,} samples")
            all_data.extend(file_data)
    
    # HuggingFace ë°ì´í„°ì…‹ ë¡œë“œ
    if dataset_names:
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        
        for ds_name in dataset_names:
            ds_data = _load_hf_dataset(ds_name)
            all_data.extend(ds_data)
    
    if not all_data:
        raise ValueError("Either dataset_names or train_files must be provided")
    
    logger.info(f"  Total: {len(all_data):,} samples")

    # Datasetìœ¼ë¡œ ë³€í™˜
    from datasets import Dataset
    dataset = {"train": Dataset.from_list(all_data)}

    logger.info(f"âœ“ SFT dataset loaded: {len(dataset['train'])} samples")
    return dataset, "text"


# ============================================================================
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
# ============================================================================

def setup_model_and_tokenizer(
    tokenizer_path: str,
    model_config: Optional[str] = None,
    pretrained_model: Optional[str] = None,
    use_flash_attention: bool = False,
    use_compile: bool = False,
    use_bf16: bool = False,
    use_fp16: bool = False,
):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""

    # dtype ê²°ì •
    if use_bf16:
        dtype = torch.bfloat16
        dtype_str = "bfloat16"
    elif use_fp16:
        dtype = torch.float16
        dtype_str = "float16"
    else:
        dtype = torch.float32
        dtype_str = "float32"

    # í† í¬ë‚˜ì´ì €
    logger.info(f"ğŸ“ Loading tokenizer from: {tokenizer_path}")
    import time
    tok_start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path,
        use_fast=True,  # Rust ê¸°ë°˜ ê³ ì† í† í¬ë‚˜ì´ì € ê°•ì œ ì‚¬ìš©
    )
    tok_time = time.time() - tok_start
    logger.info(f"âœ“ Tokenizer loaded in {tok_time:.1f}s")

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸
    if pretrained_model:
        logger.info(f"ğŸ”„ Loading pretrained model: {pretrained_model}")
        logger.info(f"  (This may take 20-30s for 8 GPUs...)")
        model_start = time.time()
        model = MoaiForCausalLM.from_pretrained(pretrained_model, dtype=dtype)
        model_time = time.time() - model_start
        logger.info(f"âœ“ Model loaded in {model_time:.1f}s")
        logger.info(f"  Model dtype: {dtype_str}")
    else:
        logger.info("ğŸ†• Creating new model from config")
        if model_config:
            config = MoaiConfig.from_json_file(model_config)
        else:
            config = MoaiConfig()
        
        # Flash Attention ì„¤ì •
        if use_flash_attention:
            try:
                import flash_attn
                config.use_flash_attention = True
                logger.info("âš¡ Flash Attention 2 enabled")
            except ImportError:
                logger.warning("âš ï¸ flash-attn not installed, using standard attention")
        
        # ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ dtype ì§€ì •
        config.dtype = dtype
        model = MoaiForCausalLM(config)
        model = model.to(dtype)
        logger.info(f"  Model dtype: {dtype_str}")
    
    # torch.compile ì ìš© (PyTorch 2.0+)
    # Note: mode="default" is more stable with DDP than "reduce-overhead"
    if use_compile:
        try:
            logger.info("ğŸ”§ Compiling model with torch.compile (mode=default)...")
            model = torch.compile(model, mode="default", dynamic=True)
            logger.info("âœ“ Model compiled successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")

    # íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ“ Model parameters: {total_params:,} ({total_params/1e9:.2f}B)")

    return model, tokenizer


# ============================================================================
# í•™ìŠµ
# ============================================================================

def train_sequential(tokenized_datasets_info: list, tokenizer, args):
    """
    ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ìˆœì°¨ í•™ìŠµ ìˆ˜í–‰

    Args:
        tokenized_datasets_info: [{'name': str, 'cache_path': Path, 'num_samples': int}, ...]
        tokenizer: í† í¬ë‚˜ì´ì €
        args: í•™ìŠµ ì¸ì
    """
    import gc
    import sys
    from datasets import Dataset as HFDataset

    # DDP í™˜ê²½ ì •ë³´
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    is_distributed = world_size > 1
    is_main_process = rank == 0

    if is_main_process:
        logger.info("="*80)
        logger.info("ğŸ¯ Sequential Training")
        logger.info("="*80)

    # W&B ì´ˆê¸°í™” (ì„ íƒì )
    if args.use_wandb:
        try:
            import wandb
            if is_main_process:
                wandb.init(
                    project=args.wandb_project,
                    name=args.wandb_run_name,
                    config=vars(args),
                )
                logger.info(f"ğŸ“Š W&B initialized: {args.wandb_project}")
        except ImportError:
            logger.warning("âš ï¸ wandb not installed")
            args.use_wandb = False

    current_checkpoint = args.pretrained_model

    for idx, dataset_info in enumerate(tokenized_datasets_info):
        if is_main_process:
            logger.info("")
            logger.info("="*80)
            logger.info(f"ğŸš€ Training [{idx+1}/{len(tokenized_datasets_info)}]: {dataset_info['name']}")
            logger.info(f"   Samples: {dataset_info['num_samples']:,}")
            logger.info("="*80)

        # ëª¨ë¸ ë¡œë“œ
        if is_main_process:
            if idx == 0:
                logger.info(f"â³ Loading model from: {current_checkpoint or 'scratch'}")
            else:
                logger.info(f"â³ Resuming from: {current_checkpoint}")

        model, _ = setup_model_and_tokenizer(
            tokenizer_path=args.tokenizer_path,
            model_config=args.model_config,
            pretrained_model=current_checkpoint,
            use_flash_attention=args.flash_attention,
            use_compile=args.compile,
            use_bf16=args.bf16,
            use_fp16=args.fp16,
        )
        if is_main_process:
            logger.info(f"âœ“ Model loaded")

        # í† í°í™”ëœ ë°ì´í„°ì…‹ ë¡œë“œ
        if is_main_process:
            logger.info(f"ğŸ“¥ Loading tokenized dataset...")

        tokenized_dataset = HFDataset.load_from_disk(str(dataset_info['cache_path']))

        if is_main_process:
            logger.info(f"âœ“ Loaded {len(tokenized_dataset):,} samples")

        # Data Collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )

        # Training Arguments
        stage_output_dir = f"{args.output_dir}/stage_{idx+1}"

        # GPU ìµœì í™” íŒŒë¼ë¯¸í„° ê³„ì‚°
        optimal_prefetch = get_optimal_prefetch_factor(batch_size=args.batch_size)

        training_args = TrainingArguments(
            output_dir=stage_output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            warmup_steps=WARMUP_STEPS_FIRST_STAGE if idx == 0 else WARMUP_STEPS_RESUME,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            save_total_limit=2,
            bf16=args.bf16,
            fp16=args.fp16,
            gradient_checkpointing=args.gradient_checkpointing,
            dataloader_num_workers=args.dataloader_num_workers,
            remove_unused_columns=False,
            report_to=["wandb"] if args.use_wandb else ["tensorboard"],
            max_steps=args.max_steps if args.max_steps > 0 else -1,
            save_safetensors=True,
            # I/O ìµœì í™”
            dataloader_pin_memory=True,
            dataloader_prefetch_factor=optimal_prefetch,  # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ì„¤ì •
            dataloader_persistent_workers=True,  # worker ì¬ì‚¬ìš©
            dataloader_drop_last=True,
            # ì˜µí‹°ë§ˆì´ì € ë° ì •ë°€ë„
            optim="adamw_torch_fused",
            ddp_find_unused_parameters=False,
            tf32=True,
            # ë°°ì¹˜ ìµœì í™”
            group_by_length=not getattr(args, 'packing', False),  # packing ì—†ì„ ë•Œë§Œ ê·¸ë£¹í•‘
            max_grad_norm=1.0,
            gradient_checkpointing_kwargs={"use_reentrant": False} if args.gradient_checkpointing else None,
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )

        if is_main_process:
            logger.info(f"ğŸƒ Starting training...")
        trainer.train()

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = f"{stage_output_dir}/checkpoint"
        trainer.save_model(checkpoint_path)

        # DDP barrier
        if torch.distributed.is_initialized():
            torch.distributed.barrier()

        # dtype ìœ ì§€í•˜ë©° ì €ì¥ (rank 0ë§Œ)
        if is_main_process:
            model_dtype = next(model.parameters()).dtype
            if model_dtype in (torch.bfloat16, torch.float16):
                logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format...")
                model.save_pretrained(checkpoint_path, torch_dtype=model_dtype, safe_serialization=True)
            logger.info(f"âœ… Stage {idx+1} completed: {checkpoint_path}")

        # ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ìœ„í•´ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì—…ë°ì´íŠ¸
        current_checkpoint = checkpoint_path

        # ë©”ëª¨ë¦¬ í•´ì œ
        del model
        del tokenized_dataset
        del trainer
        gc.collect()

        try:
            torch.cuda.empty_cache()
        except:
            pass

        # DDP barrier
        if torch.distributed.is_initialized():
            torch.distributed.barrier()

    # ìµœì¢… ì™„ë£Œ
    if is_main_process:
        logger.info("="*80)
        logger.info("ğŸ‰ Sequential training completed!")
        logger.info(f"ğŸ“ Final model: {current_checkpoint}")
        logger.info("="*80)


# ============================================================================
# Main Train Function
# ============================================================================
def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    # DDP í™˜ê²½ í™•ì¸
    ddp_info = get_ddp_info()
    rank = ddp_info['rank']
    world_size = ddp_info['world_size']
    is_distributed = ddp_info['is_distributed']
    is_main_process = ddp_info['is_main_process']

    logger.info("="*80)
    logger.info(f"ğŸš€ Starting {args.mode.upper()} training")
    logger.info(f"ğŸŒ Environment: {world_size} GPU(s), Rank {rank}")
    logger.info("="*80)

    # ============================================================================
    # STEP 0: í† í¬ë‚˜ì´ì € ë¡œë“œ (DDP ì „!)
    # ============================================================================
    if is_main_process:
        logger.info("ğŸ“ Loading tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(
        args.tokenizer_path,
        use_fast=True,
    )

    # Fast Tokenizer ê°•ì œ ì²´í¬
    if not tokenizer.is_fast:
        raise ValueError(
            f"âŒ Fast Tokenizer not available! "
            f"Current tokenizer: {type(tokenizer).__name__}\n"
            f"Please ensure you're using a tokenizer that supports Fast mode (Rust-based)."
        )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    if is_main_process:
        logger.info("âœ… Using Fast Tokenizer (Rust-based)")
        logger.info(f"   Tokenizer type: {type(tokenizer).__name__}")

        # Warmup: Rust í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ì²« í˜¸ì¶œì´ ëŠë¦´ ìˆ˜ ìˆìŒ)
        logger.info("   Warming up tokenizer...")
        warmup_start = time.time()
        for _ in range(WARMUP_TEXT_COUNT):
            _ = tokenizer(WARMUP_TEXT_PATTERN, truncation=False, padding=False)
        warmup_time = time.time() - warmup_start
        logger.info(f"   Warmup completed in {warmup_time:.2f}s")

    # ============================================================================
    # Sequential ëª¨ë“œ: ë¨¼ì € í† í¬ë‚˜ì´ì§• í›„ í•™ìŠµ
    # ============================================================================
    if args.sequential and args.dataset and len(args.dataset) > 1:
        logger.info("ğŸ“¦ Sequential mode: Processing datasets one by one")

        # ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
        dataset_names = args.dataset if args.dataset else []
        train_files = args.train_file if args.train_file else []

        all_sources = []
        for ds in dataset_names:
            all_sources.append(("hf", ds))
        for f in train_files:
            all_sources.append(("file", f))

        if is_main_process:
            logger.info(f"ğŸ“‹ Datasets to process: {len(all_sources)}")
            for i, (src_type, src_name) in enumerate(all_sources):
                logger.info(f"  {i+1}. [{src_type}] {src_name}")

        # í† í¬ë‚˜ì´ì§• (DDP ì „, Rank 0ë§Œ ì‹¤í–‰)
        tokenized_datasets_info = tokenize_all_datasets(all_sources, tokenizer, args)

        # DDP Barrier
        if is_distributed:
            import torch.distributed as dist
            if dist.is_initialized():
                dist.barrier()

        # Tokenize-only ëª¨ë“œ
        if hasattr(args, '_tokenize_only') and args._tokenize_only:
            logger.info("="*80)
            logger.info("âœ… Tokenization completed! (tokenize-only mode)")
            logger.info("="*80)
            return

        # í•™ìŠµ
        train_sequential(tokenized_datasets_info, tokenizer, args)
        return

    # ============================================================================
    # STEP 1: í† í¬ë‚˜ì´ì§• (Non-sequential ëª¨ë“œ)
    # ============================================================================
    tokenized_dataset = tokenize_non_sequential_dataset(tokenizer, args)
    
    # ============================================================================
    # STEP 2: DDP ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
    # ============================================================================
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    if world_size > 1:
        logger.info(f"ğŸŒ Distributed Training: Rank {rank}/{world_size}")
        logger.info(f"â³ Initializing DDP environment...")
    else:
        logger.info(f"ğŸ’» Single GPU Training")
    
    logger.info("â³ Loading model...")
    model, _ = setup_model_and_tokenizer(
        tokenizer_path=args.tokenizer_path,
        model_config=args.model_config,
        pretrained_model=args.pretrained_model,
        use_flash_attention=args.flash_attention,
        use_compile=args.compile,
        use_bf16=args.bf16,
        use_fp16=args.fp16,
    )
    
    # ============================================================================
    # STEP 3: í•™ìŠµ ì‹œì‘
    # ============================================================================
    logger.info("ğŸš€ Starting training with pre-tokenized data...")
    
    # 4. Data Collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # 5. Training Arguments (ìµœì í™” ì˜µì…˜ í¬í•¨)
    # GPU ìµœì í™” íŒŒë¼ë¯¸í„° ê³„ì‚°
    optimal_prefetch = get_optimal_prefetch_factor(batch_size=args.batch_size)

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        max_steps=args.max_steps if args.max_steps > 0 else -1,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        dataloader_num_workers=args.dataloader_num_workers,
        remove_unused_columns=False,
        report_to=["wandb"] if args.use_wandb else ["tensorboard"],
        save_safetensors=True,
        ddp_find_unused_parameters=False,
        # I/O ìµœì í™”
        dataloader_pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
        dataloader_prefetch_factor=optimal_prefetch,  # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ì„¤ì •
        dataloader_persistent_workers=True,  # worker ì¬ì‚¬ìš©
        dataloader_drop_last=True,  # ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±°
        # ì˜µí‹°ë§ˆì´ì € ë° ì •ë°€ë„
        optim="adamw_torch_fused",  # Fused Adam (faster)
        tf32=True,  # TF32 ì‚¬ìš© (Ampere GPU)
        # ë°°ì¹˜ ìµœì í™”
        group_by_length=not getattr(args, 'packing', False),  # packing ì—†ì„ ë•Œë§Œ ê·¸ë£¹í•‘
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        gradient_checkpointing_kwargs={"use_reentrant": False} if args.gradient_checkpointing else None,
    )

    # 6. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    # 7. í•™ìŠµ ì‹œì‘
    logger.info("="*80)
    logger.info("ğŸ¯ Training configuration:")
    logger.info(f"  Mode: {args.mode}")
    logger.info(f"  Packing: {args.packing}")
    logger.info(f"  Output: {args.output_dir}")
    logger.info(f"  Batch size: {args.batch_size}")
    logger.info(f"  Learning rate: {args.learning_rate}")
    logger.info(f"  Max steps: {args.max_steps if args.max_steps > 0 else 'Full epoch'}")
    logger.info(f"  Logging: {'wandb' if args.use_wandb else 'tensorboard'}")
    if args.resume_from_checkpoint:
        logger.info(f"  Resume from: {args.resume_from_checkpoint}")
    logger.info("="*80)

    logger.info("ğŸƒ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)

    # 8. ëª¨ë¸ ì €ì¥
    logger.info("ğŸ’¾ Saving final model...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))
    
    # ëª¨ë¸ dtype í™•ì¸ ë° bf16/fp16ìœ¼ë¡œ ëª…ì‹œì  ì €ì¥
    model_dtype = next(model.parameters()).dtype
    if model_dtype in (torch.bfloat16, torch.float16):
        logger.info(f"ğŸ’¾ Re-saving model in {model_dtype} format for compatibility...")
        model.save_pretrained(str(final_path), torch_dtype=model_dtype, safe_serialization=True)
    
    tokenizer.save_pretrained(str(final_path))
    
    logger.info("="*80)
    logger.info(f"âœ… Training completed!")
    logger.info(f"ğŸ“ Model saved to: {final_path}")
    logger.info(f"ğŸ“Š Model dtype: {model_dtype}")
    logger.info("="*80)


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="MOAI-LLM Training")

    # í•„ìˆ˜ ì¸ì
    parser.add_argument("--mode", type=str, required=True, choices=["pretrain", "sft"], help="Training mode")
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Path to tokenizer")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for model checkpoints")

    # ë°ì´í„°ì…‹ (ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë˜ëŠ” ì—¬ëŸ¬ íŒŒì¼ ì§€ì›)
    parser.add_argument("--dataset", type=str, nargs="*", help="HuggingFace dataset names")
    parser.add_argument("--dataset_config", type=str, default=None, help="Dataset configuration name")
    parser.add_argument("--train_file", type=str, nargs="*", help="Training data files (JSONL, JSON, Parquet, etc.)")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name for dataset")

    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_config", type=str, help="Model config file (JSON)")
    parser.add_argument("--pretrained_model", type=str, help="Pretrained model path")

    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--learning_rate", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.1)
    parser.add_argument("--warmup_steps", type=int, default=2000)
    parser.add_argument("--max_seq_length", type=int, default=2048)
    parser.add_argument("--max_steps", type=int, default=-1, help="Max steps (-1 for full)")

    # ìµœì í™”
    parser.add_argument("--bf16", action="store_true", help="Use BF16")
    parser.add_argument("--fp16", action="store_true", help="Use FP16")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Enable gradient checkpointing")
    
    # Packing (Pretrain/SFT ë‘˜ ë‹¤ ì§€ì›)
    parser.add_argument(
        "--packing",
        action="store_true",
        help="Enable sequence packing/concatenation. "
             "Concatenates all sequences with EOS tokens and chunks into max_seq_length. "
             "Works for both pretrain and SFT modes."
    )
    
    # Sequential ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    parser.add_argument(
        "--sequential",
        action="store_true",
        help="Process datasets sequentially one by one to save memory. "
             "Each dataset is loaded, trained, then freed before the next."
    )
    
    # Tokenize only ëª¨ë“œ (DDP ì „ì— í† í°í™”ë§Œ ìˆ˜í–‰)
    parser.add_argument(
        "--tokenize_only",
        action="store_true",
        help="Only tokenize datasets and exit (no training). "
             "Use this to pre-tokenize before running torchrun."
    )

    # ë¡œê¹…
    parser.add_argument("--logging_steps", type=int, default=100)
    parser.add_argument("--save_steps", type=int, default=1000)
    parser.add_argument("--save_total_limit", type=int, default=3)

    # Resume from checkpoint
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="Path to checkpoint to resume training from. "
             "Use this to continue training with different datasets."
    )

    # ê¸°íƒ€
    parser.add_argument("--num_proc", type=int, default=48, help="Number of processes for tokenization (default: 48 for high-performance CPUs)")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    
    # ì¶”ê°€ ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--flash_attention",
        action="store_true",
        help="Use Flash Attention 2 for faster training (requires flash-attn package)"
    )
    parser.add_argument(
        "--compile",
        action="store_true", 
        help="Use torch.compile for faster training (PyTorch 2.0+)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to cache processed datasets for faster subsequent runs"
    )
    
    # Logging ì˜µì…˜
    parser.add_argument(
        "--use_wandb",
        action="store_true",
        help="Use Weights & Biases for logging (default: tensorboard)"
    )
    parser.add_argument(
        "--wandb_project",
        type=str,
        default="moai-llm",
        help="W&B project name (default: moai-llm)"
    )
    parser.add_argument(
        "--wandb_run_name",
        type=str,
        default=None,
        help="W&B run name (default: auto-generated)"
    )

    args = parser.parse_args()

    # ê²€ì¦
    if not args.dataset and not args.train_file:
        parser.error("Either --dataset or --train_file must be provided")

    # Tokenize only ëª¨ë“œ: DDP ì—†ì´ í† í°í™”ë§Œ ìˆ˜í–‰
    if args.tokenize_only:
        print("="*80)
        print("ğŸ”¥ Tokenize-Only Mode: Pre-tokenizing datasets (no DDP)")
        print("="*80)
        
        # Sequentialì´ í•„ìš”
        if not args.sequential:
            args.sequential = True
            print("âš¡ Automatically enabling --sequential mode for tokenization")
        
        # í•µì‹¬: ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ Fast Tokenizer ì‚¬ìš©
        # num_proc=1 â†’ datasetsê°€ TOKENIZERS_PARALLELISM=false ì„¤ì • ì•ˆí•¨
        # ë°ì´í„° ë³€í™˜ì€ ë³‘ë ¬ë¡œ, í† í¬ë‚˜ì´ì§•ì€ batch iteratorë¡œ
        num_proc = os.getenv("DATASET_NUM_PROC", str(min(48, os.cpu_count() or 8)))
        os.environ["DATASET_NUM_PROC"] = num_proc
        print(f"âš¡ DATASET_NUM_PROC={num_proc} for data conversion")
        
        # train_sequential í˜¸ì¶œ (í† í°í™” ë¶€ë¶„ë§Œ ì‹¤í–‰ë¨)
        print("ğŸš€ Calling train_sequential for tokenization...")
        
        # DDP í™˜ê²½ ë³€ìˆ˜ ì œê±° (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰)
        os.environ.pop("RANK", None)
        os.environ.pop("WORLD_SIZE", None)
        os.environ.pop("LOCAL_RANK", None)
        os.environ.pop("MASTER_ADDR", None)
        os.environ.pop("MASTER_PORT", None)
        
        # tokenizationë§Œ ìˆ˜í–‰í•˜ê³  trainingì€ ìŠ¤í‚µí•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
        args._tokenize_only = True
        
        train_sequential(args)
        
        print("="*80)
        print("âœ… Tokenization completed! Now run torchrun for training.")
        print("="*80)
        return

    # í•™ìŠµ ì‹œì‘
    train(args)


if __name__ == "__main__":
    # ê°€ì¥ ë¨¼ì € ì¶œë ¥ (torchrunì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œëŒ€ë¡œ ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸)
    import sys
    import os
    
    # ì¦‰ì‹œ ì¶œë ¥
    rank = int(os.environ.get(ENV_RANK, -1))
    print(f"[INIT] Rank {rank}: Python script started!", flush=True)

    world_size = int(os.environ.get(ENV_WORLD_SIZE, 1))
    
    if rank == 0:
        print("="*80, flush=True)
        print("ğŸš€ MOAI-LLM Training Starting...", flush=True)
        print(f"ğŸŒ World size: {world_size} GPUs", flush=True)
        print("="*80, flush=True)

    main()

