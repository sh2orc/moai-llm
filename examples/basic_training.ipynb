{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOAI-LLM ê¸°ë³¸ í•™ìŠµ ê°€ì´ë“œ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ MOAI-LLMì˜ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **í™˜ê²½ í™•ì¸** - GPU, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •\n",
    "2. **í† í¬ë‚˜ì´ì € í•™ìŠµ** - HuggingFace ë°ì´í„°ì…‹ ê¸°ë°˜\n",
    "3. **ì‚¬ì „í•™ìŠµ (Pretrain)** - ì–¸ì–´ ëª¨ë¸ ê¸°ì´ˆ í•™ìŠµ\n",
    "4. **íŒŒì¸íŠœë‹ (SFT)** - Instruction following í•™ìŠµ\n",
    "5. **ì¶”ë¡  í…ŒìŠ¤íŠ¸** - í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "**ê¶Œì¥ í™˜ê²½**: Google Colab (T4/V100), Kaggle Notebook, ë˜ëŠ” ë¡œì»¬ GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì • ë° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë° í™˜ê²½ í™•ì¸\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í™˜ê²½ ì •ë³´\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python ë²„ì „: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        mem_total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"    ë©”ëª¨ë¦¬: {mem_total:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOAI-LLM ì„¤ì¹˜ (ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì‹œ)\n",
    "# Colab/Kaggleì—ì„œëŠ” ë¨¼ì € ì €ì¥ì†Œë¥¼ í´ë¡ í•˜ì„¸ìš”\n",
    "\n",
    "# Google Colab/Kaggleì¸ ê²½ìš° ì£¼ì„ í•´ì œ:\n",
    "# !git clone https://github.com/sh2orc/moai-llm.git\n",
    "# %cd moai-llm\n",
    "# !pip install -e .\n",
    "# !pip install sentencepiece datasets accelerate\n",
    "\n",
    "# Flash Attention ì„¤ì¹˜ (ì„ íƒì , ì†ë„ í–¥ìƒ)\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í† í¬ë‚˜ì´ì € í•™ìŠµ (10ë¶„)\n",
    "\n",
    "HuggingFace Wikipedia ë°ì´í„°ì…‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: 10,000 ìƒ˜í”Œ, vocab_size=32K  \n",
    "**ì‹¤ì „ í•™ìŠµ**: ì „ì²´ ë°ì´í„°, vocab_size=128K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (2-3ë¶„, ì¶”ì²œ)\n",
    "python train_tokenizer.py \\\n",
    "    --dataset wikipedia \\\n",
    "    --dataset_config 20220301.ko \\\n",
    "    --vocab_size 32000 \\\n",
    "    --max_samples 10000 \\\n",
    "    --output_dir tokenizers/test-32k\n",
    "\n",
    "# ì‹¤ì „ í•™ìŠµ (1-2ì‹œê°„, ì£¼ì„ í•´ì œ)\n",
    "# python train_tokenizer.py \\\n",
    "#     --dataset wikipedia \\\n",
    "#     --dataset_config 20220301.ko \\\n",
    "#     --vocab_size 128000 \\\n",
    "#     --output_dir tokenizers/ko-128k"
   ]
  },
  {
   "cell_type": "code",
   "source": "%%bash\n\n# BCCard ë°ì´í„°ì…‹ìœ¼ë¡œ ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í† í¬ë‚˜ì´ì € í•™ìŠµ\npython train_tokenizer.py \\\n    --dataset BCCard/BCCard-Finance-Kor-QnA \\\n    --vocab_size 32000 \\\n    --max_samples 10000 \\\n    --output_dir tokenizers/finance-32k\n\n# ë˜ëŠ” Wikipedia + BCCard í˜¼í•© í•™ìŠµ (ë” ê· í˜•ì¡íŒ í† í¬ë‚˜ì´ì €)\n# python train_tokenizer.py \\\n#     --dataset wikipedia BCCard/BCCard-Finance-Kor-QnA \\\n#     --dataset_config 20220301.ko None \\\n#     --vocab_size 32000 \\\n#     --max_samples 5000 \\\n#     --output_dir tokenizers/mixed-32k",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.1. BCCard ë°ì´í„°ì…‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ\n\nê¸ˆìœµ ë„ë©”ì¸ì— íŠ¹í™”ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ BCCard ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n",
    "import sentencepiece as spm\n",
    "\n",
    "# í•™ìŠµëœ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer_path = \"tokenizers/test-32k/tokenizer.model\"\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(tokenizer_path)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_texts = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”. MOAI-LLM í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.\",\n",
    "    \"Hello, this is a tokenizer test.\",\n",
    "    \"æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\",  # ì¼ë³¸ì–´\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì–´íœ˜ í¬ê¸°: {sp.vocab_size():,}\")\n",
    "print(f\"BOS í† í° ID: {sp.bos_id()}\")\n",
    "print(f\"EOS í† í° ID: {sp.eos_id()}\")\n",
    "print(f\"PAD í† í° ID: {sp.pad_id()}\")\n",
    "print()\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = sp.encode(text)\n",
    "    pieces = sp.encode_as_pieces(text)\n",
    "    decoded = sp.decode(tokens)\n",
    "    \n",
    "    print(f\"ì›ë¬¸: {text}\")\n",
    "    print(f\"í† í° ê°œìˆ˜: {len(tokens)}\")\n",
    "    print(f\"í† í° ì¡°ê°: {pieces[:10]}...\" if len(pieces) > 10 else f\"í† í° ì¡°ê°: {pieces}\")\n",
    "    print(f\"ë³µì›: {decoded}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "source": "%%bash\n\n# ë°©ë²• 1: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ (ê°™ì€ output_dir ì‚¬ìš©)\npython train.py \\\n    --mode pretrain \\\n    --dataset wikitext \\\n    --dataset_config wikitext-2-raw-v1 \\\n    --tokenizer_path tokenizers/test-32k \\\n    --output_dir outputs/pretrain-test \\\n    --resume_from_checkpoint outputs/pretrain-test/checkpoint-50 \\\n    --max_steps 200 \\\n    --batch_size 2 \\\n    --logging_steps 10 \\\n    --save_steps 50\n\n# ë°©ë²• 2: ê¸°ì¡´ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ê°€ í•™ìŠµ (ìƒˆë¡œìš´ output_dir ì‚¬ìš©)\n# python train.py \\\n#     --mode pretrain \\\n#     --dataset wikipedia \\\n#     --dataset_config 20220301.ko \\\n#     --tokenizer_path tokenizers/test-32k \\\n#     --pretrained_model outputs/pretrain-test/final_model \\\n#     --output_dir outputs/pretrain-continued \\\n#     --max_steps 500 \\\n#     --batch_size 2 \\\n#     --logging_steps 10 \\\n#     --save_steps 100",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1. ê¸°ì¡´ ëª¨ë¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµí•˜ê¸° (Resume Training)\n\ní•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆê±°ë‚˜ ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš° ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì‚¬ì „í•™ìŠµ (Pretrain)\n",
    "\n",
    "ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ì´ˆë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: 100 steps (5ë¶„)  \n",
    "**ì‹¤ì „ í•™ìŠµ**: ìˆ˜ì²œ~ìˆ˜ë§Œ steps (ìˆ˜ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "source": "%%bash\n\n# ë°©ë²• 1: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ\npython train.py \\\n    --mode sft \\\n    --dataset BCCard/BCCard-Finance-Kor-QnA \\\n    --tokenizer_path tokenizers/test-32k \\\n    --pretrained_model outputs/pretrain-test/final_model \\\n    --output_dir outputs/sft-test \\\n    --resume_from_checkpoint outputs/sft-test/checkpoint-25 \\\n    --max_steps 100 \\\n    --batch_size 2 \\\n    --logging_steps 10 \\\n    --save_steps 25\n\n# ë°©ë²• 2: ê¸°ì¡´ SFT ëª¨ë¸ì— ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ê°€ íŒŒì¸íŠœë‹\n# python train.py \\\n#     --mode sft \\\n#     --dataset your/another-qa-dataset \\\n#     --tokenizer_path tokenizers/test-32k \\\n#     --pretrained_model outputs/sft-test/final_model \\\n#     --output_dir outputs/sft-continued \\\n#     --num_epochs 1 \\\n#     --batch_size 2 \\\n#     --logging_steps 10 \\\n#     --save_steps 50\n\n# ë°©ë²• 3: Learning rateë¥¼ ë‚®ì¶°ì„œ fine-tuning (catastrophic forgetting ë°©ì§€)\n# python train.py \\\n#     --mode sft \\\n#     --dataset BCCard/BCCard-Finance-Kor-QnA \\\n#     --tokenizer_path tokenizers/test-32k \\\n#     --pretrained_model outputs/sft-test/final_model \\\n#     --output_dir outputs/sft-low-lr \\\n#     --learning_rate 1e-6 \\\n#     --num_epochs 1 \\\n#     --batch_size 2",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2. SFT í•™ìŠµ ì¬ê°œí•˜ê¸°\n\nSFT í•™ìŠµë„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œí•˜ê±°ë‚˜ ê¸°ì¡´ SFT ëª¨ë¸ì„ ì¶”ê°€ë¡œ íŒŒì¸íŠœë‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (100 steps, 5ë¶„)\n",
    "python train.py \\\n",
    "    --mode pretrain \\\n",
    "    --dataset wikitext \\\n",
    "    --dataset_config wikitext-2-raw-v1 \\\n",
    "    --tokenizer_path tokenizers/test-32k \\\n",
    "    --output_dir outputs/pretrain-test \\\n",
    "    --max_steps 100 \\\n",
    "    --batch_size 2 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 50\n",
    "\n",
    "# ì‹¤ì „ í•™ìŠµ (ì£¼ì„ í•´ì œ, GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •)\n",
    "# python train.py \\\n",
    "#     --mode pretrain \\\n",
    "#     --dataset wikipedia \\\n",
    "#     --dataset_config 20220301.ko \\\n",
    "#     --tokenizer_path tokenizers/ko-128k \\\n",
    "#     --output_dir outputs/pretrain-ko \\\n",
    "#     --batch_size 4 \\\n",
    "#     --gradient_accumulation_steps 8 \\\n",
    "#     --bf16 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --logging_steps 100 \\\n",
    "#     --save_steps 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë¡œê·¸ í™•ì¸\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "log_file = Path(\"outputs/pretrain-test/training_log.jsonl\")\n",
    "\n",
    "if log_file.exists():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"í•™ìŠµ ì§„í–‰ ìƒí™©\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(log_file) as f:\n",
    "        logs = [json.loads(line) for line in f]\n",
    "    \n",
    "    # ìµœê·¼ 10ê°œ ë¡œê·¸ í‘œì‹œ\n",
    "    for log in logs[-10:]:\n",
    "        step = log.get('step', 'N/A')\n",
    "        loss = log.get('loss', 'N/A')\n",
    "        lr = log.get('learning_rate', 'N/A')\n",
    "        print(f\"Step {step}: Loss={loss:.4f}, LR={lr:.2e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ í•™ìŠµ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. íŒŒì¸íŠœë‹ (SFT - Supervised Fine-Tuning)\n",
    "\n",
    "ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ Q&A í˜•ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: 50 steps (3ë¶„)  \n",
    "**ì‹¤ì „ í•™ìŠµ**: 3 epochs (ìˆ˜ì‹œê°„)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„° ê¸¸ì´ ë¶„í¬ í™•ì¸\nimport numpy as np\n\n# ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸¸ì´ ê³„ì‚°\nsample_size = min(1000, len(dataset['train']))\nsamples = dataset['train'].select(range(sample_size))\n\n# í•„ë“œëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸\nfirst_sample = dataset['train'][0]\ntext_fields = [k for k, v in first_sample.items() if isinstance(v, str)]\n\nprint(\"=\" * 60)\nprint(f\"í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„ (ì²˜ìŒ {sample_size}ê°œ ìƒ˜í”Œ)\")\nprint(\"=\" * 60)\n\nfor field in text_fields:\n    lengths = [len(sample[field]) for sample in samples]\n    \n    print(f\"\\n[{field}]\")\n    print(f\"  í‰ê·  ê¸¸ì´: {np.mean(lengths):.1f} ê¸€ì\")\n    print(f\"  ìµœì†Œ ê¸¸ì´: {np.min(lengths)} ê¸€ì\")\n    print(f\"  ìµœëŒ€ ê¸¸ì´: {np.max(lengths)} ê¸€ì\")\n    print(f\"  ì¤‘ì•™ê°’: {np.median(lengths):.1f} ê¸€ì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„° ì˜ˆì‹œ í™•ì¸\nimport random\n\nprint(\"=\" * 60)\nprint(\"BCCard ë°ì´í„°ì…‹ ìƒ˜í”Œ (ëœë¤ 5ê°œ)\")\nprint(\"=\" * 60)\n\n# ëœë¤í•˜ê²Œ 5ê°œ ìƒ˜í”Œ ì„ íƒ\nnum_samples = min(5, len(dataset['train']))\nrandom_indices = random.sample(range(len(dataset['train'])), num_samples)\n\nfor i, idx in enumerate(random_indices, 1):\n    sample = dataset['train'][idx]\n    \n    print(f\"\\n[ìƒ˜í”Œ {i}]\")\n    \n    # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ì¶œë ¥ (ì¼ë°˜ì ìœ¼ë¡œ 'instruction', 'input', 'output' ë˜ëŠ” 'question', 'answer')\n    for key, value in sample.items():\n        if isinstance(value, str) and len(value) > 200:\n            print(f\"{key}: {value[:200]}...\")\n        else:\n            print(f\"{key}: {value}\")\n    \n    print(\"-\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# BCCard ë°ì´í„°ì…‹ ë¡œë“œ\nfrom datasets import load_dataset\n\nprint(\"BCCard ê¸ˆìœµ Q&A ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\ndataset = load_dataset(\"BCCard/BCCard-Finance-Kor-QnA\")\n\nprint(\"=\" * 60)\nprint(\"ë°ì´í„°ì…‹ ì •ë³´\")\nprint(\"=\" * 60)\nprint(f\"ë°ì´í„°ì…‹ êµ¬ì¡°: {dataset}\")\nprint(f\"\\ní•™ìŠµ ë°ì´í„° í¬ê¸°: {len(dataset['train']):,} ìƒ˜í”Œ\")\n\n# ë°ì´í„° í•„ë“œ í™•ì¸\nif len(dataset['train']) > 0:\n    print(f\"\\në°ì´í„° í•„ë“œ: {dataset['train'].column_names}\")\n    print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ í‚¤: {list(dataset['train'][0].keys())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1. BCCard ê¸ˆìœµ Q&A ë°ì´í„°ì…‹ í™•ì¸\n\nSFT í•™ìŠµì— ì‚¬ìš©í•  [BCCard/BCCard-Finance-Kor-QnA](https://huggingface.co/datasets/BCCard/BCCard-Finance-Kor-QnA) ë°ì´í„°ì…‹ì„ ë¨¼ì € ì‚´í´ë´…ë‹ˆë‹¤.\n\nì´ ë°ì´í„°ì…‹ì€ ê¸ˆìœµ ê´€ë ¨ í•œêµ­ì–´ ì§ˆë¬¸-ë‹µë³€ ìŒìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (50 steps, 3ë¶„)\n",
    "python train.py \\\n",
    "    --mode sft \\\n",
    "    --dataset BCCard/BCCard-Finance-Kor-QnA \\\n",
    "    --tokenizer_path tokenizers/test-32k \\\n",
    "    --pretrained_model outputs/pretrain-test/final_model \\\n",
    "    --output_dir outputs/sft-test \\\n",
    "    --max_steps 50 \\\n",
    "    --batch_size 2 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 25\n",
    "\n",
    "# ì‹¤ì „ í•™ìŠµ (ì£¼ì„ í•´ì œ)\n",
    "# python train.py \\\n",
    "#     --mode sft \\\n",
    "#     --dataset BCCard/BCCard-Finance-Kor-QnA \\\n",
    "#     --tokenizer_path tokenizers/ko-128k \\\n",
    "#     --pretrained_model outputs/pretrain-ko/final_model \\\n",
    "#     --output_dir outputs/sft-finance \\\n",
    "#     --num_epochs 3 \\\n",
    "#     --batch_size 4 \\\n",
    "#     --gradient_accumulation_steps 4 \\\n",
    "#     --bf16 \\\n",
    "#     --gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT í•™ìŠµ ë¡œê·¸ í™•ì¸\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "log_file = Path(\"outputs/sft-test/training_log.jsonl\")\n",
    "\n",
    "if log_file.exists():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SFT í•™ìŠµ ì§„í–‰ ìƒí™©\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with open(log_file) as f:\n",
    "        logs = [json.loads(line) for line in f]\n",
    "    \n",
    "    # ì „ì²´ ë¡œê·¸ í‘œì‹œ\n",
    "    for log in logs:\n",
    "        step = log.get('step', 'N/A')\n",
    "        loss = log.get('loss', 'N/A')\n",
    "        lr = log.get('learning_rate', 'N/A')\n",
    "        print(f\"Step {step}: Loss={loss:.4f}, LR={lr:.2e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ SFT í•™ìŠµ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì¶”ë¡ \n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "from moai_llm.modeling.model import MoaiForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"outputs/sft-test/final_model\"\n",
    "tokenizer_path = \"tokenizers/test-32k\"\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "model = MoaiForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# GPUë¡œ ì´ë™\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({device})\")\n",
    "print(f\"íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def generate_text(prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "    # í”„ë¡¬í”„íŠ¸ í† í°í™”\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "print(\"âœ… ìƒì„± í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ 1: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "print(\"=\" * 60)\n",
    "print(\"í…ŒìŠ¤íŠ¸ 1: ê°„ë‹¨í•œ ìƒì„±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt = \"ì•ˆë…•í•˜ì„¸ìš”.\"\n",
    "result = generate_text(prompt, max_new_tokens=50)\n",
    "\n",
    "print(f\"í”„ë¡¬í”„íŠ¸: {prompt}\")\n",
    "print(f\"ìƒì„± ê²°ê³¼:\\n{result}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ 2: Q&A í˜•ì‹ (SFT í•™ìŠµ í˜•ì‹)\n",
    "print(\"=\" * 60)\n",
    "print(\"í…ŒìŠ¤íŠ¸ 2: Q&A í˜•ì‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# BCCard í•™ìŠµ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ\n",
    "question = \"ì‹ ìš©ì¹´ë“œ ë°œê¸‰ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "result = generate_text(prompt, max_new_tokens=100, temperature=0.7)\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {question}\")\n",
    "print(f\"\\nì „ì²´ ìƒì„± ê²°ê³¼:\\n{result}\")\n",
    "print()\n",
    "\n",
    "# ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "if \"<|im_start|>assistant\" in result:\n",
    "    answer = result.split(\"<|im_start|>assistant\")[-1]\n",
    "    answer = answer.split(\"<|im_end|>\")[0].strip()\n",
    "    print(f\"ë‹µë³€ ë¶€ë¶„:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ 3: ì—¬ëŸ¬ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"=\" * 60)\n",
    "print(\"í…ŒìŠ¤íŠ¸ 3: ë‹¤ì–‘í•œ ì§ˆë¬¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"ê¸ˆìœµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"ëŒ€ì¶œì„ ë°›ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\",\n",
    "    \"ì´ììœ¨ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    result = generate_text(prompt, max_new_tokens=80, temperature=0.7)\n",
    "    \n",
    "    # ë‹µë³€ ì¶”ì¶œ\n",
    "    if \"<|im_start|>assistant\" in result:\n",
    "        answer = result.split(\"<|im_start|>assistant\")[-1]\n",
    "        answer = answer.split(\"<|im_end|>\")[0].strip()\n",
    "    else:\n",
    "        answer = result\n",
    "    \n",
    "    print(f\"\\n[ì§ˆë¬¸ {i}] {question}\")\n",
    "    print(f\"[ë‹µë³€] {answer}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ í‰ê°€\n",
    "\n",
    "ê°„ë‹¨í•œ perplexity ê³„ì‚° ë° ì„±ëŠ¥ ì¸¡ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity ê³„ì‚°\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì˜ perplexity ê³„ì‚°\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "    \n",
    "    perplexity = math.exp(loss)\n",
    "    return perplexity, loss\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸\n",
    "test_texts = [\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” MOAI-LLMì…ë‹ˆë‹¤.\",\n",
    "    \"ê¸ˆìœµ ì„œë¹„ìŠ¤ëŠ” ê³ ê°ì˜ ìì‚°ì„ ê´€ë¦¬í•˜ê³  ì¦ì‹ì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\",\n",
    "    \"asdkfjlasjdf lkjasldkfj alskdjf\",  # ë¬´ì‘ìœ„ í…ìŠ¤íŠ¸ (ë†’ì€ perplexity ì˜ˆìƒ)\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Perplexity í‰ê°€\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    ppl, loss = calculate_perplexity(text)\n",
    "    print(f\"\\ní…ìŠ¤íŠ¸: {text}\")\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    print(f\"Perplexity: {ppl:.2f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ëŒ€í™”í˜• ì±„íŒ… (ê°„ë‹¨í•œ ë²„ì „)\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ê³¼ ê°„ë‹¨í•˜ê²Œ ëŒ€í™”í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"ê°„ë‹¨í•œ ì±„íŒ… í•¨ìˆ˜\"\"\"\n",
    "    prompt = f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    result = generate_text(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    \n",
    "    # ë‹µë³€ ì¶”ì¶œ\n",
    "    if \"<|im_start|>assistant\" in result:\n",
    "        answer = result.split(\"<|im_start|>assistant\")[-1]\n",
    "        answer = answer.split(\"<|im_end|>\")[0].strip()\n",
    "    else:\n",
    "        answer = result\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"=\" * 60)\n",
    "print(\"ì±„íŒ… í…ŒìŠ¤íŠ¸ (ì…ë ¥í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì—¬ê¸°ì— ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”\n",
    "my_question = \"ì•ˆë…•í•˜ì„¸ìš”. ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "answer = chat(my_question, max_new_tokens=100, temperature=0.7)\n",
    "print(f\"\\nì§ˆë¬¸: {my_question}\")\n",
    "print(f\"ë‹µë³€: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ê°€ ì§ˆë¬¸ (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•´ì„œ ì‹¤í–‰í•˜ì„¸ìš”)\n",
    "my_questions = [\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì´ ë­”ê°€ìš”?\",\n",
    "    \"íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ë ¤ë©´?\",\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œìš”?\",\n",
    "]\n",
    "\n",
    "for q in my_questions:\n",
    "    answer = chat(q, max_new_tokens=80)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì €ì¥ ë° ê³µìœ \n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ HuggingFace Hubì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Hub ë¡œê·¸ì¸ (ì„ íƒì )\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# ëª¨ë¸ ì—…ë¡œë“œ\n",
    "# model.push_to_hub(\"your-username/moai-llm-finance-ko\")\n",
    "# tokenizer.push_to_hub(\"your-username/moai-llm-finance-ko\")\n",
    "\n",
    "print(\"ğŸ’¡ HuggingFace Hubì— ì—…ë¡œë“œí•˜ë ¤ë©´ ìœ„ ì½”ë“œì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìš”ì•½\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œ ì™„ë£Œí•œ ì‘ì—…:\n",
    "\n",
    "âœ… **1ë‹¨ê³„**: í™˜ê²½ í™•ì¸ (GPU, ë¼ì´ë¸ŒëŸ¬ë¦¬)  \n",
    "âœ… **2ë‹¨ê³„**: í† í¬ë‚˜ì´ì € í•™ìŠµ (Wikipedia ë°ì´í„°ì…‹)  \n",
    "âœ… **3ë‹¨ê³„**: ì‚¬ì „í•™ìŠµ (Pretrain) - ì–¸ì–´ ëª¨ë¸ ê¸°ì´ˆ  \n",
    "âœ… **4ë‹¨ê³„**: íŒŒì¸íŠœë‹ (SFT) - Q&A í•™ìŠµ  \n",
    "âœ… **5ë‹¨ê³„**: ì¶”ë¡  í…ŒìŠ¤íŠ¸ - í…ìŠ¤íŠ¸ ìƒì„±  \n",
    "âœ… **6ë‹¨ê³„**: ëª¨ë¸ í‰ê°€ - Perplexity ì¸¡ì •  \n",
    "âœ… **7ë‹¨ê³„**: ëŒ€í™”í˜• ì±„íŒ… í…ŒìŠ¤íŠ¸  \n",
    "\n",
    "---\n",
    "\n",
    "## ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´:\n",
    "\n",
    "1. **í† í¬ë‚˜ì´ì €**: `vocab_size=128000`, ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "2. **ì‚¬ì „í•™ìŠµ**: ë” ë§ì€ steps, ë” í° ë°ì´í„°ì…‹ (Wikipedia ì „ì²´, C4 ë“±)\n",
    "3. **SFT**: ë” ë§ì€ epochs, ë” ë‹¤ì–‘í•œ Q&A ë°ì´í„°ì…‹\n",
    "4. **í‰ê°€**: ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ì •ëŸ‰ í‰ê°€\n",
    "\n",
    "### ê³ ê¸‰ ê¸°ëŠ¥:\n",
    "\n",
    "- **ê¸´ ì»¨í…ìŠ¤íŠ¸**: YaRNìœ¼ë¡œ 32K â†’ 128K í™•ì¥\n",
    "- **ë©€í‹° GPU**: DeepSpeed, FSDP ì‚¬ìš©\n",
    "- **ìµœì í™”**: Flash Attention, Gradient Checkpointing\n",
    "- **ë°°í¬**: vLLM, TensorRT-LLMìœ¼ë¡œ ì¶”ë¡  ìµœì í™”\n",
    "\n",
    "### ì°¸ê³  ë¬¸ì„œ:\n",
    "\n",
    "- `QUICKSTART.md` - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ\n",
    "- `USER_GUIDE.md` - ì™„ì „í•œ í•™ìŠµ ê°€ì´ë“œ\n",
    "- `DATASETS.md` - ë°ì´í„°ì…‹ ì„ íƒ ê°€ì´ë“œ\n",
    "- `ARCHITECTURE.md` - Qwen3 ì•„í‚¤í…ì²˜ ìƒì„¸\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! MOAI-LLM í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}