{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOAI-LLM Basic Usage\n",
    "\n",
    "This notebook demonstrates basic usage of MOAI-LLM:\n",
    "1. Training a tokenizer\n",
    "2. Creating a model from scratch\n",
    "3. Training (small example)\n",
    "4. Inference\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from moai_llm.config import MoaiConfig, MoaiConfig3B\n",
    "from moai_llm.modeling.model import MoaiForCausalLM\n",
    "from moai_llm.losses import create_loss_function\n",
    "from moai_llm.data import HierarchicalBalancePacker\n",
    "from moai_llm.inference import MoaiInferencePipeline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use predefined 3B config\n",
    "config = MoaiConfig3B()\n",
    "\n",
    "# Option 2: Create custom config\n",
    "# config = MoaiConfig(\n",
    "#     vocab_size=128000,\n",
    "#     hidden_size=3840,\n",
    "#     num_hidden_layers=28,\n",
    "#     num_attention_heads=28,\n",
    "#     num_key_value_heads=4,\n",
    "# )\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Layers: {config.num_hidden_layers}\")\n",
    "print(f\"Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"KV heads: {config.num_key_value_heads}\")\n",
    "print(f\"Vocab size: {config.vocab_size}\")\n",
    "print(f\"Max sequence: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model from config\n",
    "model = MoaiForCausalLM(config)\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input\n",
    "batch_size = 2\n",
    "seq_length = 128\n",
    "\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "print(f\"Loss: {outputs.loss.item():.4f}\")\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Expected shape: (batch_size={batch_size}, seq_len={seq_length}, vocab_size={config.vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample logits and labels\n",
    "vocab_size = 1000\n",
    "logits = torch.randn(batch_size * seq_length, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size * seq_length,))\n",
    "\n",
    "# Test different loss functions\n",
    "print(\"Loss Function Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cross-entropy\n",
    "ce_loss = create_loss_function({\"type\": \"cross_entropy\"})\n",
    "ce_value = ce_loss(logits, labels)\n",
    "print(f\"Cross-Entropy: {ce_value.item():.4f}\")\n",
    "\n",
    "# Focal loss\n",
    "focal_loss = create_loss_function({\"type\": \"focal\", \"params\": {\"gamma\": 2.0}})\n",
    "focal_value = focal_loss(logits, labels)\n",
    "print(f\"Focal Loss (γ=2.0): {focal_value.item():.4f}\")\n",
    "\n",
    "# Label smoothing\n",
    "smooth_loss = create_loss_function({\"type\": \"label_smoothing\", \"params\": {\"smoothing\": 0.1}})\n",
    "smooth_value = smooth_loss(logits, labels)\n",
    "print(f\"Label Smoothing (α=0.1): {smooth_value.item():.4f}\")\n",
    "\n",
    "# Multi-objective\n",
    "multi_loss = create_loss_function({\n",
    "    \"type\": \"multi_objective\",\n",
    "    \"params\": {\n",
    "        \"ce_weight\": 0.6,\n",
    "        \"focal_weight\": 0.3,\n",
    "        \"smooth_weight\": 0.1,\n",
    "    }\n",
    "})\n",
    "multi_value = multi_loss(logits, labels)\n",
    "print(f\"Multi-Objective: {multi_value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Sequence Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sequences of varying lengths\n",
    "sequences = [\n",
    "    list(range(100, 200)),      # Length 100\n",
    "    list(range(200, 450)),      # Length 250\n",
    "    list(range(450, 600)),      # Length 150\n",
    "    list(range(600, 1100)),     # Length 500\n",
    "    list(range(1100, 1300)),    # Length 200\n",
    "    list(range(1300, 1450)),    # Length 150\n",
    "    list(range(1450, 1850)),    # Length 400\n",
    "    list(range(1850, 1950)),    # Length 100\n",
    "]\n",
    "\n",
    "print(f\"Original sequences: {len(sequences)}\")\n",
    "print(f\"Sequence lengths: {[len(s) for s in sequences]}\")\n",
    "print(f\"Total tokens: {sum(len(s) for s in sequences)}\")\n",
    "\n",
    "# Create packer\n",
    "packer = HierarchicalBalancePacker(\n",
    "    max_seq_length=512,\n",
    "    num_bins=4,\n",
    ")\n",
    "\n",
    "# Pack sequences\n",
    "packed = packer.pack(sequences, return_tensors=True)\n",
    "\n",
    "print(f\"\\nPacked batches: {len(packed)}\")\n",
    "print(f\"Total capacity: {len(packed) * 512}\")\n",
    "print(f\"Efficiency: {sum(len(s) for s in sequences) / (len(packed) * 512):.2%}\")\n",
    "\n",
    "# Show packing details\n",
    "for i, pack in enumerate(packed):\n",
    "    print(f\"\\nBatch {i+1}:\")\n",
    "    print(f\"  Sequences packed: {pack.num_sequences}\")\n",
    "    print(f\"  Sequence lengths: {pack.sequence_lengths}\")\n",
    "    print(f\"  Total tokens: {sum(pack.sequence_lengths)}\")\n",
    "    print(f\"  Padding: {512 - sum(pack.sequence_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Example\n",
    "\n",
    "Note: This requires a trained model and tokenizer. Uncomment when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when you have a trained model\n",
    "\n",
    "# # Initialize inference pipeline\n",
    "# pipeline = MoaiInferencePipeline(\n",
    "#     model_path=\"outputs/moai-3b/final_model\",\n",
    "#     tokenizer_path=\"tokenizers/moai_tokenizer\",\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "\n",
    "# # Simple generation\n",
    "# text = pipeline.generate(\n",
    "#     \"Once upon a time\",\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0.7,\n",
    "# )\n",
    "# print(\"Generated text:\")\n",
    "# print(text)\n",
    "\n",
    "# # Chat example\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "# ]\n",
    "# response = pipeline.chat(messages, max_new_tokens=200)\n",
    "# print(\"\\nChat response:\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"test_model\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir)\n",
    "config.save_pretrained(output_dir)\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = MoaiForCausalLM.from_pretrained(output_dir)\n",
    "print(f\"Model loaded from: {output_dir}\")\n",
    "\n",
    "# Verify loaded model\n",
    "loaded_params = sum(p.numel() for p in loaded_model.parameters())\n",
    "print(f\"Loaded model parameters: {loaded_params:,}\")\n",
    "print(f\"Match original: {loaded_params == total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Creating model configuration\n",
    "- ✅ Initializing MOAI-LLM model\n",
    "- ✅ Testing forward pass\n",
    "- ✅ Comparing loss functions\n",
    "- ✅ Using sequence packing\n",
    "- ✅ Saving and loading models\n",
    "\n",
    "Next steps:\n",
    "1. Train a tokenizer: `python scripts/train_tokenizer.py`\n",
    "2. Pre-train the model: `python scripts/pretrain.py`\n",
    "3. Evaluate: `python scripts/evaluate.py`\n",
    "4. Use inference pipeline for generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
