# BCCard Finance ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì‹œ

## ğŸ“‹ ë°ì´í„°ì…‹ ì •ë³´
- **ì´ë¦„**: BCCard/BCAI-Finance-Kor
- **ìƒ˜í”Œ ìˆ˜**: ~100,000ê°œ
- **ì–¸ì–´**: í•œêµ­ì–´
- **í¬ë§·**: input/output
- **ìš©ë„**: ê¸ˆìœµ ë„ë©”ì¸ SFT

---

## ğŸ¯ ì „ì²´ ì›Œí¬í”Œë¡œìš°

### 1ë‹¨ê³„: í† í¬ë‚˜ì´ì € í•™ìŠµ (ë‹¤êµ­ì–´)

```bash
# Step 1: ë‹¤êµ­ì–´ ê¸°ë³¸ í† í¬ë‚˜ì´ì € (64K)
python train_tokenizer.py \
    --multilingual ko en ja zh \
    --vocab_size 64000 \
    --max_samples_per_lang 60000 \
    --turbo \
    --output_dir tokenizers/ \
    --model_prefix moai_multilingual
```

**ë˜ëŠ” ë¡œì»¬ íŒŒì¼ë¡œ í•™ìŠµ:**
```bash
python train_tokenizer.py \
    --input_files data/tokenizer_train/*.txt \
    --vocab_size 64000 \
    --turbo \
    --output_dir tokenizers/ \
    --model_prefix moai
```

**ì¤‘ìš”**: í† í¬ë‚˜ì´ì €ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµí•©ë‹ˆë‹¤ (Q&A ì•„ë‹˜)
- Wikipedia ë‹¤êµ­ì–´
- ë‰´ìŠ¤ í…ìŠ¤íŠ¸
- ì›¹ í¬ë¡¤ë§ ë°ì´í„°

---

### 2ë‹¨ê³„: í† í¬ë‚˜ì´ì € í™•ì¥ (ê¸ˆìœµ ë„ë©”ì¸)

```bash
# Step 2: Alpaca í•œêµ­ì–´ ë°ì´í„°ë¡œ í™•ì¥ (+16K â†’ 80K)
python train_tokenizer.py \
    --base_tokenizer tokenizers/moai_multilingual \
    --dataset unoooo/alpaca-korean \
    --vocab_size 80000 \
    --max_samples 30000 \
    --turbo \
    --output_dir tokenizers/ \
    --model_prefix moai_alpaca

# Step 3: ê¸ˆìœµ ë°ì´í„°ë¡œ í™•ì¥ (+16K â†’ 96K)
python train_tokenizer.py \
    --base_tokenizer tokenizers/moai_alpaca \
    --dataset Mineru/kor-open-finance \
    --vocab_size 96000 \
    --max_samples 30000 \
    --turbo \
    --output_dir tokenizers/ \
    --model_prefix moai_finance

# Step 4: BCCard ê¸ˆìœµ ë°ì´í„°ë¡œ í™•ì¥ (+32K â†’ 128K)
python train_tokenizer.py \
    --base_tokenizer tokenizers/moai_finance \
    --dataset BCCard/BCAI-Finance-Kor \
    --vocab_size 128000 \
    --max_samples 100000 \
    --turbo \
    --output_dir tokenizers/ \
    --model_prefix moai_finance_bccard
```

---

### 3ë‹¨ê³„: ì‚¬ì „í•™ìŠµ (ì¼ë°˜ ì–¸ì–´ ëŠ¥ë ¥)

```bash
# í•œêµ­ì–´ Wikipediaë¡œ ì‚¬ì „í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --output_dir outputs/pretrain-korean \
    --batch_size 16 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-6 \
    --bf16 \
    --gradient_checkpointing
```

**ë˜ëŠ” ì˜ì–´+í•œêµ­ì–´ í˜¼í•©:**
```bash
# 1. ì˜ì–´ Wikipedia
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.en \
    --tokenizer_path tokenizers/moai \
    --output_dir outputs/pretrain-en \
    --max_steps 50000

# 2. í•œêµ­ì–´ Wikipedia (ì´ì–´ì„œ)
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --tokenizer_path tokenizers/moai \
    --pretrained_model outputs/pretrain-en/final_model \
    --output_dir outputs/pretrain-en-ko \
    --max_steps 20000
```

---

### 4ë‹¨ê³„: SFT with BCCard ë°ì´í„°ì…‹ â­

```bash
# BCCard ë°ì´í„°ì…‹ìœ¼ë¡œ ê¸ˆìœµ ë„ë©”ì¸ íŒŒì¸íŠœë‹
python train.py \
    --mode sft \
    --dataset BCCard/BCAI-Finance-Kor \
    --tokenizer_path tokenizers/moai_finance_bccard \
    --pretrained_model outputs/pretrain-korean/final_model \
    --output_dir outputs/sft-bccard \
    --batch_size 4 \
    --learning_rate 1e-5 \
    --num_epochs 3 \
    --bf16
```

**ì™„ë£Œ!** ì´ì œ ê¸ˆìœµ ë„ë©”ì¸ Q&A ëª¨ë¸ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## ğŸ” ë°ì´í„° í¬ë§· ìë™ ë³€í™˜

**ì›ë³¸ ë°ì´í„°:**
```json
{
  "input": "ì‹ ìš©ì¹´ë“œ ì—°íšŒë¹„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
  "output": "ì—°íšŒë¹„ëŠ” ì¹´ë“œ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¼ë°˜ ì¹´ë“œëŠ” ë¬´ë£Œë¶€í„° 5ë§Œì›ê¹Œì§€..."
}
```

**ìë™ ë³€í™˜ ê²°ê³¼:**
```text
<|im_start|>user
ì‹ ìš©ì¹´ë“œ ì—°íšŒë¹„ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?<|im_end|>
<|im_start|>assistant
ì—°íšŒë¹„ëŠ” ì¹´ë“œ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ì¼ë°˜ ì¹´ë“œëŠ” ë¬´ë£Œë¶€í„° 5ë§Œì›ê¹Œì§€...<|im_end|>
```

`train.py`ê°€ ìë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤!

---

## ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
# ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python train.py \
    --mode sft \
    --dataset BCCard/BCAI-Finance-Kor \
    --tokenizer_path tokenizers/moai_finance_bccard \
    --pretrained_model outputs/pretrain-korean/final_model \
    --output_dir outputs/test-bccard \
    --max_steps 100 \
    --batch_size 2
```

---

## ğŸ’¡ íŒ

### 1. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°

```python
from datasets import load_dataset

dataset = load_dataset("BCCard/BCAI-Finance-Kor")
print(dataset["train"][0])
# {'input': 'ì§ˆë¬¸...', 'output': 'ë‹µë³€...'}
```

### 2. ì—¬ëŸ¬ SFT ë°ì´í„°ì…‹ ìˆœì°¨ í•™ìŠµ

```bash
# 1. ì¼ë°˜ í•œêµ­ì–´ SFT
python train.py \
    --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --tokenizer_path tokenizers/moai_finance_bccard \
    --pretrained_model outputs/pretrain-korean/final_model \
    --output_dir outputs/sft-general-korean

# 2. ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” (ì´ì–´ì„œ)
python train.py \
    --mode sft \
    --dataset BCCard/BCAI-Finance-Kor \
    --tokenizer_path tokenizers/moai_finance_bccard \
    --pretrained_model outputs/sft-general-korean/final_model \
    --output_dir outputs/sft-finance-korean
```

### 3. í† í¬ë‚˜ì´ì € í•™ìŠµ ëª¨ë“œ

| ëª¨ë“œ | ì„¤ëª… | ì†ë„ |
|------|------|------|
| ê¸°ë³¸ | BPE, ë†’ì€ í’ˆì§ˆ | 1x |
| `--fast` | min_freq=5, limit_alphabet=10K | 10x |
| `--turbo` | min_freq=10, limit_alphabet=5K | 20x |
| `--ultrafast` | Unigram ì•Œê³ ë¦¬ì¦˜ (merge ì—†ìŒ) | 50x |

---

## âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìš”ì•½

```
1. í† í¬ë‚˜ì´ì € í•™ìŠµ (ë‹¤êµ­ì–´)
   â†“ (Wikipedia ë‹¤êµ­ì–´)

2. í† í¬ë‚˜ì´ì € í™•ì¥ (ë„ë©”ì¸ íŠ¹í™”)
   â†“ (ê¸ˆìœµ ë°ì´í„°)

3. ì‚¬ì „í•™ìŠµ (Pretrain)
   â†“ (Wikipedia ë“±)

4. SFT (Fine-tuning)
   â†“ (BCCard ë°ì´í„°ì…‹)

5. ê¸ˆìœµ Q&A ëª¨ë¸ ì™„ì„±! ğŸ‰
```

---

## ğŸ¯ í•µì‹¬ ì •ë¦¬

| ë‹¨ê³„ | ë°ì´í„° íƒ€ì… | í¬ë§· | ë°ì´í„°ì…‹ ì˜ˆì‹œ |
|------|------------|------|--------------|
| **í† í¬ë‚˜ì´ì €** | ì¼ë°˜ í…ìŠ¤íŠ¸ | Plain text | Wikipedia, ë‰´ìŠ¤ |
| **í† í¬ë‚˜ì´ì € í™•ì¥** | ë„ë©”ì¸ í…ìŠ¤íŠ¸ | Plain text | ê¸ˆìœµ ë°ì´í„° |
| **ì‚¬ì „í•™ìŠµ** | ì¼ë°˜ í…ìŠ¤íŠ¸ | Plain text | Wikipedia, C4 |
| **SFT** | Q&A | input/output | **BCCard** âœ… |

**BCCard ë°ì´í„°ì…‹ì€ í† í¬ë‚˜ì´ì € í™•ì¥ê³¼ SFTì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤!**
