#!/usr/bin/env python3
"""
ì‚¬ì „ í† í¬ë‚˜ì´ì§• ìŠ¤í¬ë¦½íŠ¸
- í•™ìŠµ ì „ì— í•œ ë²ˆë§Œ ì‹¤í–‰
- ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
- í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
- ì´í›„ ëª¨ë“  í•™ìŠµì—ì„œ ì¬ì‚¬ìš©
"""

import os
import sys
import argparse
import hashlib
from pathlib import Path
import multiprocessing
from transformers import AutoTokenizer
from datasets import load_dataset, Dataset

def parse_args():
    parser = argparse.ArgumentParser(description="Pre-tokenize dataset for training")
    parser.add_argument("--dataset", type=str, required=True, help="HuggingFace dataset name")
    parser.add_argument("--tokenizer", type=str, required=True, help="Tokenizer path")
    parser.add_argument("--output_dir", type=str, default="./tokenized_data", help="Output directory")
    parser.add_argument("--num_proc", type=int, default=None, help="Number of processes (default: CPU count // 6)")
    parser.add_argument("--batch_size", type=int, default=10000, help="Batch size for tokenization")
    parser.add_argument("--max_seq_length", type=int, default=2048, help="Maximum sequence length")
    parser.add_argument("--packing", action="store_true", help="Use sequence packing mode")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")
    return parser.parse_args()

def main():
    args = parse_args()
    
    # CPU ì½”ì–´ ìˆ˜ ê¸°ë°˜ ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜
    if args.num_proc is None:
        cpu_count = multiprocessing.cpu_count()
        args.num_proc = min(32, max(16, cpu_count // 6))
    
    print("="*80)
    print("ğŸš€ Pre-tokenization Script")
    print("="*80)
    print(f"Dataset: {args.dataset}")
    print(f"Tokenizer: {args.tokenizer}")
    print(f"Num processes: {args.num_proc}")
    print(f"Batch size: {args.batch_size}")
    print(f"Packing mode: {args.packing}")
    print(f"CPU cores: {multiprocessing.cpu_count()}")
    print()
    
    # TOKENIZERS_PARALLELISM ì„¤ì •
    # multiprocessing ì‚¬ìš© ì‹œ datasetsê°€ ìë™ìœ¼ë¡œ falseë¡œ ì„¤ì •í•˜ì§€ë§Œ ëª…ì‹œ
    os.environ["TOKENIZERS_PARALLELISM"] = "false"  # datasetsê°€ multiprocessing ì‹œ ê°•ì œ ì„¤ì •
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„°ì…‹ í•´ì‹œ ìƒì„±
    dataset_hash = hashlib.md5(args.dataset.encode()).hexdigest()[:16]
    tokenized_path = output_dir / f"{dataset_hash}_tokenized"
    
    # ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    if tokenized_path.exists():
        print(f"âœ… Tokenized data already exists: {tokenized_path}")
        response = input("Overwrite? (y/N): ")
        if response.lower() != 'y':
            print("Aborted.")
            return
        import shutil
        shutil.rmtree(tokenized_path)
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ“ Loading tokenizer from {args.tokenizer}...")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, use_fast=True)
    
    if not tokenizer.is_fast:
        print("âš ï¸  WARNING: Not using Fast Tokenizer! This will be slow.")
    else:
        print("âœ… Using Fast Tokenizer (Rust-based)")
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“š Loading dataset: {args.dataset}...")
    dataset = load_dataset(args.dataset, split="train")
    print(f"âœ“ Loaded {len(dataset):,} samples")
    
    # 3. í† í¬ë‚˜ì´ì§•
    print(f"\nğŸ”¤ Tokenizing with {args.num_proc} processes...")
    print(f"   Batch size: {args.batch_size:,}")
    print(f"   This will take approximately: {len(dataset) / (args.num_proc * 7000) / 60:.1f} minutes")
    print()
    
    if args.packing:
        # Packing mode: ì‹œí€€ìŠ¤ ì—°ê²°
        def batch_tokenize(examples):
            return tokenizer(
                examples[args.text_column],
                truncation=False,
                padding=False,
                add_special_tokens=True,
            )
        
        tokenized_ds = dataset.map(
            batch_tokenize,
            batched=True,
            batch_size=args.batch_size,
            num_proc=args.num_proc,
            remove_columns=dataset.column_names,
            desc="Tokenizing",
        )
        
        # ì‹œí€€ìŠ¤ ì—°ê²°
        print("\nğŸ“¦ Packing sequences...")
        def pack_sequences(examples):
            concatenated_ids = []
            current_length = 0
            current_batch = []
            
            for ids in examples["input_ids"]:
                if current_length + len(ids) <= args.max_seq_length:
                    current_batch.extend(ids)
                    current_length += len(ids)
                else:
                    if current_batch:
                        # íŒ¨ë”©
                        while len(current_batch) < args.max_seq_length:
                            current_batch.append(tokenizer.pad_token_id)
                        concatenated_ids.append(current_batch)
                    current_batch = ids[:args.max_seq_length]
                    current_length = len(current_batch)
            
            if current_batch:
                while len(current_batch) < args.max_seq_length:
                    current_batch.append(tokenizer.pad_token_id)
                concatenated_ids.append(current_batch)
            
            return {"input_ids": concatenated_ids}
        
        # ë°°ì¹˜ ì²˜ë¦¬
        all_packed = []
        batch_size = 50000
        for i in range(0, len(tokenized_ds), batch_size):
            batch = tokenized_ds[i:i+batch_size]
            packed = pack_sequences(batch)
            all_packed.extend(packed["input_ids"])
            if (i // batch_size + 1) % 10 == 0:
                print(f"  Packed {i+batch_size:,} / {len(tokenized_ds):,} samples...")
        
        final_dataset = Dataset.from_dict({"input_ids": all_packed})
        print(f"âœ“ Packed into {len(final_dataset):,} sequences")
        
    else:
        # ì¼ë°˜ mode: truncation
        def tokenize_function(examples):
            return tokenizer(
                examples[args.text_column],
                truncation=True,
                max_length=args.max_seq_length,
                padding=False,
            )
        
        final_dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=args.batch_size,
            num_proc=args.num_proc,
            remove_columns=dataset.column_names,
            desc="Tokenizing",
        )
    
    # 4. ì €ì¥
    print(f"\nğŸ’¾ Saving to {tokenized_path}...")
    final_dataset.save_to_disk(str(tokenized_path))
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_path = tokenized_path / "metadata.txt"
    with open(metadata_path, 'w') as f:
        f.write(f"dataset={args.dataset}\n")
        f.write(f"tokenizer={args.tokenizer}\n")
        f.write(f"num_samples={len(final_dataset)}\n")
        f.write(f"max_seq_length={args.max_seq_length}\n")
        f.write(f"packing={args.packing}\n")
    
    print()
    print("="*80)
    print("âœ… Pre-tokenization Complete!")
    print("="*80)
    print(f"Output: {tokenized_path}")
    print(f"Samples: {len(final_dataset):,}")
    print()
    print("ğŸ“ To use this in training, add to your train command:")
    print(f"   --pretokenized_data {tokenized_path}")
    print("="*80)

if __name__ == "__main__":
    main()

