#!/bin/bash
# MOAI-LLM Pretrain Script (Multi-Dataset)
# 
# Îç∞Ïù¥ÌÑ∞ÏÖã:
# - sh2orc/bccard-maywell-jojo0217-markai-lcw99-kendamarron-microsoft (instruction/output)
# - BCCard/BCAI-Finance-Kor-1862K (instruction/output)
# - HAERAE-HUB/KOREAN-WEBTEXT (text)
#
# Usage: ./pretrain.sh [config_size]
# Example: 
#   ./pretrain.sh 2b                              # Use tensorboard (default)
#   USE_WANDB=true ./pretrain.sh 2b               # Use W&B with default project
#   USE_WANDB=true WANDB_PROJECT=my-project ./pretrain.sh 2b  # Custom W&B project

set -e

# ============================================================================
# Configuration
# ============================================================================

CONFIG_SIZE=${1:-2b}
NUM_GPUS=${NUM_GPUS:-4}
GPU_MEMORY=${GPU_MEMORY:-32}  # GPU memory in GB (32, 48, 80)

# Model config based on size and GPU memory
case $CONFIG_SIZE in
    2b)
        MODEL_CONFIG="configs/model_config_2b.json"
        case $GPU_MEMORY in
            32)
                BATCH_SIZE=4   # RTX 5090 32GB
                GRADIENT_ACCUMULATION_STEPS=24  # effective = 4*4*24 = 384
                ;;
            48)
                BATCH_SIZE=12  # A40 48GB
                GRADIENT_ACCUMULATION_STEPS=4   # effective = 12*8*4 = 384 (8 GPUs)
                ;;
            80)
                BATCH_SIZE=24  # A100 80GB
                GRADIENT_ACCUMULATION_STEPS=4   # effective = 24*4*4 = 384
                ;;
            *)
                BATCH_SIZE=4
                GRADIENT_ACCUMULATION_STEPS=24
                ;;
        esac
        ;;
    5b)
        MODEL_CONFIG="configs/model_config.json"
        BATCH_SIZE=1
        GRADIENT_ACCUMULATION_STEPS=96  # effective = 1*4*96 = 384
        ;;
    *)
        echo "Unknown config size: $CONFIG_SIZE (use 2b or 5b)"
        exit 1
        ;;
esac

# Common settings
TOKENIZER_PATH="tokenizers/moai"
MAX_SEQ_LENGTH=1024
LEARNING_RATE=1e-4
WARMUP_STEPS=2000
NUM_EPOCHS=2

# Output directory
OUTPUT_DIR="outputs/moai-${CONFIG_SIZE}"

# Logging settings (W&B or Tensorboard)
USE_WANDB=${USE_WANDB:-false}  # Set to "true" to use W&B
WANDB_PROJECT=${WANDB_PROJECT:-"moai-llm"}
WANDB_RUN_NAME=${WANDB_RUN_NAME:-"pretrain-${CONFIG_SIZE}-$(date +%Y%m%d-%H%M%S)"}

# ============================================================================
# Dataset Configuration
# ============================================================================

# Ïó¨Îü¨ HuggingFace Îç∞Ïù¥ÌÑ∞ÏÖã ÏÇ¨Ïö© (ÏΩ§Îßà ÏóÜÏù¥!)
# configÍ∞Ä ÌïÑÏöîÌïú Í≤ΩÏö∞: "dataset_name:config_name" ÌòïÏãù ÏÇ¨Ïö©
DATASETS=(
    "BCCard/BCCard-Finance-Kor-QnA"
    "sh2orc/bccard-maywell-jojo0217-markai-lcw99-kendamarron-microsoft"
    "nvidia/Nemotron-CC-Math-v1:3"
    "nvidia/OpenCodeGeneticInstruct:qwen2.5-32b-instruct"
    "BCCard/BCAI-Finance-Kor-1862K"
    "HAERAE-HUB/KOREAN-WEBTEXT"
)

# Îç∞Ïù¥ÌÑ∞ÏÖã Î∞∞Ïó¥ÏùÄ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö© (Î¨∏ÏûêÏó¥ Î≥ÄÌôò Î∂àÌïÑÏöî)

# ============================================================================
# Environment Setup
# ============================================================================

# CUDA settings - Generate GPU list based on NUM_GPUS
if [ -z "$CUDA_VISIBLE_DEVICES" ]; then
    GPU_LIST=""
    for ((i=0; i<NUM_GPUS; i++)); do
        if [ $i -eq 0 ]; then
            GPU_LIST="$i"
        else
            GPU_LIST="$GPU_LIST,$i"
        fi
    done
    export CUDA_VISIBLE_DEVICES="$GPU_LIST"
fi
# TOKENIZERS_PARALLELISMÏùÄ train.pyÏóêÏÑú ÏûêÎèô ÏÑ§Ï†ïÎê® (num_proc>1 Ïãú false)
# export TOKENIZERS_PARALLELISM=false

# NCCL settings
# P2P ÎπÑÌôúÏÑ±Ìôî: RTX Í≥ÑÏó¥ + A40 (A40ÏùÄ P2P Ïù¥Ïäà ÏûàÏùå)
# A100, H100 Îì±ÏùÄ P2P ÏßÄÏõê
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
if [[ "$GPU_NAME" == *"RTX"* ]] || [[ "$GPU_NAME" == *"GeForce"* ]] || [[ "$GPU_NAME" == *"A40"* ]]; then
    echo "‚ö†Ô∏è  P2P disabled for: $GPU_NAME"
    export NCCL_P2P_DISABLE=1
else
    echo "‚úì P2P enabled for: $GPU_NAME"
    export NCCL_P2P_DISABLE=0
fi

# NCCL Ï∂îÍ∞Ä ÏµúÏ†ÅÌôî
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1  # ÎπÑÎèôÍ∏∞ ÏóêÎü¨ Ï≤òÎ¶¨
export NCCL_IB_DISABLE=1            # InfiniBand ÎπÑÌôúÏÑ±Ìôî (PCIe ÌôòÍ≤Ω)
export NCCL_NET_GDR_LEVEL=0         # GPU Direct RDMA ÎπÑÌôúÏÑ±Ìôî (Ìò∏ÌôòÏÑ±)

# NCCL Timeout ÏÑ§Ï†ï (chunked CE + gradient checkpointingÏúºÎ°ú Ïù∏Ìïú ÎäêÎ¶∞ Ïó∞ÏÇ∞ ÎåÄÏùë)
export TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC=1800  # 30Î∂Ñ (Í∏∞Î≥∏ 480Ï¥à)
export NCCL_TIMEOUT=1800                       # 30Î∂Ñ
export TORCH_DISTRIBUTED_DEBUG=OFF             # ÎîîÎ≤ÑÍ∑∏ ÎπÑÌôúÏÑ±Ìôî (ÏÑ±Îä•)

# Memory optimization
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,garbage_collection_threshold:0.8

# CUDA ÏµúÏ†ÅÌôî
export CUDA_LAUNCH_BLOCKING=0       # ÎπÑÎèôÍ∏∞ Ïª§ÎÑê Ïã§Ìñâ
export TORCH_CUDNN_V8_API_ENABLED=1 # cuDNN v8 API

# CPU ÏµúÏ†ÅÌôî
export OMP_NUM_THREADS=48
export MKL_NUM_THREADS=48

# ============================================================================
# Dataset Loading Optimization (ÎåÄÍ∑úÎ™® Îç∞Ïù¥ÌÑ∞ÏÖã ÏµúÏ†ÅÌôî)
# ============================================================================
# DATASET_NUM_PROC: train.pyÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Ïóê Îî∞Îùº ÏûêÎèô Ï°∞Ï†à
# - >500Îßå: num_proc=8 (ÏïàÏ†ïÏÑ± Ïö∞ÏÑ†)
# - 100Îßå~500Îßå: num_proc=16
# - <100Îßå: num_proc=32 (ÏÜçÎèÑ Ïö∞ÏÑ†)
# ÏàòÎèô ÏÑ§Ï†ï Ïãú: export DATASET_NUM_PROC=16 ./pretrain.sh
# export DATASET_NUM_PROC=${DATASET_NUM_PROC:-48}  # ÏûêÎèô ÌäúÎãùÏúºÎ°ú Î≥ÄÍ≤Ω

echo "üìä Dataset loading settings:"
echo "  - Parallel processes: AUTO (based on dataset size)"
echo "  - Large (>5M): 8 procs | Medium (1-5M): 16 procs | Small (<1M): 32 procs"

# ============================================================================
# Tokenization Optimization (ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï ÏµúÏ†ÅÌôî) ‚ö°‚ö°‚ö°
# ============================================================================
# train.pyÏùò tokenize_dataset()Ïù¥ ÏûêÎèôÏúºÎ°ú ÏµúÏ†Å ÏÑ§Ï†ï Ï†ÅÏö©:
# - TOKENIZERS_PARALLELISM=false (Î©ÄÌã∞ÌîÑÎ°úÏÑ∏Ïã± ÏÇ¨Ïö© Ïãú ÌïÑÏàò)
# - num_proc=48 (Í∞Å ÌîÑÎ°úÏÑ∏Ïä§Í∞Ä ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ïã§Ìñâ)
# - batch_size=50000 (IPC Ïò§Î≤ÑÌó§Îìú ÏµúÏÜåÌôî)

# Python Î©ÄÌã∞ÌîÑÎ°úÏÑ∏Ïã± ÏµúÏ†ÅÌôî
export PYTHONUNBUFFERED=1

# CPU affinity ÏµúÏ†ÅÌôî (Í∞ÄÎä•Ìïú Í≤ΩÏö∞)
export OMP_PROC_BIND=close
export OMP_PLACES=cores

# PyArrow ÏµúÏ†ÅÌôî
export ARROW_DEFAULT_MEMORY_POOL=mimalloc  # Îçî Îπ†Î•∏ Î©îÎ™®Î¶¨ Ìï†ÎãπÏûê
export ARROW_IO_THREADS=16  # I/O Ïä§Î†àÎìú Ïàò

echo "‚ö° Sequential Mode Optimization:"
echo "  - Fast Tokenizer: ENABLED (Rust-based)"
echo "  - Strategy: Pre-tokenize ALL datasets BEFORE DDP"
echo "  - Mode: Single process + Fast Tokenizer threading"
echo "  - CPUs: 96 cores (RAYON_NUM_THREADS=96)"
echo "  - Batch size: 50000 (optimized for speed)"
echo "  - Writer batch: 100000 (optimized I/O)"
echo "  - Cache reuse: ENABLED"
echo "  - Expected: Maximum speed! (no process overhead)"

# TF32 ÌôúÏÑ±Ìôî (Ampere+ GPU, ~2x matmul ÏÜçÎèÑ)
export NVIDIA_TF32_OVERRIDE=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1

# HuggingFace cache (optional)
# export HF_HOME="/path/to/cache"
# export HF_DATASETS_CACHE="/path/to/datasets/cache"

# HuggingFace ÏµúÏ†ÅÌôî (8Í∞ú ÌîÑÎ°úÏÑ∏Ïä§ ÎèôÏãú ÏãúÏûë Ïãú Ï∫êÏãú Í≤ΩÌï© Î∞©ÏßÄ)
export HF_DATASETS_OFFLINE=0  # Ïò®ÎùºÏù∏ Ïú†ÏßÄ (ÌïÑÏöîÏãú Îã§Ïö¥Î°úÎìú)
export HF_HUB_DISABLE_TELEMETRY=1  # ÌÖîÎ†àÎ©îÌä∏Î¶¨ ÎπÑÌôúÏÑ±Ìôî
export TRANSFORMERS_OFFLINE=0  # Ïò®ÎùºÏù∏ Ïú†ÏßÄ
export TRANSFORMERS_NO_ADVISORY_WARNINGS=1  # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÏµúÏÜåÌôî

# Python ÏµúÏ†ÅÌôî (import ÏÜçÎèÑ Ìñ•ÏÉÅ)
export PYTHONDONTWRITEBYTECODE=1  # .pyc ÌååÏùº ÏÉùÏÑ± ÏïàÌï® (SSDÏóêÏÑúÎäî Îçî Îπ†Î¶Ñ)
export PYTHONUNBUFFERED=1  # Î≤ÑÌçº ÏóÜÏù¥ Ï¶âÏãú Ï∂úÎ†•
export PYTHONHASHSEED=0  # hash seed Í≥†Ï†ï (ÏïΩÍ∞ÑÏùò ÏÜçÎèÑ Ìñ•ÏÉÅ)

# CUDA Ï¥àÍ∏∞Ìôî ÏµúÏ†ÅÌôî (8Í∞ú ÌîÑÎ°úÏÑ∏Ïä§ ÎèôÏãú ÏãúÏûë Ïãú Í≤ΩÌï© Î∞©ÏßÄ)
export CUDA_MODULE_LOADING=LAZY  # CUDA Î™®Îìà ÏßÄÏó∞ Î°úÎî©
export TORCH_CUDA_ARCH_LIST="8.0"  # A40ÏùÄ Ampere (8.0), Î∂àÌïÑÏöîÌïú ÏïÑÌÇ§ÌÖçÏ≤ò Ïä§ÌÇµ

# ============================================================================
# Print Configuration
# ============================================================================

echo "========================================================================"
echo "üöÄ MOAI-LLM Pretrain Script (Multi-Dataset)"
echo "========================================================================"
echo "Model Config:          $MODEL_CONFIG"
echo "Config Size:           $CONFIG_SIZE"
echo "Tokenizer:             $TOKENIZER_PATH"
echo "Output:                $OUTPUT_DIR"
echo "========================================================================"
echo "Datasets:"
for ds in "${DATASETS[@]}"; do
    echo "  - $ds"
done
echo "========================================================================"
echo "GPUs:                  $NUM_GPUS"
echo "GPU Memory:            ${GPU_MEMORY}GB"
echo "Batch Size (per GPU):  $BATCH_SIZE"
echo "Gradient Accumulation: $GRADIENT_ACCUMULATION_STEPS"
echo "Effective Batch Size:  $((BATCH_SIZE * NUM_GPUS * GRADIENT_ACCUMULATION_STEPS))"
echo "Max Seq Length:        $MAX_SEQ_LENGTH"
echo "Learning Rate:         $LEARNING_RATE"
echo "Warmup Steps:          $WARMUP_STEPS"
echo "Epochs:                $NUM_EPOCHS"
echo "Mode:                  Sequential (one dataset at a time)"
echo "Packing:               Enabled"
echo "Logging:               $([ "$USE_WANDB" = "true" ] && echo "W&B ($WANDB_PROJECT)" || echo "Tensorboard")"
echo "========================================================================"
echo "üìù Training Flow:"
echo "  1Ô∏è‚É£  Pre-tokenize all datasets (FAST! ~100k+ ex/s)"
echo "  2Ô∏è‚É£  Initialize DDP and load model"
echo "  3Ô∏è‚É£  Train on each dataset sequentially"
echo "  4Ô∏è‚É£  Save checkpoint after each dataset"
echo "========================================================================"

# ============================================================================
# Run Training
# ============================================================================

# Debug: print command
echo "DEBUG: Running command:"
echo "OUTPUT_DIR=$OUTPUT_DIR"
echo "torchrun --nproc_per_node=$NUM_GPUS --master_port=29500 train.py \\"
echo "  --mode pretrain \\"
echo "  --dataset ${DATASETS[*]} \\"
echo "  --tokenizer_path $TOKENIZER_PATH \\"
echo "  --model_config $MODEL_CONFIG \\"
echo "  --output_dir $OUTPUT_DIR \\"
echo "  ..."

# Find available port
MASTER_PORT=${MASTER_PORT:-29500}
while lsof -Pi :$MASTER_PORT -sTCP:LISTEN -t >/dev/null 2>&1 ; do
    MASTER_PORT=$((MASTER_PORT + 1))
done
echo "Using master port: $MASTER_PORT"

# Build wandb arguments conditionally
WANDB_ARGS=""
if [ "$USE_WANDB" = "true" ]; then
    WANDB_ARGS="--use_wandb --wandb_project $WANDB_PROJECT --wandb_run_name $WANDB_RUN_NAME"
fi

# ============================================================================
# üöÄ STEP 1: Pre-tokenize datasets (BEFORE torchrun!)
# ============================================================================
echo ""
echo "========================================================================"
echo "üî• STEP 1: Pre-tokenizing all datasets (before DDP)"
echo "========================================================================"
echo "‚ö° Tokenization settings:"
echo "  - num_proc: AUTO (8 for >5M, 16 for 1-5M, 32 for <1M samples)"
echo "  - Each process runs tokenizer independently"
echo ""

# train.pyÍ∞Ä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¨Í∏∞Ïóê Îî∞Îùº num_proc ÏûêÎèô Ï°∞Ï†à
python train.py \
    --mode pretrain \
    --dataset "${DATASETS[@]}" \
    --tokenizer_path "$TOKENIZER_PATH" \
    --model_config "$MODEL_CONFIG" \
    --output_dir "$OUTPUT_DIR" \
    --max_seq_length "$MAX_SEQ_LENGTH" \
    --packing \
    --tokenize_only

echo "‚úÖ Pre-tokenization completed!"
echo "========================================================================"
echo ""

# ============================================================================
# üöÄ STEP 2: Run distributed training with torchrun
# ============================================================================
echo "========================================================================"
echo "üöÄ STEP 2: Starting distributed training"
echo "========================================================================"

torchrun \
    --nproc_per_node=$NUM_GPUS \
    --master_port=$MASTER_PORT \
    train.py \
    --mode pretrain \
    --dataset "${DATASETS[@]}" \
    --tokenizer_path "$TOKENIZER_PATH" \
    --model_config "$MODEL_CONFIG" \
    --output_dir "$OUTPUT_DIR" \
    --batch_size "$BATCH_SIZE" \
    --gradient_accumulation_steps "$GRADIENT_ACCUMULATION_STEPS" \
    --max_seq_length "$MAX_SEQ_LENGTH" \
    --learning_rate "$LEARNING_RATE" \
    --warmup_steps "$WARMUP_STEPS" \
    --num_epochs "$NUM_EPOCHS" \
    --bf16 \
    --gradient_checkpointing \
    --packing \
    --sequential \
    --flash_attention \
    --num_proc 48 \
    --dataloader_num_workers 8 \
    --logging_steps 10 \
    --save_steps 500 \
    --save_total_limit 3 \
    $WANDB_ARGS

echo "========================================================================"
echo "‚úÖ Pretrain completed!"
echo "üìÅ Model saved to: $OUTPUT_DIR/final_model"
echo "========================================================================"

