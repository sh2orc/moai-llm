#!/usr/bin/env python3
"""
Tokenize Datasets Script (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

ëª¨ë“  ë°ì´í„°ì…‹ì„ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ í† í¬ë‚˜ì´ì§•í•˜ì—¬ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤.
ì´í›„ train.pyë¥¼ --skip_tokenization í”Œë˜ê·¸ë¡œ ì‹¤í–‰í•˜ë©´ ìºì‹œì—ì„œ ë°”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

Usage:
    python tokenize_datasets.py \\
        --dataset "dataset1" "dataset2" \\
        --tokenizer_path "tokenizers/moai" \\
        --max_seq_length 1024 \\
        --packing

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” torchrun ì—†ì´ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
TOKENIZERS_PARALLELISM=trueë¡œ Rust Fast Tokenizerì˜ ë‚´ë¶€ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤.
"""

# âš ï¸ ì¤‘ìš”: ëª¨ë“  import ì „ì— TOKENIZERS_PARALLELISM ì„¤ì •!
# tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ import ì‹œì ì— ì´ ê°’ì„ ìºì‹±í•˜ë¯€ë¡œ ê°€ì¥ ë¨¼ì € ì„¤ì •í•´ì•¼ í•¨
import os
os.environ["TOKENIZERS_PARALLELISM"] = "true"

# Rust Rayon ìŠ¤ë ˆë“œ í’€ ìµœì í™”
cpu_count = os.cpu_count() or 8
os.environ["RAYON_NUM_THREADS"] = str(cpu_count)

import argparse
import hashlib
import logging
import sys
from pathlib import Path

# ============================================================================
# Logging Setup
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Import train.py functions
# ============================================================================
# train.pyì˜ í•¨ìˆ˜ë“¤ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤
from train import (
    load_pretrain_dataset,
    tokenize_dataset,
    concatenate_sequences,
    get_cache_version_key,
    get_optimal_num_shards,
)

# ============================================================================
# Check for Rust tokenizer availability
# ============================================================================
def check_rust_tokenizer():
    """Check if Rust tokenizer binary is available"""
    rust_binary = Path(__file__).parent / "tokenize_rust" / "target" / "release" / "moai-tokenizer"
    return rust_binary.exists()

RUST_AVAILABLE = check_rust_tokenizer()


def tokenize_single_dataset(
    source: tuple,
    tokenizer,
    args,
    idx: int,
):
    """
    ë‹¨ì¼ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• (ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)

    Args:
        source: ("hf", "dataset_name") ë˜ëŠ” ("file", "path")
        tokenizer: í† í¬ë‚˜ì´ì €
        args: ì¸ì
        idx: ì†ŒìŠ¤ ì¸ë±ìŠ¤
    """
    import gc
    from datasets import Dataset as HFDataset

    src_type, src_name = source
    cache_home = os.environ.get("HF_HOME", os.path.expanduser("~/.cache/huggingface"))

    # ìºì‹œ ê²½ë¡œ ê³„ì‚°
    cache_version = get_cache_version_key(
        tokenizer,
        additional_info=f"packing_{args.packing}_maxlen_{args.max_seq_length}_seq_{idx}"
    )
    dataset_hash = hashlib.md5(f"{src_name}_{cache_version}".encode()).hexdigest()[:16]
    tokenized_cache_path = Path(cache_home) / "datasets" / f"{dataset_hash}_tokenized"
    tokenized_marker = Path(cache_home) / "datasets" / f".{dataset_hash}_tokenized.marker"

    logger.info(f"ğŸ“¦ [{idx+1}] Dataset: {src_name}")

    # ìºì‹œ í™•ì¸
    if tokenized_cache_path.exists() and tokenized_marker.exists():
        logger.info(f"  âœ… Already tokenized (using cache)")
        tokenized_dataset = HFDataset.load_from_disk(str(tokenized_cache_path))
        num_samples = len(tokenized_dataset)
        logger.info(f"  âœ… Cached: {num_samples:,} samples")
        del tokenized_dataset
        gc.collect()
        return

    # ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info(f"  ğŸ“š Loading dataset...")
    if src_type == "hf":
        dataset, text_column = load_pretrain_dataset(
            dataset_names=[src_name],
            dataset_config=args.dataset_config if idx == 0 else None,
            train_files=None,
            text_column=args.text_column,
        )
    else:
        dataset, text_column = load_pretrain_dataset(
            dataset_names=None,
            dataset_config=None,
            train_files=[src_name],
            text_column=args.text_column,
        )

    # í† í¬ë‚˜ì´ì§• - Rust ìš°ì„ , Python fallback
    logger.info(f"  ğŸ”¤ Tokenizing dataset...")

    if RUST_AVAILABLE:
        logger.info(f"  ğŸš€ Using Rust tokenizer (ultra-fast mode)")
        from tokenize_rust_wrapper import tokenize_with_rust

        tokenized_ds = tokenize_with_rust(
            dataset=dataset["train"],
            tokenizer_path=args.tokenizer_path,
            text_column=text_column,
            max_seq_length=0 if args.packing else args.max_seq_length,
            batch_size=10000,
        )
    else:
        logger.info(f"  ğŸ Using Python tokenizer (Rust not available)")
        logger.info(f"     TOKENIZERS_PARALLELISM={os.environ.get('TOKENIZERS_PARALLELISM', 'not set')}")
        logger.info(f"     Tip: Run ./setup_rust_tokenizer.sh for 25x faster tokenization!")
        tokenized_ds = tokenize_dataset(
            dataset=dataset["train"],
            tokenizer=tokenizer,
            text_column=text_column,
            max_seq_length=args.max_seq_length,
            packing=args.packing,
        )

    # Packing (ì„ íƒì ) - PyArrow ìŠ¤íŠ¸ë¦¬ë° + ì ì§„ì  ë””ìŠ¤í¬ ì“°ê¸°
    if args.packing:
        import tempfile
        import shutil
        from datasets import load_from_disk, concatenate_datasets

        logger.info(f"  ğŸ“¦ Packing sequences (incremental disk writing)...")

        total_samples = len(tokenized_ds)
        logger.info(f"     Total samples: {total_samples:,}")

        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = Path(tempfile.mkdtemp(prefix="moai_packing_"))
        try:
            # 1. í† í¬ë‚˜ì´ì§• ê²°ê³¼ë¥¼ ì„ì‹œ Arrow íŒŒì¼ë¡œ ì €ì¥
            logger.info(f"     Saving tokenized data to disk...")
            tokenized_ds.save_to_disk(str(temp_dir / "tokenized"))
            del tokenized_ds
            gc.collect()

            # 2. ë°°ì¹˜ë³„ packing í›„ ì¦‰ì‹œ ë””ìŠ¤í¬ì— ì €ì¥
            STREAM_BATCH_SIZE = 500000  # 50ë§Œ ìƒ˜í”Œì”©
            dataset_on_disk = load_from_disk(str(temp_dir / "tokenized"))
            num_batches = (total_samples + STREAM_BATCH_SIZE - 1) // STREAM_BATCH_SIZE

            logger.info(f"     Processing {num_batches} batches of {STREAM_BATCH_SIZE:,} samples")

            packed_shards_dir = temp_dir / "packed_shards"
            packed_shards_dir.mkdir()

            for batch_idx in range(num_batches):
                start_idx = batch_idx * STREAM_BATCH_SIZE
                end_idx = min(start_idx + STREAM_BATCH_SIZE, total_samples)

                logger.info(f"     Batch {batch_idx+1}/{num_batches}: Loading & packing {start_idx:,} - {end_idx:,}")

                # ë°°ì¹˜ ë¡œë“œ ë° packing
                batch_data = dataset_on_disk.select(range(start_idx, end_idx))
                tokenized_list = [{"input_ids": ids} for ids in batch_data["input_ids"]]
                del batch_data
                gc.collect()

                packed_batch = concatenate_sequences(
                    tokenized_sequences=tokenized_list,
                    max_seq_length=args.max_seq_length,
                    eos_token_id=tokenizer.eos_token_id,
                )
                del tokenized_list
                gc.collect()

                # ì¦‰ì‹œ ë””ìŠ¤í¬ì— ì €ì¥ (ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€)
                packed_batch_ds = HFDataset.from_list(packed_batch)
                shard_path = packed_shards_dir / f"shard_{batch_idx:04d}"
                packed_batch_ds.save_to_disk(str(shard_path))

                logger.info(f"     âœ“ Batch {batch_idx+1}/{num_batches} saved ({len(packed_batch):,} chunks)")

                del packed_batch, packed_batch_ds
                gc.collect()

            del dataset_on_disk
            gc.collect()

            # 3. ëª¨ë“  ìƒ¤ë“œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³‘í•© (Arrowê°€ ì•Œì•„ì„œ ìµœì í™”)
            logger.info(f"     Merging {num_batches} shards...")
            shard_datasets = []
            for batch_idx in range(num_batches):
                shard_path = packed_shards_dir / f"shard_{batch_idx:04d}"
                shard_datasets.append(load_from_disk(str(shard_path)))

            tokenized_dataset = concatenate_datasets(shard_datasets)
            logger.info(f"  âœ“ Total packed chunks: {len(tokenized_dataset):,}")

            del shard_datasets
            gc.collect()

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            logger.info(f"     Cleaning up temporary files...")
            shutil.rmtree(temp_dir, ignore_errors=True)
    else:
        tokenized_dataset = tokenized_ds

    # ì €ì¥
    logger.info(f"  ğŸ’¾ Saving tokenized dataset...")
    num_shards = get_optimal_num_shards(len(tokenized_dataset), os.cpu_count() or 8)
    tokenized_dataset.save_to_disk(str(tokenized_cache_path), num_shards=num_shards)
    tokenized_marker.touch()
    num_samples = len(tokenized_dataset)
    logger.info(f"  âœ… Tokenized: {num_samples:,} samples (shards={num_shards})")

    # ë©”ëª¨ë¦¬ í•´ì œ
    del dataset
    del tokenized_dataset
    gc.collect()


def main():
    parser = argparse.ArgumentParser(description="Tokenize datasets for MOAI-LLM")

    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument("--dataset", nargs="+", help="HuggingFace dataset names")
    parser.add_argument("--dataset_config", type=str, help="Dataset config name")
    parser.add_argument("--train_file", nargs="+", help="Local training files (JSONL)")
    parser.add_argument("--text_column", type=str, default="text", help="Text column name")

    # í† í¬ë‚˜ì´ì € ì„¤ì •
    parser.add_argument("--tokenizer_path", type=str, required=True, help="Tokenizer path")

    # í† í¬ë‚˜ì´ì§• ì„¤ì •
    parser.add_argument("--max_seq_length", type=int, default=1024, help="Max sequence length")
    parser.add_argument("--packing", action="store_true", help="Enable sequence packing")

    args = parser.parse_args()

    # ========================================================================
    # STEP 1: í† í¬ë‚˜ì´ì € ë¡œë“œ
    # ========================================================================
    logger.info("="*80)
    logger.info("ğŸš€ MOAI-LLM Dataset Tokenization (Single Process)")
    logger.info("="*80)
    logger.info(f"Tokenizer: {args.tokenizer_path}")
    logger.info(f"Max seq length: {args.max_seq_length}")
    logger.info(f"Packing: {'Enabled' if args.packing else 'Disabled'}")
    logger.info("="*80)

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    logger.info(f"âœ“ Tokenizer loaded: {tokenizer.__class__.__name__}")
    logger.info(f"  Vocab size: {tokenizer.vocab_size:,}")

    # ========================================================================
    # STEP 2: ë°ì´í„° ì†ŒìŠ¤ ì¤€ë¹„
    # ========================================================================
    all_sources = []
    if args.dataset:
        for ds in args.dataset:
            all_sources.append(("hf", ds))
    if args.train_file:
        for f in args.train_file:
            all_sources.append(("file", f))

    if not all_sources:
        logger.error("âŒ No datasets specified! Use --dataset or --train_file")
        sys.exit(1)

    logger.info(f"ğŸ“‹ Total datasets: {len(all_sources)}")
    for i, (src_type, src_name) in enumerate(all_sources):
        logger.info(f"  {i+1}. [{src_type}] {src_name}")
    logger.info("="*80)

    # ========================================================================
    # STEP 3: í† í¬ë‚˜ì´ì§•
    # ========================================================================
    # TOKENIZERS_PARALLELISM ê°•ì œ ì„¤ì • (Rust ë‚´ë¶€ ë³‘ë ¬ ì²˜ë¦¬)
    os.environ["TOKENIZERS_PARALLELISM"] = "true"
    logger.info("ğŸ”¥ TOKENIZERS_PARALLELISM=true (Rust internal parallelism enabled)")
    logger.info("   Note: datasets library may still show 'Setting TOKENIZERS_PARALLELISM=false'")
    logger.info("   This is a library message, but Rust parallelism is active inside batch functions")
    logger.info("="*80)

    for idx, source in enumerate(all_sources):
        tokenize_single_dataset(source, tokenizer, args, idx)
        logger.info("")

    # ========================================================================
    # STEP 4: ì™„ë£Œ
    # ========================================================================
    logger.info("="*80)
    logger.info("âœ… All datasets tokenized successfully!")
    logger.info("="*80)
    logger.info("Next step: Run train.py with --skip_tokenization flag")
    logger.info("  Example:")
    logger.info(f"    torchrun --nproc_per_node=4 train.py \\")
    logger.info(f"      --skip_tokenization \\")
    logger.info(f"      --dataset {' '.join([src[1] for src in all_sources[:2]])} \\")
    logger.info(f"      --tokenizer_path {args.tokenizer_path} \\")
    logger.info(f"      ...")
    logger.info("="*80)


if __name__ == "__main__":
    main()
