"""
ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

Usage:
    python chat.py --model_path outputs/sft-bccard/final_model
"""

import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_path",
        type=str,
        default="outputs/sft-bccard/final_model",
        help="Trained model path"
    )
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/",
        help="Tokenizer path"
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=256,
        help="Max new tokens to generate"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Temperature for sampling"
    )
    args = parser.parse_args()

    print("="*80)
    print("MOAI-LLM Chat Interface")
    print("="*80)

    # ëª¨ë¸ ë¡œë“œ
    print(f"Loading model from: {args.model_path}")
    print(f"Loading tokenizer from: {args.tokenizer_path}")

    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    model = AutoModelForCausalLM.from_pretrained(args.model_path)
    model.eval()

    # GPU ì‚¬ìš©
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    model = model.to(device)

    print("="*80)
    print("Ready! Type 'exit' to quit.")
    print("="*80)

    # ëŒ€í™” ë£¨í”„
    conversation_history = []

    while True:
        # ì‚¬ìš©ì ì…ë ¥
        user_input = input("\nğŸ’¬ You: ")

        if user_input.lower() in ["exit", "quit", "q"]:
            print("\nGoodbye! ğŸ‘‹")
            break

        if not user_input.strip():
            continue

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        conversation_history.append(f"<|im_start|>user\n{user_input}<|im_end|>")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = "\n".join(conversation_history) + "\n<|im_start|>assistant\n"

        # í† í°í™”
        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=0.9,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )

        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # Assistant ì‘ë‹µë§Œ ì¶”ì¶œ
        try:
            response = full_response.split("<|im_start|>assistant")[-1]
            response = response.split("<|im_end|>")[0].strip()
        except:
            response = full_response

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        conversation_history.append(f"<|im_start|>assistant\n{response}<|im_end|>")

        # ì¶œë ¥
        print(f"\nğŸ¤– Assistant: {response}")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(conversation_history) > 10:  # ìµœê·¼ 10í„´ë§Œ ìœ ì§€
            conversation_history = conversation_history[-10:]

if __name__ == "__main__":
    main()
