#!/usr/bin/env python3
"""
MOAI í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python test_tokenizer.py
    python test_tokenizer.py --tokenizer_path tokenizers/moai
    python test_tokenizer.py --compare  # ëª¨ë“  í† í¬ë‚˜ì´ì € ë¹„êµ
"""

import argparse
from pathlib import Path

try:
    import orjson as json  # Rust-based, faster
except ImportError:
    import json


def load_tokenizer_info(tokenizer_path: str) -> dict:
    """í† í¬ë‚˜ì´ì € ì •ë³´ ë¡œë“œ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)"""
    path = Path(tokenizer_path)
    
    # tokenizer.json ì°¾ê¸°
    if path.is_file() and path.suffix == '.json':
        json_path = path
    elif path.is_dir():
        json_path = path / "tokenizer.json"
    else:
        raise FileNotFoundError(f"Tokenizer not found: {tokenizer_path}")
    
    with open(json_path, 'rb') as f:  # Binary for orjson
        data = json.loads(f.read())
    
    vocab = data.get('model', {}).get('vocab', {})
    merges = data.get('model', {}).get('merges', [])
    
    return {
        'path': str(path),
        'vocab_size': len(vocab),
        'merges': len(merges),
        'vocab': vocab,
        'model_type': data.get('model', {}).get('type', 'unknown'),
    }


def test_tokenizer(tokenizer_path: str = "tokenizers/moai"):
    """í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    try:
        from tokenizers import Tokenizer
        has_tokenizers = True
    except ImportError:
        has_tokenizers = False
    
    info = load_tokenizer_info(tokenizer_path)
    
    print("=" * 70)
    print("ğŸ§ª MOAI Tokenizer Test")
    print("=" * 70)
    print(f"ğŸ“ Path: {info['path']}")
    print(f"ğŸ“Š Vocab size: {info['vocab_size']:,}")
    print(f"ğŸ”— Merges: {info['merges']:,}")
    print(f"ğŸ·ï¸  Model type: {info['model_type']}")
    print()
    
    # Special tokens í™•ì¸
    special_tokens = ['<pad>', '<s>', '</s>', '<unk>', '<|endoftext|>', '<|im_start|>', '<|im_end|>']
    print("ğŸ”¤ Special Tokens:")
    for token in special_tokens:
        token_id = info['vocab'].get(token)
        status = f"id={token_id}" if token_id is not None else "âŒ missing"
        print(f"   {token}: {status}")
    print()
    
    if has_tokenizers:
        # ì‹¤ì œ í† í°í™” í…ŒìŠ¤íŠ¸
        path = Path(tokenizer_path)
        if path.is_dir():
            json_path = path / "tokenizer.json"
        else:
            json_path = path
        
        tokenizer = Tokenizer.from_file(str(json_path))
        
        test_cases = [
            ("ì˜ì–´", "Hello, world! This is a test."),
            ("í•œêµ­ì–´", "ì•ˆë…•í•˜ì„¸ìš”. MOAI-LLM í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤."),
            ("ì¼ë³¸ì–´", "ã“ã‚“ã«ã¡ã¯ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"),
            ("ì¤‘êµ­ì–´", "ä½ å¥½ã€‚è¿™æ˜¯åˆ†è¯å™¨æµ‹è¯•ã€‚"),
            ("ì½”ë“œ", "def hello(): print('Hello, World!')"),
            ("ê¸ˆìœµ", "ì‹ ìš©ì¹´ë“œ ê²°ì œ í•œë„ë¥¼ ì¦ì•¡í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."),
            ("ê¸ˆìœµ2", "BCì¹´ë“œ í¬ì¸íŠ¸ ì ë¦½ë¥ ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"),
            ("ìˆ«ì", "2024ë…„ 1ì›” 15ì¼ ì˜¤í›„ 3ì‹œ 30ë¶„"),
            ("í˜¼í•©", "AI ëª¨ë¸ì˜ hidden_sizeëŠ” 3840ì…ë‹ˆë‹¤."),
        ]
        
        print("=" * 70)
        print("ğŸ“ Tokenization Test")
        print("=" * 70)
        
        total_chars = 0
        total_tokens = 0
        
        for label, text in test_cases:
            encoded = tokenizer.encode(text)
            tokens = encoded.tokens
            total_chars += len(text)
            total_tokens += len(tokens)
            
            print(f"[{label}] {text}")
            if len(tokens) > 12:
                print(f"   â†’ {len(tokens)} tokens: {tokens[:6]} ... {tokens[-3:]}")
            else:
                print(f"   â†’ {len(tokens)} tokens: {tokens}")
            print()
        
        # íš¨ìœ¨ì„± í†µê³„
        chars_per_token = total_chars / total_tokens if total_tokens > 0 else 0
        print("=" * 70)
        print("ğŸ“ˆ Statistics")
        print("=" * 70)
        print(f"Total characters: {total_chars}")
        print(f"Total tokens: {total_tokens}")
        print(f"Characters per token: {chars_per_token:.2f}")
        print(f"Compression ratio: {chars_per_token:.1f}:1")
        
    else:
        print("âš ï¸  'tokenizers' library not installed. Skipping tokenization test.")
        print("   Install with: pip install tokenizers")
    
    print()
    print("=" * 70)
    print("âœ… Test completed!")
    print("=" * 70)


def compare_tokenizers(tokenizers_dir: str = "tokenizers"):
    """ëª¨ë“  í† í¬ë‚˜ì´ì € ë¹„êµ"""
    path = Path(tokenizers_dir)
    
    print("=" * 70)
    print("ğŸ“Š Tokenizer Comparison")
    print("=" * 70)
    print()
    
    tokenizers = []
    
    # í´ë” í˜•íƒœ í† í¬ë‚˜ì´ì €
    for subdir in sorted(path.iterdir()):
        if subdir.is_dir() and (subdir / "tokenizer.json").exists():
            try:
                info = load_tokenizer_info(str(subdir))
                tokenizers.append(info)
            except Exception as e:
                print(f"âš ï¸  Failed to load {subdir.name}: {e}")
    
    if not tokenizers:
        print("No tokenizers found!")
        return
    
    # í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
    print(f"{'Name':<25} {'Vocab':>12} {'Merges':>12} {'Type':<10}")
    print("-" * 60)
    
    for info in tokenizers:
        name = Path(info['path']).name
        print(f"{name:<25} {info['vocab_size']:>12,} {info['merges']:>12,} {info['model_type']:<10}")
    
    print()
    
    # ìµœì¢… í† í¬ë‚˜ì´ì € í•˜ì´ë¼ì´íŠ¸
    final = next((t for t in tokenizers if Path(t['path']).name == 'moai'), None)
    if final:
        print("=" * 70)
        print(f"ğŸ¯ Final Tokenizer: moai")
        print(f"   Vocab: {final['vocab_size']:,} | Merges: {final['merges']:,}")
        print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description="Test MOAI tokenizer")
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="tokenizers/moai",
        help="Path to tokenizer (default: tokenizers/moai)"
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="Compare all tokenizers in the tokenizers directory"
    )
    
    args = parser.parse_args()
    
    if args.compare:
        compare_tokenizers()
    else:
        test_tokenizer(args.tokenizer_path)


if __name__ == "__main__":
    main()

