# ğŸš€ MOAI-LLM ì™„ì „ êµ¬í˜„ ê°€ì´ë“œ

**3B íŒŒë¼ë¯¸í„° ì–¸ì–´ëª¨ë¸ì„ HuggingFace Datasetsë¡œ ì²˜ìŒë¶€í„° ëê¹Œì§€ êµ¬í˜„í•˜ëŠ” ì™„ì „ ê°€ì´ë“œ**

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [í™˜ê²½ ì„¤ì •](#2-í™˜ê²½-ì„¤ì •)
3. [í† í¬ë‚˜ì´ì € í•™ìŠµ](#3-í† í¬ë‚˜ì´ì €-í•™ìŠµ)
4. [ì‚¬ì „í•™ìŠµ (Pretrain)](#4-ì‚¬ì „í•™ìŠµ-pretrain)
5. [íŒŒì¸íŠœë‹ (SFT)](#5-íŒŒì¸íŠœë‹-sft)
6. [ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡ ](#6-ëª¨ë¸-í‰ê°€-ë°-ì¶”ë¡ )
7. [ê³ ê¸‰ ê¸°ëŠ¥](#7-ê³ ê¸‰-ê¸°ëŠ¥)
8. [ë¬¸ì œ í•´ê²°](#8-ë¬¸ì œ-í•´ê²°)

---

## 1. ê°œìš”

### 1.1 MOAI-LLMì´ë€?

**MOAI-LLM**ì€ 3B íŒŒë¼ë¯¸í„° ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.

#### í•µì‹¬ íŠ¹ì§•:

- âœ… **ìµœì‹  ì•„í‚¤í…ì²˜**: ìµœì‹  LLM ê¸°ìˆ  ì™„ì „ êµ¬í˜„
- âœ… **HuggingFace í†µí•©**: ëª¨ë“  ë‹¨ê³„ì—ì„œ datasets ì‚¬ìš©
- âœ… **ê¸´ ì»¨í…ìŠ¤íŠ¸**: 32K tokens (YaRNìœ¼ë¡œ 128K+ í™•ì¥ ê°€ëŠ¥)
- âœ… **íš¨ìœ¨ì **: GQA (7:1), Flash Attention ì§€ì›
- âœ… **ë‹¤êµ­ì–´**: í•œêµ­ì–´, ì˜ì–´, ì½”ë“œ ë™ì‹œ ì§€ì›
- âœ… **ìë™í™”**: ë°ì´í„° ë‹¤ìš´ë¡œë“œë¶€í„° í•™ìŠµê¹Œì§€ ì›ìŠ¤í†±

#### ì•„í‚¤í…ì²˜ ì‚¬ì–‘:

```python
ëª¨ë¸ í¬ê¸°: 3B parameters
- Layers: 28
- Hidden size: 3,840
- Attention heads: 28 (Q) / 4 (KV)
- Vocabulary: 128,000 (SentencePiece BPE)
- Max sequence: 32,768 tokens
- RoPE theta: 1,000,000
- Activation: SwiGLU
```

### 1.2 ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MOAI-LLM í•™ìŠµ íŒŒì´í”„ë¼ì¸                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. í† í¬ë‚˜ì´ì € í•™ìŠµ
   â””â”€> HuggingFace datasets (wikipedia, C4, etc.)
   â””â”€> SentencePiece BPE í•™ìŠµ (128K vocab)
   â””â”€> ì¶œë ¥: tokenizers/moai_tokenizer.model

2. ì‚¬ì „í•™ìŠµ (Pretrain)
   â””â”€> HuggingFace datasets (wikipedia, bookcorpus, etc.)
   â””â”€> Causal Language Modeling (Next Token Prediction)
   â””â”€> ì¶œë ¥: outputs/pretrain/final_model/

3. íŒŒì¸íŠœë‹ (SFT)
   â””â”€> HuggingFace datasets (alpaca, KULLM, etc.)
   â””â”€> Instruction Following í•™ìŠµ
   â””â”€> ì¶œë ¥: outputs/sft/final_model/

4. ì¶”ë¡  ë° ë°°í¬
   â””â”€> chat.pyë¡œ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
   â””â”€> HuggingFace Hub ë°°í¬
```

---

## 2. í™˜ê²½ ì„¤ì •

### 2.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ìš”êµ¬ì‚¬í•­:
```
Python: 3.10+
GPU: RTX 3090 (24GB) Ã— 1
RAM: 64GB
Disk: 200GB
CUDA: 11.8+
```

#### ê¶Œì¥ ì‚¬ì–‘:
```
GPU: RTX 4090 (24GB) Ã— 4 ë˜ëŠ” A100 (80GB) Ã— 2
RAM: 128GB+
Disk: 1TB SSD
```

#### ì˜ˆìƒ ë¹„ìš©:
```
í† í¬ë‚˜ì´ì € í•™ìŠµ: ~1ì‹œê°„ (ë¬´ë£Œ, CPU ê°€ëŠ¥)
ì‚¬ì „í•™ìŠµ: ~3ì¼ (A100 Ã— 4 ê¸°ì¤€)
SFT: ~6ì‹œê°„ (A100 Ã— 1 ê¸°ì¤€)
```

### 2.2 ì„¤ì¹˜

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/sh2orc/moai-llm.git
cd moai-llm

# 2. ê°€ìƒí™˜ê²½ ì„¤ì •
python3.10 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 3. ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt

# 4. ê°œë°œ ëª¨ë“œë¡œ ì„¤ì¹˜
pip install -e .

# 5. Flash Attention ì„¤ì¹˜ (ì„ íƒ, GPU í•„ìˆ˜)
pip install flash-attn --no-build-isolation

# 6. Weights & Biases (ë¡œê¹…, ì„ íƒ)
pip install wandb
wandb login
```

### 2.3 í™˜ê²½ í™•ì¸

```bash
# CUDA í™•ì¸
nvidia-smi
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.device_count()}')"

# Flash Attention í™•ì¸ (ì„ íƒ)
python -c "try: import flash_attn; print('Flash Attention: âœ…')
except: print('Flash Attention: âŒ')"

# HuggingFace datasets í™•ì¸
python -c "from datasets import load_dataset; print('Datasets: âœ…')"
```

---

## 3. í† í¬ë‚˜ì´ì € í•™ìŠµ

### 3.1 ê°œìš”

í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í°(ìˆ«ì)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. MOAI-LLMì€ **SentencePiece BPE**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**í•µì‹¬**: HuggingFace datasetsë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤!

### 3.2 ê¸°ë³¸ í† í¬ë‚˜ì´ì € í•™ìŠµ

#### ë°©ë²• 1: HuggingFace Dataset ì‚¬ìš© (ê¶Œì¥)

```bash
# í•œêµ­ì–´ Wikipediaë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 128000 \
    --output_dir tokenizers/korean/

# ì˜ì–´ Wikipedia
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 128000 \
    --output_dir tokenizers/english/

# C4 (ëŒ€ìš©ëŸ‰ ì˜ì–´)
python train_tokenizer.py \
    --dataset allenai/c4 \
    --dataset_config en \
    --vocab_size 128000 \
    --max_samples 1000000 \
    --output_dir tokenizers/c4/
```

#### ë°©ë²• 2: ë¡œì»¬ íŒŒì¼ ì‚¬ìš©

```bash
# ë¡œì»¬ txt íŒŒì¼
python train_tokenizer.py \
    --input_files data/pretrain/*.txt \
    --vocab_size 128000 \
    --output_dir tokenizers/local/
```

### 3.3 ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €

```bash
# í•œêµ­ì–´ + ì˜ì–´ í˜¼í•© (ì¶”ì²œ)
# Step 1: í•œêµ­ì–´ ë² ì´ìŠ¤
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 100000 \
    --output_dir tokenizers/base/

# Step 2: ì˜ì–´ ì¶”ê°€ (ì—…ë°ì´íŠ¸ ëª¨ë“œ)
python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 180000 \
    --max_samples 1000000 \
    --output_dir tokenizers/bilingual/
```

### 3.4 ì½”ë“œ íŠ¹í™” í† í¬ë‚˜ì´ì €

```bash
# Python ì½”ë“œ í† í° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/english/moai_tokenizer.model \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --vocab_size 150000 \
    --max_samples 200000 \
    --output_dir tokenizers/code/
```

### 3.5 ë„ë©”ì¸ íŠ¹í™” í† í¬ë‚˜ì´ì €

```bash
# ê¸ˆìœµ ë„ë©”ì¸
python train_tokenizer.py \
    --base_tokenizer tokenizers/korean/moai_tokenizer.model \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --vocab_size 150000 \
    --output_dir tokenizers/finance/
```

### 3.6 í† í¬ë‚˜ì´ì € í™•ì¸

```python
# test_tokenizer.py
import sentencepiece as spm

sp = spm.SentencePieceProcessor()
sp.load('tokenizers/korean/moai_tokenizer.model')

# í…ŒìŠ¤íŠ¸
texts = [
    "ì•ˆë…•í•˜ì„¸ìš”. MOAI-LLMì…ë‹ˆë‹¤.",
    "Hello, this is a test.",
    "print('Hello, World!')",
]

for text in texts:
    tokens = sp.encode(text, out_type=str)
    print(f"Text: {text}")
    print(f"Tokens: {tokens}")
    print(f"IDs: {sp.encode(text)}")
    print(f"Decoded: {sp.decode(sp.encode(text))}")
    print("-" * 50)
```

### 3.7 ë°ì´í„°ì…‹ Config í™•ì¸

ì–´ë–¤ dataset_configë¥¼ ì‚¬ìš©í•´ì•¼ í• ì§€ ëª¨ë¥¼ ë•Œ:

```bash
# Dataset config í™•ì¸ ë„êµ¬
python check_dataset.py wikipedia
python check_dataset.py allenai/c4
python check_dataset.py BCCard/BCCard-Finance-Kor-QnA
```

---

## 4. ì‚¬ì „í•™ìŠµ (Pretrain)

### 4.1 ê°œìš”

ì‚¬ì „í•™ìŠµì€ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¡œ **ë‹¤ìŒ í† í° ì˜ˆì¸¡(Causal LM)**ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**ëª¨ë“  ë°ì´í„°ëŠ” HuggingFace datasetsì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤!**

### 4.2 ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ë¶„)

```bash
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
python train.py \
    --mode pretrain \
    --dataset wikitext \
    --dataset_config wikitext-2-raw-v1 \
    --output_dir outputs/test \
    --max_steps 100 \
    --batch_size 2 \
    --learning_rate 1e-4
```

### 4.3 í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ

```bash
# Wikipedia í•œêµ­ì–´ (1GB, ~ìˆ˜ì¼)
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --tokenizer_path tokenizers/korean/moai_tokenizer.model \
    --output_dir outputs/pretrain-ko \
    --num_epochs 3 \
    --batch_size 4 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-4 \
    --warmup_ratio 0.05 \
    --bf16 \
    --gradient_checkpointing \
    --save_steps 1000 \
    --logging_steps 10
```

### 4.4 ì˜ì–´ ì‚¬ì „í•™ìŠµ

```bash
# C4 ì˜ì–´ (300GB, ~ìˆ˜ì£¼)
python train.py \
    --mode pretrain \
    --dataset allenai/c4 \
    --dataset_config en \
    --tokenizer_path tokenizers/english/moai_tokenizer.model \
    --output_dir outputs/pretrain-c4 \
    --max_steps 100000 \
    --batch_size 4 \
    --gradient_accumulation_steps 16 \
    --learning_rate 3e-4 \
    --warmup_steps 2000 \
    --bf16 \
    --gradient_checkpointing
```

### 4.5 ë‹¤êµ­ì–´ ì‚¬ì „í•™ìŠµ

```bash
# mC4 ë‹¤êµ­ì–´
python train.py \
    --mode pretrain \
    --dataset allenai/c4 \
    --dataset_config multilingual \
    --tokenizer_path tokenizers/bilingual/moai_tokenizer.model \
    --output_dir outputs/pretrain-multilingual \
    --max_steps 50000 \
    --bf16
```

### 4.6 ì½”ë“œ ì‚¬ì „í•™ìŠµ

```bash
# The Stack (Python)
python train.py \
    --mode pretrain \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --tokenizer_path tokenizers/code/moai_tokenizer.model \
    --output_dir outputs/pretrain-code \
    --num_epochs 1 \
    --batch_size 2 \
    --bf16
```

### 4.7 ë©€í‹° GPU í•™ìŠµ

```bash
# 4 GPUë¡œ ë¶„ì‚° í•™ìŠµ
torchrun --nproc_per_node=4 train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir outputs/pretrain-distributed \
    --batch_size 4 \
    --bf16 \
    --gradient_checkpointing

# DeepSpeed ì‚¬ìš© (ZeRO Stage 2)
deepspeed --num_gpus=4 train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir outputs/pretrain-deepspeed \
    --deepspeed configs/deepspeed_config.json \
    --bf16
```

### 4.8 ë¡œì»¬ txt íŒŒì¼ ì‚¬ìš©

```bash
# ë¡œì»¬ íŒŒì¼ë¡œ ì‚¬ì „í•™ìŠµ
python train.py \
    --mode pretrain \
    --train_file data/pretrain/train.txt \
    --tokenizer_path tokenizers/korean/moai_tokenizer.model \
    --output_dir outputs/pretrain-local \
    --num_epochs 3 \
    --bf16
```

---

## 5. íŒŒì¸íŠœë‹ (SFT)

### 5.1 ê°œìš”

SFT(Supervised Fine-Tuning)ëŠ” **Instruction Following** ëŠ¥ë ¥ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**ìë™ í¬ë§· ê°ì§€**: Alpaca, Chat, ShareGPT, input/output ë“± ìë™ ë³€í™˜!

### 5.2 ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

```bash
# BCCard ê¸ˆìœµ Q&A (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
python train.py \
    --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-test \
    --max_steps 50 \
    --batch_size 2
```

### 5.3 í•œêµ­ì–´ SFT

#### 5.3.1 KULLM (150K ìƒ˜í”Œ)

```bash
python train.py \
    --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-kullm \
    --num_epochs 3 \
    --batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --bf16
```

#### 5.3.2 KoAlpaca (52K ìƒ˜í”Œ)

```bash
python train.py \
    --mode sft \
    --dataset beomi/KoAlpaca-v1.1a \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-koalpaca \
    --num_epochs 2 \
    --bf16
```

#### 5.3.3 BCCard ê¸ˆìœµ (4K ìƒ˜í”Œ)

```bash
python train.py \
    --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-finance \
    --num_epochs 5 \
    --bf16
```

### 5.4 ì˜ì–´ SFT

#### 5.4.1 Alpaca (52K ìƒ˜í”Œ)

```bash
python train.py \
    --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-alpaca \
    --num_epochs 3 \
    --bf16
```

#### 5.4.2 LIMA (1K ê³ í’ˆì§ˆ)

```bash
python train.py \
    --mode sft \
    --dataset GAIR/lima \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-lima \
    --num_epochs 10 \
    --learning_rate 1e-5 \
    --bf16
```

#### 5.4.3 OpenAssistant (161K ìƒ˜í”Œ)

```bash
python train.py \
    --mode sft \
    --dataset OpenAssistant/oasst1 \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-oasst \
    --num_epochs 2 \
    --bf16
```

### 5.5 ì½”ë“œ SFT

```bash
# Code Alpaca
python train.py \
    --mode sft \
    --dataset sahil2801/CodeAlpaca-20k \
    --pretrained_model outputs/pretrain-code/final_model \
    --output_dir outputs/sft-code \
    --num_epochs 3 \
    --bf16
```

### 5.6 ë¡œì»¬ JSON íŒŒì¼ ì‚¬ìš©

```bash
# ë¡œì»¬ JSON íŒŒì¼ë¡œ SFT
python train.py \
    --mode sft \
    --train_file data/sft/my_dataset.json \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-custom \
    --num_epochs 3 \
    --bf16
```

**JSON í¬ë§· ì˜ˆì‹œ:**

```json
[
  {
    "instruction": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
    "output": "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
  },
  {
    "input": "GPTê°€ ë­ì•¼?",
    "output": "GPTëŠ” Generative Pre-trained Transformerì˜ ì•½ìì…ë‹ˆë‹¤."
  },
  {
    "messages": [
      {"role": "user", "content": "ì•ˆë…•?"},
      {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}
    ]
  }
]
```

### 5.7 ë‹¤ì¤‘ ë°ì´í„°ì…‹ í˜¼í•©

```bash
# ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ
# Step 1: Alpaca
python train.py --mode sft --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain/final_model \
    --output_dir outputs/sft-stage1

# Step 2: LIMA (ê³ í’ˆì§ˆ)
python train.py --mode sft --dataset GAIR/lima \
    --pretrained_model outputs/sft-stage1/final_model \
    --output_dir outputs/sft-stage2 \
    --learning_rate 1e-5  # Lower LR for refinement
```

---

## 6. ëª¨ë¸ í‰ê°€ ë° ì¶”ë¡ 

### 6.1 ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸

```bash
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
python chat.py \
    --model_path outputs/sft-kullm/final_model \
    --max_new_tokens 256 \
    --temperature 0.7
```

**ëŒ€í™” ì˜ˆì‹œ:**
```
ğŸ’¬ You: í•œêµ­ì˜ ìˆ˜ë„ëŠ”?
ğŸ¤– MOAI: í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.

ğŸ’¬ You: Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•´ì¤˜
ğŸ¤– MOAI:
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

ğŸ’¬ You: exit
```

### 6.2 ë°°ì¹˜ ì¶”ë¡  í…ŒìŠ¤íŠ¸

```bash
# ë¯¸ë¦¬ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
python test_inference.py \
    --model_path outputs/sft-kullm/final_model
```

### 6.3 Perplexity í‰ê°€

```python
# evaluate_ppl.py
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "outputs/sft-kullm/final_model",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(
    "outputs/sft-kullm/final_model"
)

# í‰ê°€ ë°ì´í„°
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")

# Perplexity ê³„ì‚°
encodings = tokenizer("\n\n".join(dataset["text"]), return_tensors="pt")
max_length = 2048
stride = 512

nlls = []
for i in range(0, encodings.input_ids.size(1), stride):
    begin_loc = max(i + stride - max_length, 0)
    end_loc = min(i + stride, encodings.input_ids.size(1))
    trg_len = end_loc - i

    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -100

    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)
        neg_log_likelihood = outputs.loss * trg_len

    nlls.append(neg_log_likelihood)

ppl = torch.exp(torch.stack(nlls).sum() / end_loc)
print(f"Perplexity: {ppl.item():.2f}")
```

### 6.4 HuggingFace Hub ì—…ë¡œë“œ

```bash
# HuggingFaceì— ëª¨ë¸ ì—…ë¡œë“œ
huggingface-cli login

# ì—…ë¡œë“œ
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained('outputs/sft-kullm/final_model')
tokenizer = AutoTokenizer.from_pretrained('outputs/sft-kullm/final_model')

model.push_to_hub('your-username/moai-llm-3b-ko')
tokenizer.push_to_hub('your-username/moai-llm-3b-ko')
"
```

---

## 7. ê³ ê¸‰ ê¸°ëŠ¥

### 7.1 ê¸´ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ (YaRN)

ê¸°ë³¸ 32K tokensë¥¼ 128Kê¹Œì§€ í™•ì¥:

```python
# configs/long_context_config.json
{
  "max_position_embeddings": 32768,
  "rope_theta": 1000000.0,
  "rope_scaling": {
    "type": "yarn",
    "factor": 4.0,
    "original_max_position_embeddings": 32768
  }
}
```

```bash
# ê¸´ ì»¨í…ìŠ¤íŠ¸ë¡œ í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --config_file configs/long_context_config.json \
    --output_dir outputs/pretrain-128k \
    --bf16
```

### 7.2 LoRA íŒŒì¸íŠœë‹ (ë©”ëª¨ë¦¬ ì ˆì•½)

```python
# train_lora.py
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    "outputs/pretrain-ko/final_model",
    torch_dtype=torch.bfloat16
)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 20M || all params: 3020M || trainable%: 0.66%
```

### 7.3 Gradient Checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)

```bash
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir outputs/pretrain \
    --gradient_checkpointing \  # ë©”ëª¨ë¦¬ 50% ì ˆì•½
    --batch_size 1 \
    --gradient_accumulation_steps 32 \
    --bf16
```

### 7.4 í˜¼í•© ì •ë°€ë„ í•™ìŠµ

```bash
# BF16 (ê¶Œì¥, A100/H100)
python train.py --bf16 ...

# FP16 (V100/RTX)
python train.py --fp16 ...

# FP8 (H100)
python train.py --fp8 ...
```

### 7.5 Wandb ë¡œê¹…

```bash
# Wandb í™œì„±í™”
export WANDB_PROJECT="moai-llm"
export WANDB_ENTITY="your-username"

python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir outputs/pretrain \
    --report_to wandb \
    --run_name "pretrain-wikipedia-ko"
```

### 7.6 ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ

```bash
# ì¤‘ë‹¨ëœ í•™ìŠµ ì¬ê°œ
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir outputs/pretrain \
    --resume_from_checkpoint outputs/pretrain/checkpoint-5000 \
    --bf16
```

---

## 8. ë¬¸ì œ í•´ê²°

### 8.1 OOM (Out of Memory)

**ì¦ìƒ**: `CUDA out of memory` ì—ëŸ¬

**í•´ê²°ì±…**:
```bash
# 1. Batch size ì¤„ì´ê¸°
--batch_size 1 --gradient_accumulation_steps 32

# 2. Gradient checkpointing
--gradient_checkpointing

# 3. Sequence length ì¤„ì´ê¸°
--max_seq_length 1024  # ê¸°ë³¸ 2048

# 4. Mixed precision
--bf16  # ë˜ëŠ” --fp16

# 5. LoRA ì‚¬ìš©
# train_lora.py ì°¸ê³ 
```

### 8.2 Dataset Config ì—ëŸ¬

**ì¦ìƒ**: `ValueError: Config name is missing`

**í•´ê²°ì±…**:
```bash
# Config í™•ì¸
python check_dataset.py wikipedia

# Config ëª…ì‹œ
--dataset wikipedia --dataset_config 20220301.ko
```

### 8.3 í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ**: `FileNotFoundError: moai_tokenizer.model`

**í•´ê²°ì±…**:
```bash
# í† í¬ë‚˜ì´ì € ë¨¼ì € í•™ìŠµ
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir tokenizers/

# ì •í™•í•œ ê²½ë¡œ ì§€ì •
--tokenizer_path tokenizers/moai_tokenizer.model
```

### 8.4 í•™ìŠµ ì†ë„ ëŠë¦¼

**ì¦ìƒ**: 1 stepì— 10ì´ˆ ì´ìƒ ì†Œìš”

**í•´ê²°ì±…**:
```bash
# 1. Flash Attention ì„¤ì¹˜
pip install flash-attn --no-build-isolation

# 2. DataLoader workers ì¦ê°€
--dataloader_num_workers 4

# 3. ë©€í‹° GPU ì‚¬ìš©
torchrun --nproc_per_node=4 train.py ...

# 4. ë°ì´í„° í”„ë¦¬í˜ì¹˜
--dataloader_prefetch_factor 2
```

### 8.5 HuggingFace ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ**: `Connection timeout` ë˜ëŠ” `403 Forbidden`

**í•´ê²°ì±…**:
```bash
# 1. HuggingFace ë¡œê·¸ì¸
huggingface-cli login

# 2. ë¯¸ëŸ¬ ì‚¬ìš©
export HF_ENDPOINT=https://hf-mirror.com

# 3. ìºì‹œ ì´ˆê¸°í™”
rm -rf ~/.cache/huggingface/datasets/

# 4. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
from datasets import load_dataset
dataset = load_dataset("wikipedia", "20220301.ko", cache_dir="/path/to/cache")
```

---

## 9. ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤

### 9.1 í•œêµ­ì–´ ë²”ìš© ëª¨ë¸

```bash
# 1. í† í¬ë‚˜ì´ì € (í•œêµ­ì–´ ì¤‘ì‹¬)
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.ko \
    --vocab_size 128000 --output_dir tokenizers/ko/

# 2. ì‚¬ì „í•™ìŠµ (Wikipedia)
python train.py --mode pretrain \
    --dataset wikipedia --dataset_config 20220301.ko \
    --tokenizer_path tokenizers/ko/moai_tokenizer.model \
    --output_dir outputs/pretrain-ko --num_epochs 3 --bf16

# 3. SFT (KULLM)
python train.py --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-ko --num_epochs 3 --bf16

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-ko/final_model
```

### 9.2 ì˜ì–´ ê³ í’ˆì§ˆ ëª¨ë¸ (LIMA ìŠ¤íƒ€ì¼)

```bash
# 1. í† í¬ë‚˜ì´ì € (ì˜ì–´)
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.en \
    --vocab_size 128000 --output_dir tokenizers/en/

# 2. ì‚¬ì „í•™ìŠµ (C4)
python train.py --mode pretrain \
    --dataset allenai/c4 --dataset_config en \
    --tokenizer_path tokenizers/en/moai_tokenizer.model \
    --output_dir outputs/pretrain-c4 --max_steps 50000 --bf16

# 3. SFT (LIMA - 1K ê³ í’ˆì§ˆ)
python train.py --mode sft \
    --dataset GAIR/lima \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-lima \
    --num_epochs 10 --learning_rate 1e-5 --bf16

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-lima/final_model
```

### 9.3 ì½”ë“œ ìƒì„± ëª¨ë¸

```bash
# 1. í† í¬ë‚˜ì´ì € (ì½”ë“œ íŠ¹í™”)
python train_tokenizer.py \
    --dataset bigcode/the-stack --dataset_config data/python \
    --vocab_size 128000 --max_samples 200000 \
    --output_dir tokenizers/code/

# 2. ì‚¬ì „í•™ìŠµ (The Stack)
python train.py --mode pretrain \
    --dataset bigcode/the-stack --dataset_config data/python \
    --tokenizer_path tokenizers/code/moai_tokenizer.model \
    --output_dir outputs/pretrain-code --num_epochs 1 --bf16

# 3. SFT (Code Alpaca)
python train.py --mode sft \
    --dataset sahil2801/CodeAlpaca-20k \
    --pretrained_model outputs/pretrain-code/final_model \
    --output_dir outputs/sft-code --num_epochs 3 --bf16

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-code/final_model
```

### 9.4 ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸

```bash
# 1. í† í¬ë‚˜ì´ì € (í•œêµ­ì–´ + ê¸ˆìœµ)
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.ko \
    --vocab_size 100000 --output_dir tokenizers/base/

python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --vocab_size 128000 --output_dir tokenizers/finance/

# 2. ì‚¬ì „í•™ìŠµ (Wikipedia)
python train.py --mode pretrain \
    --dataset wikipedia --dataset_config 20220301.ko \
    --tokenizer_path tokenizers/finance/moai_tokenizer.model \
    --output_dir outputs/pretrain-ko --bf16

# 3. SFT (BCCard)
python train.py --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-finance --num_epochs 5 --bf16

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-finance/final_model
```

---

## 10. ì¶”ê°€ ìë£Œ

### 10.1 ë¬¸ì„œ

- **QUICKSTART.md**: 10ë¶„ ë¹ ë¥¸ ì‹œì‘
- **ARCHITECTURE.md**: ì•„í‚¤í…ì²˜ ìƒì„¸
- **EMBEDDING_GUIDE.md**: ì„ë² ë”© ê°€ì´ë“œ
- **TOKENIZER_UPDATE_GUIDE.md**: í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸
- **DATASET_CONFIGS.md**: ë°ì´í„°ì…‹ Config ê°€ì´ë“œ
- **POPULAR_DATASETS.md**: ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡

### 10.2 ìŠ¤í¬ë¦½íŠ¸

```
moai-llm/
â”œâ”€â”€ train_tokenizer.py      # í† í¬ë‚˜ì´ì € í•™ìŠµ
â”œâ”€â”€ train.py                 # í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ chat.py                  # ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ test_inference.py        # ì¶”ë¡  í…ŒìŠ¤íŠ¸
â”œâ”€â”€ check_dataset.py         # ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸
â””â”€â”€ configs/
    â””â”€â”€ model_config.json    # ëª¨ë¸ ì„¤ì •
```

### 10.3 HuggingFace Datasets

**ì‚¬ì „í•™ìŠµìš©**:
- `wikipedia` (í•œêµ­ì–´/ì˜ì–´)
- `allenai/c4` (ì˜ì–´, 300GB)
- `bigcode/the-stack` (ì½”ë“œ)
- `mc4` (ë‹¤êµ­ì–´)

**SFTìš©**:
- `tatsu-lab/alpaca` (ì˜ì–´, 52K)
- `nlpai-lab/kullm-v2` (í•œêµ­ì–´, 150K)
- `BCCard/BCCard-Finance-Kor-QnA` (ê¸ˆìœµ, 4K)
- `GAIR/lima` (ê³ í’ˆì§ˆ, 1K)

### 10.4 ì°¸ê³  ë…¼ë¬¸

- **Qwen3**: https://arxiv.org/abs/2506.05176
- **RoPE**: https://arxiv.org/abs/2104.09864
- **YaRN**: https://arxiv.org/abs/2309.00071
- **Flash Attention**: https://arxiv.org/abs/2307.08691
- **GQA**: https://arxiv.org/abs/2305.13245

---

## ğŸ‰ ì™„ë£Œ!

ì´ì œ MOAI-LLMì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

**í•µì‹¬ ìš”ì•½**:
1. âœ… **ëª¨ë“  ë‹¨ê³„ê°€ HuggingFace datasets ê¸°ë°˜**
2. âœ… **ë°ì´í„° ë‹¤ìš´ë¡œë“œ/ë³€í™˜ ìë™í™”**
3. âœ… **ìµœì‹  ì•„í‚¤í…ì²˜**
4. âœ… **í† í¬ë‚˜ì´ì € â†’ ì‚¬ì „í•™ìŠµ â†’ SFT â†’ ì¶”ë¡  ì „ì²´ íŒŒì´í”„ë¼ì¸**

**ë‹¤ìŒ ë‹¨ê³„**:
```bash
# 10ë¶„ ë¹ ë¥¸ ì‹œì‘
cat QUICKSTART.md

# ë³¸ê²© í•™ìŠµ
python train_tokenizer.py --dataset wikipedia --dataset_config 20220301.ko --output_dir tokenizers/
python train.py --mode pretrain --dataset wikipedia --dataset_config 20220301.ko --bf16
python chat.py --model_path outputs/pretrain/final_model
```

**ì§ˆë¬¸/ì´ìŠˆ**: https://github.com/yourusername/moai-llm/issues

Happy Training! ğŸš€
