# ğŸ“Š MOAI-LLM ë°ì´í„°ì…‹ ì™„ì „ ê°€ì´ë“œ

**HuggingFace Datasets ì‚¬ìš©ë²•ê³¼ ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡**

---

## ğŸ“‹ ëª©ì°¨

1. [Dataset Configë€?](#1-dataset-configë€)
2. [ì£¼ìš” ë°ì´í„°ì…‹ë³„ Config](#2-ì£¼ìš”-ë°ì´í„°ì…‹ë³„-config)
3. [Config í™•ì¸ ë°©ë²•](#3-config-í™•ì¸-ë°©ë²•)
4. [ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡](#4-ì¶”ì²œ-ë°ì´í„°ì…‹-ëª©ë¡)
5. [ë°ì´í„°ì…‹ ì¡°í•© ì¶”ì²œ](#5-ë°ì´í„°ì…‹-ì¡°í•©-ì¶”ì²œ)

---

## 1. Dataset Configë€?

### 1.1 ê°œë…

**ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ë²„ì „/ì–¸ì–´/ì„¤ì •ì„ ì§€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.**

```bash
--dataset wikipedia           # ë°ì´í„°ì…‹ ì´ë¦„
--dataset_config 20220301.ko  # ì„¤ì • (2022ë…„ 3ì›”, í•œêµ­ì–´)
```

### 1.2 ì™œ í•„ìš”í•œê°€?

ë§ì€ HuggingFace ë°ì´í„°ì…‹ì€ **ì—¬ëŸ¬ ì–¸ì–´, ë²„ì „, ì„¤ì •**ì„ í¬í•¨í•©ë‹ˆë‹¤:
- Wikipedia: 300+ ì–¸ì–´
- C4: ë‹¤êµ­ì–´ ë²„ì „
- The Stack: í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë³„

Configë¥¼ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.

---

## 2. ì£¼ìš” ë°ì´í„°ì…‹ë³„ Config

### 2.1 Wikipedia

**Format**: `ë‚ ì§œ.ì–¸ì–´ì½”ë“œ`

| ì–¸ì–´ | Config | í¬ê¸° | ìš©ë„ |
|-----|--------|------|------|
| í•œêµ­ì–´ | `20220301.ko` | ~1GB | í•œêµ­ì–´ ëª¨ë¸ |
| ì˜ì–´ | `20220301.en` | ~20GB | ì˜ì–´ ëª¨ë¸ |
| ì¤‘êµ­ì–´ | `20220301.zh` | ~3GB | ì¤‘êµ­ì–´ ëª¨ë¸ |
| ì¼ë³¸ì–´ | `20220301.ja` | ~2GB | ì¼ë³¸ì–´ ëª¨ë¸ |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# í† í¬ë‚˜ì´ì € í•™ìŠµ
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --output_dir tokenizers/

# ì‚¬ì „í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --output_dir outputs/pretrain
```

---

### 2.2 C4 (Common Crawl)

**Format**: ì–¸ì–´ì½”ë“œ

| ì–¸ì–´ | Config | í¬ê¸° | ìš©ë„ |
|-----|--------|------|------|
| ì˜ì–´ | `en` | ~300GB | ëŒ€ê·œëª¨ ì˜ì–´ ì‚¬ì „í•™ìŠµ |
| ë‹¤êµ­ì–´ | `multilingual` | ëŒ€ìš©ëŸ‰ | ë‹¤êµ­ì–´ ëª¨ë¸ |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
python train.py \
    --mode pretrain \
    --dataset allenai/c4 \
    --dataset_config en \
    --output_dir outputs/pretrain
```

---

### 2.3 WikiText

**Format**: ë²„ì „ëª…

| ë²„ì „ | Config | í¬ê¸° | ìš©ë„ |
|-----|--------|------|------|
| WikiText-2 | `wikitext-2-raw-v1` | 4MB | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ |
| WikiText-103 | `wikitext-103-raw-v1` | 500MB | ì‹¤í—˜ìš© |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ë¶„)
python train.py \
    --mode pretrain \
    --dataset wikitext \
    --dataset_config wikitext-2-raw-v1 \
    --output_dir outputs/test \
    --max_steps 100
```

---

### 2.4 The Stack (ì½”ë“œ)

**Format**: `data/ì–¸ì–´`

| ì–¸ì–´ | Config | í¬ê¸° | ìš©ë„ |
|-----|--------|------|------|
| Python | `data/python` | ëŒ€ìš©ëŸ‰ | ì½”ë“œ ìƒì„± ëª¨ë¸ |
| JavaScript | `data/javascript` | ëŒ€ìš©ëŸ‰ | JS íŠ¹í™” |
| Java | `data/java` | ëŒ€ìš©ëŸ‰ | Java íŠ¹í™” |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
python train.py \
    --mode pretrain \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --output_dir outputs/pretrain-code
```

---

### 2.5 Config ì—†ëŠ” ë°ì´í„°ì…‹

ë‹¤ìŒ ë°ì´í„°ì…‹ë“¤ì€ **configê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤**:

| ë°ì´í„°ì…‹ | Config | ì‚¬ìš©ë²• |
|---------|--------|-------|
| BookCorpus | âŒ ë¶ˆí•„ìš” | `--dataset bookcorpus` |
| BCCard Finance | âŒ ë¶ˆí•„ìš” | `--dataset BCCard/BCCard-Finance-Kor-QnA` |
| Alpaca | âŒ ë¶ˆí•„ìš” | `--dataset tatsu-lab/alpaca` |
| KULLM | âŒ ë¶ˆí•„ìš” | `--dataset nlpai-lab/kullm-v2` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# Config ì—†ì´ ì‚¬ìš©
python train.py \
    --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain/final_model \
    --output_dir outputs/sft
```

---

## 3. Config í™•ì¸ ë°©ë²•

### 3.1 ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

```bash
# Wikipedia config í™•ì¸
python check_dataset.py wikipedia

# C4 config í™•ì¸
python check_dataset.py allenai/c4

# BCCard config í™•ì¸
python check_dataset.py BCCard/BCCard-Finance-Kor-QnA
```

### 3.2 Python ì½”ë“œ

```python
from datasets import get_dataset_config_names

# Wikipediaì˜ ëª¨ë“  config í™•ì¸
configs = get_dataset_config_names("wikipedia")

# í•œêµ­ì–´ ê´€ë ¨ configë§Œ í•„í„°ë§
ko_configs = [c for c in configs if 'ko' in c]
print(ko_configs)
# ['20220301.ko']

# ì˜ì–´ ê´€ë ¨ config
en_configs = [c for c in configs if 'en' in c]
print(en_configs[:3])
# ['20220301.en', '20220301.en-simple', ...]
```

### 3.3 HuggingFace ì›¹ì‚¬ì´íŠ¸

1. https://huggingface.co/datasets ì ‘ì†
2. ë°ì´í„°ì…‹ ê²€ìƒ‰ (ì˜ˆ: wikipedia)
3. "Viewer" íƒ­ì—ì„œ "Configuration" í™•ì¸

---

## 4. ì¶”ì²œ ë°ì´í„°ì…‹ ëª©ë¡

### 4.1 í•œêµ­ì–´ ë°ì´í„°ì…‹

#### ì‚¬ì „í•™ìŠµìš©

| ë°ì´í„°ì…‹ | Config | í¬ê¸° | ì„¤ëª… | ëª…ë ¹ì–´ |
|---------|--------|------|------|--------|
| **Wikipedia** | `20220301.ko` | ~1GB | í•œêµ­ì–´ ë°±ê³¼ì‚¬ì „ | `--dataset wikipedia --dataset_config 20220301.ko` |
| **mC4** | `ko` | ~ìˆ˜ì‹­GB | Common Crawl í•œêµ­ì–´ | `--dataset allenai/c4 --dataset_config ko` |
| **OSCAR** | `unshuffled_deduplicated_ko` | ëŒ€ìš©ëŸ‰ | Common Crawl ì •ì œë³¸ | `--dataset oscar --dataset_config unshuffled_deduplicated_ko` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# Wikipedia í•œêµ­ì–´
python train.py --mode pretrain \
    --dataset wikipedia --dataset_config 20220301.ko \
    --output_dir outputs/pretrain-ko --bf16
```

#### SFTìš©

| ë°ì´í„°ì…‹ | ìƒ˜í”Œ | ì„¤ëª… | ëª…ë ¹ì–´ |
|---------|------|------|--------|
| **KULLM** | 150K | í•œêµ­ì–´ Q&A | `--dataset nlpai-lab/kullm-v2` |
| **KoAlpaca** | 52K | í•œêµ­ì–´ Alpaca | `--dataset beomi/KoAlpaca-v1.1a` |
| **BCCard Finance** | 4K | ê¸ˆìœµ Q&A | `--dataset BCCard/BCCard-Finance-Kor-QnA` |
| **KorQuAD** | 60K | ì§ˆì˜ì‘ë‹µ | `--dataset squad_kor_v1` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# KULLM
python train.py --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-kullm --bf16
```

---

### 4.2 ì˜ì–´ ë°ì´í„°ì…‹

#### ì‚¬ì „í•™ìŠµìš©

| ë°ì´í„°ì…‹ | Config | í¬ê¸° | ì„¤ëª… | ëª…ë ¹ì–´ |
|---------|--------|------|------|--------|
| **Wikipedia** | `20220301.en` | ~20GB | ì˜ì–´ ë°±ê³¼ì‚¬ì „ | `--dataset wikipedia --dataset_config 20220301.en` |
| **C4** | `en` | ~300GB | Common Crawl ì •ì œë³¸ | `--dataset allenai/c4 --dataset_config en` |
| **RefinedWeb** | - | ~5TB | Falcon ì‚¬ì „í•™ìŠµ ë°ì´í„° | `--dataset tiiuae/falcon-refinedweb` |
| **RedPajama** | - | ~1.2TB | LLaMA ë³µì œ ë°ì´í„° | `--dataset togethercomputer/RedPajama-Data-1T` |
| **BookCorpus** | - | ~5GB | ì±… ë°ì´í„° | `--dataset bookcorpus` |
| **The Pile** | - | ~800GB | ë‹¤ì–‘í•œ ì†ŒìŠ¤ í˜¼í•© | `--dataset EleutherAI/pile` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# C4 ì˜ì–´ (ëŒ€ê·œëª¨)
python train.py --mode pretrain \
    --dataset allenai/c4 --dataset_config en \
    --output_dir outputs/pretrain-c4 --max_steps 100000 --bf16
```

#### SFTìš©

| ë°ì´í„°ì…‹ | ìƒ˜í”Œ | ì„¤ëª… | ëª…ë ¹ì–´ |
|---------|------|------|--------|
| **Alpaca** | 52K | Stanford Alpaca | `--dataset tatsu-lab/alpaca` |
| **Dolly** | 15K | Databricks ê³ í’ˆì§ˆ | `--dataset databricks/databricks-dolly-15k` |
| **OpenAssistant** | 161K | RLHF ë°ì´í„° | `--dataset OpenAssistant/oasst1` |
| **ShareGPT** | 90K | ChatGPT ëŒ€í™” | `--dataset RyokoAI/ShareGPT52K` |
| **LIMA** | 1K | ì´ˆê³ í’ˆì§ˆ (Less is More) | `--dataset GAIR/lima` |
| **HH-RLHF** | 169K | Anthropic RLHF | `--dataset Anthropic/hh-rlhf` |
| **Evol-Instruct** | 70K | WizardLM ë°ì´í„° | `--dataset WizardLM/WizardLM_evol_instruct_V2_196k` |
| **UltraChat** | 1.4M | ëŒ€ê·œëª¨ ëŒ€í™” | `--dataset stingning/ultrachat` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# Alpaca (ë²”ìš©)
python train.py --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-alpaca --bf16

# LIMA (ê³ í’ˆì§ˆ)
python train.py --mode sft \
    --dataset GAIR/lima \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-lima \
    --num_epochs 10 --learning_rate 1e-5 --bf16
```

---

### 4.3 ì½”ë“œ ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹ | Config | í¬ê¸° | ì„¤ëª… | ëª…ë ¹ì–´ |
|---------|--------|------|------|--------|
| **The Stack** | `data/python` | ëŒ€ìš©ëŸ‰ | GitHub ì½”ë“œ | `--dataset bigcode/the-stack --dataset_config data/python` |
| **StarCoder** | - | ëŒ€ìš©ëŸ‰ | StarCoder í•™ìŠµ ë°ì´í„° | `--dataset bigcode/starcoderdata` |
| **Code Alpaca** | 20K | ì½”ë“œ ìƒì„± Q&A | `--dataset sahil2801/CodeAlpaca-20k` |

**ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# ì‚¬ì „í•™ìŠµ (Python)
python train.py --mode pretrain \
    --dataset bigcode/the-stack --dataset_config data/python \
    --output_dir outputs/pretrain-code --bf16

# SFT (Code Alpaca)
python train.py --mode sft \
    --dataset sahil2801/CodeAlpaca-20k \
    --pretrained_model outputs/pretrain-code/final_model \
    --output_dir outputs/sft-code --bf16
```

---

### 4.4 ë‹¤êµ­ì–´ ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹ | Config | í¬ê¸° | ì–¸ì–´ | ëª…ë ¹ì–´ |
|---------|--------|------|------|--------|
| **mC4** | `multilingual` | ëŒ€ìš©ëŸ‰ | 100+ ì–¸ì–´ | `--dataset allenai/c4 --dataset_config multilingual` |
| **OSCAR** | `unshuffled_deduplicated_*` | ëŒ€ìš©ëŸ‰ | 150+ ì–¸ì–´ | `--dataset oscar --dataset_config unshuffled_deduplicated_*` |
| **CulturaX** | - | 6.3T tokens | 167 ì–¸ì–´ | `--dataset uonlp/CulturaX` |

---

## 5. ë°ì´í„°ì…‹ ì¡°í•© ì¶”ì²œ

### 5.1 í•œêµ­ì–´ ë²”ìš© ëª¨ë¸

```bash
# í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.ko \
    --vocab_size 128000 --output_dir tokenizers/ko/

# ì‚¬ì „í•™ìŠµ (Wikipedia)
python train.py --mode pretrain \
    --dataset wikipedia --dataset_config 20220301.ko \
    --output_dir outputs/pretrain-ko --num_epochs 3 --bf16

# SFT (KULLM - 150K ìƒ˜í”Œ)
python train.py --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-ko --num_epochs 3 --bf16
```

**ì˜ˆìƒ ì„±ëŠ¥**: í•œêµ­ì–´ ì¼ë°˜ ì§€ì‹ + Q&A ëŠ¥ë ¥

---

### 5.2 ì˜ì–´ ê³ ì„±ëŠ¥ ëª¨ë¸

```bash
# í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.en \
    --vocab_size 128000 --output_dir tokenizers/en/

# ì‚¬ì „í•™ìŠµ (C4 - 300GB)
python train.py --mode pretrain \
    --dataset allenai/c4 --dataset_config en \
    --output_dir outputs/pretrain-c4 --max_steps 100000 --bf16

# SFT 1ë‹¨ê³„ (Alpaca - ë²”ìš©)
python train.py --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-stage1 --num_epochs 3 --bf16

# SFT 2ë‹¨ê³„ (LIMA - ê³ í’ˆì§ˆ ì •ì œ)
python train.py --mode sft \
    --dataset GAIR/lima \
    --pretrained_model outputs/sft-stage1/final_model \
    --output_dir outputs/sft-lima \
    --num_epochs 10 --learning_rate 1e-5 --bf16
```

**ì˜ˆìƒ ì„±ëŠ¥**: GPT-3.5 ìˆ˜ì¤€ì˜ ê³ í’ˆì§ˆ ì‘ë‹µ

---

### 5.3 ì½”ë“œ ìƒì„± íŠ¹í™” ëª¨ë¸

```bash
# í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset bigcode/the-stack --dataset_config data/python \
    --vocab_size 128000 --max_samples 200000 --output_dir tokenizers/code/

# ì‚¬ì „í•™ìŠµ (The Stack Python)
python train.py --mode pretrain \
    --dataset bigcode/the-stack --dataset_config data/python \
    --output_dir outputs/pretrain-code --num_epochs 1 --bf16

# SFT (Code Alpaca)
python train.py --mode sft \
    --dataset sahil2801/CodeAlpaca-20k \
    --pretrained_model outputs/pretrain-code/final_model \
    --output_dir outputs/sft-code --num_epochs 3 --bf16
```

**ì˜ˆìƒ ì„±ëŠ¥**: Python ì½”ë“œ ìƒì„± íŠ¹í™” (GitHub Copilot ìŠ¤íƒ€ì¼)

---

### 5.4 ë‹¤êµ­ì–´ ëª¨ë¸ (í•œì˜ ë°”ì´ë§ê¶)

```bash
# í† í¬ë‚˜ì´ì € (í•œêµ­ì–´ ë² ì´ìŠ¤ + ì˜ì–´ ì¶”ê°€)
python train_tokenizer.py \
    --dataset wikipedia --dataset_config 20220301.ko \
    --vocab_size 100000 --output_dir tokenizers/base/

python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset wikipedia --dataset_config 20220301.en \
    --vocab_size 180000 --max_samples 1000000 --output_dir tokenizers/bilingual/

# ì‚¬ì „í•™ìŠµ (mC4 ë‹¤êµ­ì–´)
python train.py --mode pretrain \
    --dataset allenai/c4 --dataset_config multilingual \
    --output_dir outputs/pretrain-multilingual --max_steps 50000 --bf16

# SFT (í˜¼í•©: KULLM + Alpaca)
# Stage 1: í•œêµ­ì–´
python train.py --mode sft \
    --dataset nlpai-lab/kullm-v2 \
    --pretrained_model outputs/pretrain-multilingual/final_model \
    --output_dir outputs/sft-stage1 --num_epochs 2 --bf16

# Stage 2: ì˜ì–´
python train.py --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/sft-stage1/final_model \
    --output_dir outputs/sft-bilingual --num_epochs 2 --bf16
```

**ì˜ˆìƒ ì„±ëŠ¥**: í•œêµ­ì–´ + ì˜ì–´ ëª¨ë‘ ê°€ëŠ¥í•œ ë°”ì´ë§ê¶ ëª¨ë¸

---

## 6. ë¹ ë¥¸ ì°¸ì¡°í‘œ

### Config í•„ìˆ˜ ì—¬ë¶€

| ë°ì´í„°ì…‹ | Config í•„ìš”? | ì˜ˆì‹œ |
|---------|-----------|------|
| wikipedia | âœ… | `--dataset_config 20220301.ko` |
| allenai/c4 | âœ… | `--dataset_config en` |
| wikitext | âœ… | `--dataset_config wikitext-2-raw-v1` |
| bigcode/the-stack | âœ… | `--dataset_config data/python` |
| bookcorpus | âŒ | (ìƒëµ) |
| tatsu-lab/alpaca | âŒ | (ìƒëµ) |
| BCCard/BCCard-Finance-Kor-QnA | âŒ | (ìƒëµ) |
| nlpai-lab/kullm-v2 | âŒ | (ìƒëµ) |
| GAIR/lima | âŒ | (ìƒëµ) |

### ë°ì´í„°ì…‹ í¬ê¸° ë¹„êµ

| ë°ì´í„°ì…‹ | í¬ê¸° | í•™ìŠµ ì‹œê°„ (A100 Ã— 4) | ìš©ë„ |
|---------|------|-------------------|------|
| WikiText-2 | 4MB | 10ë¶„ | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ |
| Wikipedia (ko) | ~1GB | ~1ì¼ | í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ |
| Wikipedia (en) | ~20GB | ~3ì¼ | ì˜ì–´ ì‚¬ì „í•™ìŠµ |
| C4 (en) | ~300GB | ~ìˆ˜ì£¼ | ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ |
| The Pile | ~800GB | ~ìˆ˜ê°œì›” | SOTA ì‚¬ì „í•™ìŠµ |

---

## 7. ë¬¸ì œ í•´ê²°

### Config ì—ëŸ¬

```bash
# âŒ ValueError: Config name is missing
python train.py --dataset wikipedia --output_dir outputs/

# âœ… í•´ê²°: Config ì¶”ê°€
python train.py --dataset wikipedia --dataset_config 20220301.ko --output_dir outputs/
```

### Config í™•ì¸

```bash
# ì˜ ëª¨ë¥¼ ë•Œ ì´ ëª…ë ¹ì–´ë¡œ í™•ì¸
python check_dataset.py <dataset_name>
```

---

## 8. ë” ì•Œì•„ë³´ê¸°

- **USER_GUIDE.md**: ì™„ì „í•œ í•™ìŠµ ê°€ì´ë“œ
- **QUICKSTART.md**: 10ë¶„ ë¹ ë¥¸ ì‹œì‘
- **ARCHITECTURE.md**: ì•„í‚¤í…ì²˜ ìƒì„¸
- **examples/bccard_example.md**: BCCard ì‹¤ì „ ì˜ˆì œ

---

**ğŸ‰ ì´ì œ ì›í•˜ëŠ” ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!**

```bash
# ì¶”ì²œ: í•œêµ­ì–´ ë²”ìš© ëª¨ë¸
python train.py --mode pretrain --dataset wikipedia --dataset_config 20220301.ko --bf16
```
