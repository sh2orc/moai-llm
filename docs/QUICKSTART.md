# âš¡ MOAI-LLM ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸ¯ í•œ ì¤„ ìš”ì•½

**HuggingFace ë°ì´í„°ì…‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¶€í„° SFTê¹Œì§€ ìë™í™”!**

---

## ğŸ“‹ ì „ì²´ ì›Œí¬í”Œë¡œìš° (3ë‹¨ê³„)

```bash
# 1. í† í¬ë‚˜ì´ì € í•™ìŠµ â†’ 2. ì‚¬ì „í•™ìŠµ â†’ 3. SFT íŒŒì¸íŠœë‹
```

---

## âš¡ 10ë¶„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

íŒŒì´í”„ë¼ì¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”:

```bash
# Step 1: í† í¬ë‚˜ì´ì € (ë¹ ë¥¸ ë²„ì „, 2ë¶„)
python train_tokenizer.py \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --vocab_size 32000 \
    --max_samples 10000 \
    --output_dir tokenizers/test

# Step 2: ì‚¬ì „í•™ìŠµ (100 steps, 3ë¶„)
python train.py \
    --mode pretrain \
    --dataset wikitext \
    --dataset_config wikitext-2-raw-v1 \
    --output_dir outputs/test \
    --max_steps 100

# Step 3: SFT (50 steps, 2ë¶„)
python train.py \
    --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/test/final_model \
    --output_dir outputs/test-sft \
    --max_steps 50

# Step 4: ì±„íŒ… í…ŒìŠ¤íŠ¸ (ì¦‰ì‹œ)
python chat.py --model_path outputs/test-sft/final_model
```

**âœ… ëª¨ë“  ë‹¨ê³„ê°€ ì •ìƒ ì‘ë™í•˜ë©´ ì‹¤ì „ í•™ìŠµìœ¼ë¡œ!**

---

## ğŸš€ ì‹¤ì „ í•™ìŠµ (í”„ë¡œë•ì…˜)

### Step 1: í† í¬ë‚˜ì´ì € í•™ìŠµ (1-2ì‹œê°„)

```bash
# HuggingFace ë°ì´í„°ì…‹ ì‚¬ìš© (ê¶Œì¥)
python train_tokenizer.py \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --vocab_size 128000 \
    --output_dir tokenizers/ko-128k

# ë˜ëŠ” ë¡œì»¬ í…ìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©
python train_tokenizer.py \
    --input_files data/pretrain/*.txt \
    --vocab_size 128000 \
    --output_dir tokenizers/custom
```

### Step 2: ì‚¬ì „í•™ìŠµ (ìˆ˜ì¼~ìˆ˜ì£¼)

```bash
# í•œêµ­ì–´ Wikipedia (ê¸°ë³¸)
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --output_dir outputs/pretrain-ko \
    --bf16 \
    --gradient_checkpointing

# ì˜ì–´ C4 (ëŒ€ê·œëª¨, ë©€í‹° GPU)
torchrun --nproc_per_node=8 train.py \
    --mode pretrain \
    --dataset allenai/c4 \
    --dataset_config en \
    --output_dir outputs/pretrain-c4 \
    --bf16 \
    --gradient_checkpointing
```

### Step 3: SFT íŒŒì¸íŠœë‹ (ìˆ˜ì‹œê°„~1ì¼)

```bash
# BCCard ê¸ˆìœµ Q&A
python train.py \
    --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-finance \
    --num_epochs 3

# Alpaca ë²”ìš© Q&A
python train.py \
    --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-alpaca \
    --num_epochs 3

# ë¡œì»¬ JSONL íŒŒì¼
python train.py \
    --mode sft \
    --train_file data/sft/custom.jsonl \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-custom \
    --num_epochs 3
```

### Step 4: í…ŒìŠ¤íŠ¸ ë° ë°°í¬

```bash
# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-finance/final_model

# ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python test_inference.py --model_path outputs/sft-finance/final_model

# HuggingFace Hubì— ì—…ë¡œë“œ
python -c "
from moai_llm.modeling.model import MoaiForCausalLM
model = MoaiForCausalLM.from_pretrained('outputs/sft-finance/final_model')
model.push_to_hub('your-username/moai-llm-finance')
"
```

---

## ğŸ“Š ì¶”ì²œ ë°ì´í„°ì…‹

### í† í¬ë‚˜ì´ì € í•™ìŠµìš©

| ë°ì´í„°ì…‹ | í¬ê¸° | ëª…ë ¹ì–´ |
|---------|------|--------|
| Wikipedia (í•œêµ­ì–´) | 1GB | `--dataset wikimedia/wikipedia --dataset_config 20231101.ko` |
| Wikipedia (ì˜ì–´) | 20GB | `--dataset wikimedia/wikipedia --dataset_config 20231101.en` |
| C4 (ì˜ì–´) | 300GB | `--dataset allenai/c4 --dataset_config en` |

### ì‚¬ì „í•™ìŠµìš©

| ë°ì´í„°ì…‹ | í¬ê¸° | ìš©ë„ |
|---------|------|------|
| WikiText-2 | 4MB | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (`wikitext-2-raw-v1`) |
| Wikipedia (í•œêµ­ì–´) | 1GB | í•œêµ­ì–´ ì¼ë°˜ ì§€ì‹ |
| C4 (ì˜ì–´) | 300GB | ì˜ì–´ ë²”ìš© |

### SFTìš©

| ë°ì´í„°ì…‹ | ìƒ˜í”Œ | ë„ë©”ì¸ |
|---------|------|--------|
| BCCard | 4K | ê¸ˆìœµ Q&A |
| Alpaca | 52K | ë²”ìš© Instruction |
| KULLM-v2 | 150K | í•œêµ­ì–´ ë²”ìš© |

**ë” ë§ì€ ë°ì´í„°ì…‹**: `DATASETS.md` ì°¸ê³ 

---

## ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì™„ì „ ê°€ì´ë“œ

### ì‹œë‚˜ë¦¬ì˜¤ 1: í•œêµ­ì–´ ê¸ˆìœµ ì±—ë´‡

```bash
# 1. í† í¬ë‚˜ì´ì € (í•œêµ­ì–´)
python train_tokenizer.py \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --output_dir tokenizers/ko

# 2. ì‚¬ì „í•™ìŠµ (í•œêµ­ì–´)
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.ko \
    --output_dir outputs/pretrain-ko \
    --bf16

# 3. SFT (ê¸ˆìœµ)
python train.py \
    --mode sft \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --pretrained_model outputs/pretrain-ko/final_model \
    --output_dir outputs/sft-finance

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-finance/final_model
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì˜ì–´ ë²”ìš© ëª¨ë¸

```bash
# 1. í† í¬ë‚˜ì´ì € (ì˜ì–´)
python train_tokenizer.py \
    --dataset wikimedia/wikipedia \
    --dataset_config 20231101.en \
    --output_dir tokenizers/en

# 2. ì‚¬ì „í•™ìŠµ (C4)
torchrun --nproc_per_node=8 train.py \
    --mode pretrain \
    --dataset allenai/c4 \
    --dataset_config en \
    --output_dir outputs/pretrain-c4

# 3. SFT (Alpaca)
python train.py \
    --mode sft \
    --dataset tatsu-lab/alpaca \
    --pretrained_model outputs/pretrain-c4/final_model \
    --output_dir outputs/sft-alpaca

# 4. í…ŒìŠ¤íŠ¸
python chat.py --model_path outputs/sft-alpaca/final_model
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë„ë©”ì¸ íŠ¹í™” (ì˜ë£Œ/ë²•ë¥ )

```bash
# 1. ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ (ë„ë©”ì¸ ìš©ì–´ ì¶”ê°€)
python train_tokenizer.py \
    --dataset your-org/medical-corpus \
    --base_tokenizer tokenizers/ko \
    --output_dir tokenizers/ko-medical

# 2. ë„ë©”ì¸ ë°ì´í„° ì‚¬ì „í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset your-org/medical-corpus \
    --output_dir outputs/pretrain-medical \
    --bf16

# 3. ë„ë©”ì¸ SFT
python train.py \
    --mode sft \
    --train_file data/medical_qa.jsonl \
    --pretrained_model outputs/pretrain-medical/final_model \
    --output_dir outputs/sft-medical
```

---

## ğŸ› ï¸ ì£¼ìš” ì˜µì…˜ ìš”ì•½

### ê³µí†µ ì˜µì…˜

```bash
--bf16                      # BF16 í˜¼í•© ì •ë°€ë„ (ê¶Œì¥, A100/H100)
--fp16                      # FP16 í˜¼í•© ì •ë°€ë„ (V100/RTX)
--gradient_checkpointing    # ë©”ëª¨ë¦¬ ì ˆì•½ (í•„ìˆ˜)
--batch_size 4              # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ)
--max_steps 10000           # ìµœëŒ€ ìŠ¤í… (í…ŒìŠ¤íŠ¸ìš©)
```

### í† í¬ë‚˜ì´ì € ì˜µì…˜

```bash
--dataset wikimedia/wikipedia   # HuggingFace ë°ì´í„°ì…‹
--dataset_config 20231101.ko    # ë°ì´í„°ì…‹ ì„¤ì •
--input_files data/*.txt        # ë¡œì»¬ íŒŒì¼
--vocab_size 128000             # ì–´íœ˜ í¬ê¸°
--max_samples 10000             # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì œí•œ
--base_tokenizer path/          # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸
```

### ì‚¬ì „í•™ìŠµ ì˜µì…˜

```bash
--mode pretrain                 # ì‚¬ì „í•™ìŠµ ëª¨ë“œ
--dataset wikimedia/wikipedia   # ë°ì´í„°ì…‹
--dataset_config 20231101.ko    # ì„¤ì •
--output_dir outputs/           # ì¶œë ¥ ë””ë ‰í† ë¦¬
```

### SFT ì˜µì…˜

```bash
--mode sft                  # SFT ëª¨ë“œ
--dataset BCCard/...        # HuggingFace ë°ì´í„°ì…‹
--train_file custom.jsonl   # ë¡œì»¬ JSONL íŒŒì¼
--pretrained_model path/    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê²½ë¡œ
--num_epochs 3              # ì—í­ ìˆ˜
```

---

## ğŸ’¡ í•µì‹¬ ìš”ì 

### âœ… ìë™í™”ëœ ê²ƒë“¤

- **ë°ì´í„° ë‹¤ìš´ë¡œë“œ**: HuggingFaceì—ì„œ ìë™
- **í¬ë§· ë³€í™˜**: input/output, instruction/output ë“± ìë™ ê°ì§€
- **í•™ìŠµ ì¬ê°œ**: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìë™ ì¬ê°œ
- **ëª¨ë¸ ì €ì¥**: ìµœì¢… ëª¨ë¸ ë° ì²´í¬í¬ì¸íŠ¸ ìë™ ì €ì¥

### âŒ í•„ìš” ì—†ëŠ” ê²ƒë“¤

- ~~í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ë™ ì¤€ë¹„~~
- ~~ë³µì¡í•œ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸~~
- ~~í¬ë§· ë³€í™˜ ì½”ë“œ ì‘ì„±~~
- ~~í•™ìŠµ íŒŒë¼ë¯¸í„° ë³µì¡í•œ ì„¤ì •~~

---

## ğŸ“š ì¶”ê°€ ë¬¸ì„œ

| ë¬¸ì„œ | ë‚´ìš© |
|------|------|
| `USER_GUIDE.md` | ì™„ì „í•œ í•™ìŠµ ê°€ì´ë“œ (í™˜ê²½ ì„¤ì •ë¶€í„° ë°°í¬ê¹Œì§€) |
| `DATASETS.md` | ë°ì´í„°ì…‹ ì„ íƒ ë° ì„¤ì • ê°€ì´ë“œ |
| `ARCHITECTURE.md` | Qwen3 ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ëª… |
| `TOKENIZER_UPDATE_GUIDE.md` | í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ ë°©ë²• |
| `examples/bccard_example.md` | BCCard ë°ì´í„°ì…‹ ì™„ì „ ì˜ˆì œ |

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

```bash
# 1. Gradient checkpointing í™œì„±í™”
--gradient_checkpointing

# 2. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--batch_size 1 --gradient_accumulation_steps 4

# 3. FP16 ì‚¬ìš©
--fp16
```

### ë°ì´í„°ì…‹ ì„¤ì •(config) ì°¾ê¸°

```bash
# ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸ ë„êµ¬
python check_dataset.py wikimedia/wikipedia
python check_dataset.py allenai/c4
```

### Dataset scripts ì—ëŸ¬

```
RuntimeError: Dataset scripts are no longer supported, but found wikipedia.py
```

ìµœì‹  `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬(3.x)ì—ì„œëŠ” ì»¤ìŠ¤í…€ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ ë°ì´í„°ì…‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
ê¸°ì¡´ `wikipedia` ëŒ€ì‹  `wikimedia/wikipedia` ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:

```bash
# ê¸°ì¡´ (ì§€ì› ì•ˆë¨)
--dataset wikipedia --dataset_config 20220301.ko

# ìƒˆë¡œìš´ ë°©ì‹ (ê¶Œì¥)
--dataset wikimedia/wikipedia --dataset_config 20231101.ko
```

### í•™ìŠµ ì¬ê°œ

```bash
# ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œë¨
python train.py \
    --mode pretrain \
    --dataset wikimedia/wikipedia \
    --output_dir outputs/pretrain  # ê°™ì€ ë””ë ‰í† ë¦¬ ì§€ì •
```

### Wikipedia ë°ì´í„°ì…‹ ì—ëŸ¬ (Dataset scripts are no longer supported)

ìµœì‹  `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ `wikipedia` ë°ì´í„°ì…‹ ë¡œë“œ ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, ë¬¸ì œê°€ ê³„ì†ë˜ë©´:

**í•´ê²° ë°©ë²• 1: ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì‚¬ìš© (ê¶Œì¥)**
```bash
# mC4 í•œêµ­ì–´ ë°ì´í„°ì…‹ ì‚¬ìš©
python train_tokenizer.py \
    --dataset allenai/c4 \
    --dataset_config ko \
    --vocab_size 128000 \
    --output_dir tokenizers/ko
```

**í•´ê²° ë°©ë²• 2: ë¡œì»¬ íŒŒì¼ ì‚¬ìš©**
```bash
python train_tokenizer.py \
    --input_files data/*.txt \
    --vocab_size 128000 \
    --output_dir tokenizers/custom
```

**í•´ê²° ë°©ë²• 3: datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ê·¸ë ˆì´ë“œ (ì„ì‹œ)**
```bash
pip install "datasets<4.0.0"
```

---

## ğŸ‰ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘!

### ì²« ì‹¤í–‰ (10ë¶„ í…ŒìŠ¤íŠ¸)

```bash
python train_tokenizer.py --dataset wikimedia/wikipedia --dataset_config 20231101.ko --vocab_size 32000 --max_samples 10000 --output_dir tokenizers/test

python train.py --mode pretrain --dataset wikitext --dataset_config wikitext-2-raw-v1 --output_dir outputs/test --max_steps 100

python chat.py --model_path outputs/test/final_model
```

### ì‹¤ì „ í•™ìŠµ (í”„ë¡œë•ì…˜)

```bash
# ì „ì²´ ê°€ì´ë“œ ì½ê¸°
cat USER_GUIDE.md

# ì‹¤ì „ í•™ìŠµ ì‹œì‘
python train_tokenizer.py --dataset wikimedia/wikipedia --dataset_config 20231101.ko --output_dir tokenizers/ko

python train.py --mode pretrain --dataset wikimedia/wikipedia --dataset_config 20231101.ko --output_dir outputs/pretrain --bf16 --gradient_checkpointing
```

---

**ğŸš€ MOAI-LLMìœ¼ë¡œ ë‚˜ë§Œì˜ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!**
