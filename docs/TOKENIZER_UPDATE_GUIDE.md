# ğŸ”„ í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ ê°€ì´ë“œ

## ğŸ“Œ ê°œìš”

ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì— **ìƒˆë¡œìš´ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ì¶”ê°€**í•˜ì—¬ ì–´íœ˜ë¥¼ í™•ì¥í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

---

## ğŸ¤” ì™œ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œê°€?

### ì‚¬ìš© ì‚¬ë¡€

1. **ë„ë©”ì¸ í™•ì¥**
   - ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµí•œ í† í¬ë‚˜ì´ì €ì— **ì˜ë£Œ ìš©ì–´** ì¶”ê°€
   - í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ì— **ê¸ˆìœµ ì „ë¬¸ ìš©ì–´** ì¶”ê°€
   - ì˜ì–´ í† í¬ë‚˜ì´ì €ì— **ì½”ë“œ í† í°** ì¶”ê°€

2. **ì–¸ì–´ ì¶”ê°€**
   - í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ì— **ì˜ì–´ ì–´íœ˜** ì¶”ê°€
   - ì˜ì–´ í† í¬ë‚˜ì´ì €ì— **ì¤‘êµ­ì–´ ì–´íœ˜** ì¶”ê°€

3. **ì‹ ì¡°ì–´ ëŒ€ì‘**
   - 2022ë…„ í† í¬ë‚˜ì´ì €ì— **2024ë…„ ì‹ ì¡°ì–´** ì¶”ê°€
   - ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì— **ìµœì‹  ê¸°ìˆ  ìš©ì–´** ì¶”ê°€

---

## âš–ï¸ ì²˜ìŒë¶€í„° vs ì—…ë°ì´íŠ¸

### ì²˜ìŒë¶€í„° í•™ìŠµ (From Scratch)

```bash
# ì˜ˆ: Wikipediaë§Œìœ¼ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 128000 \
    --output_dir tokenizers/
```

**ì¥ì :**
- âœ… ê¹”ë”í•˜ê³  ì¼ê´€ëœ ì–´íœ˜
- âœ… ì¤‘ë³µ ì—†ëŠ” ìµœì í™”ëœ êµ¬ì¡°

**ë‹¨ì :**
- âŒ ê¸°ì¡´ ëª¨ë¸ê³¼ í˜¸í™˜ ë¶ˆê°€ (í† í° IDê°€ ë°”ë€œ)
- âŒ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•¨

---

### ì—…ë°ì´íŠ¸ (Update)

```bash
# ì˜ˆ: Wikipedia í† í¬ë‚˜ì´ì €ì— ê¸ˆìœµ ë°ì´í„° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/moai_tokenizer.model \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --vocab_size 150000 \
    --output_dir tokenizers/updated/
```

**ì¥ì :**
- âœ… ê¸°ì¡´ ì–´íœ˜ ìœ ì§€ (ê¸°ì¡´ ëª¨ë¸ í™œìš© ê°€ëŠ¥)
- âœ… ìƒˆ ë„ë©”ì¸ì— íŠ¹í™”ëœ í† í° ì¶”ê°€
- âœ… ì ì§„ì  í™•ì¥ ê°€ëŠ¥

**ë‹¨ì :**
- âŒ vocab_size ì¦ê°€ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€)
- âŒ ê¸°ì¡´ ë°ì´í„° ì¬êµ¬ì„± í•„ìš”

---

## ğŸ“‹ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì—…ë°ì´íŠ¸

```bash
# Step 1: ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë¡œ ì¼ë°˜ ëª¨ë¸ í•™ìŠµ
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 128000 \
    --output_dir tokenizers/base/

# Step 2: ê¸ˆìœµ ë°ì´í„° ì¶”ê°€í•˜ì—¬ ì—…ë°ì´íŠ¸
python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --vocab_size 150000 \
    --output_dir tokenizers/finance/
```

**ê²°ê³¼:**
- ê¸°ì¡´ vocab: 128,000ê°œ â†’ ìƒˆ vocab: 150,000ê°œ
- ê¸°ì¡´ ì¼ë°˜ ì–´íœ˜ ìœ ì§€ + ê¸ˆìœµ ìš©ì–´ 22,000ê°œ ì¶”ê°€

---

### 2. ì˜ë£Œ ë„ë©”ì¸ ì¶”ê°€

```bash
# Step 1: ê¸°ì¡´ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 100000 \
    --output_dir tokenizers/base/

# Step 2: ì˜ë£Œ ë°ì´í„° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset medical_dataset \
    --vocab_size 120000 \
    --output_dir tokenizers/medical/
```

---

### 3. ì½”ë“œ í† í° ì¶”ê°€

```bash
# Step 1: ì˜ì–´ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 100000 \
    --output_dir tokenizers/text/

# Step 2: Python ì½”ë“œ í† í° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/text/moai_tokenizer.model \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --vocab_size 130000 \
    --max_samples 100000 \
    --output_dir tokenizers/code/
```

---

### 4. ë‹¤êµ­ì–´ í™•ì¥

```bash
# Step 1: í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 80000 \
    --output_dir tokenizers/korean/

# Step 2: ì˜ì–´ ì–´íœ˜ ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/korean/moai_tokenizer.model \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 150000 \
    --max_samples 500000 \
    --output_dir tokenizers/bilingual/
```

---

## ğŸ” ë‚´ë¶€ ë™ì‘ ì›ë¦¬

### ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤

```python
# 1. ê¸°ì¡´ í† í¬ë‚˜ì´ì € ë¡œë“œ
base_tokenizer = load("tokenizers/base/moai_tokenizer.model")

# 2. ê¸°ì¡´ ì–´íœ˜ ìƒ˜í”Œ ì¶”ì¶œ
existing_vocab = extract_vocabulary_samples(base_tokenizer)
# â†’ ['ì•ˆë…•', 'í•˜ì„¸ìš”', 'ì„¸ê³„', 'ì»´í“¨í„°', ...]

# 3. ìƒˆ ë°ì´í„° ì¶”ê°€
new_data = load_dataset("BCCard/BCCard-Finance-Kor-QnA")
# â†’ ['ëŒ€ì¶œ', 'ì´ììœ¨', 'ì‹ ìš©', 'ë‹´ë³´', ...]

# 4. ë°ì´í„° ë³‘í•©
merged_data = existing_vocab + new_data
# â†’ ['ì•ˆë…•', 'í•˜ì„¸ìš”', ..., 'ëŒ€ì¶œ', 'ì´ììœ¨', ...]

# 5. ìƒˆ í† í¬ë‚˜ì´ì € í•™ìŠµ (vocab_size ì¦ê°€)
train_tokenizer(merged_data, vocab_size=150000)
```

### ë³‘í•© ë¹„ìœ¨

- **ê¸°ì¡´ ì–´íœ˜**: ê¸°ì¡´ vocabì—ì„œ ìµœëŒ€ 10,000ê°œ ìƒ˜í”Œ ì¶”ì¶œ
- **ìƒˆ ë°ì´í„°**: ì „ì²´ ìƒˆ ë°ì´í„°ì…‹ ì‚¬ìš©
- **ìë™ ê· í˜•**: SentencePieceê°€ ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì–´íœ˜ ìƒì„±

---

## ğŸ’¡ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì¼ë°˜ â†’ ê¸ˆìœµ íŠ¹í™”

```bash
# 1. ì¼ë°˜ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (Wikipedia)
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 100000 \
    --output_dir tokenizers/general/

# 2. ê¸ˆìœµ ë°ì´í„° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/general/moai_tokenizer.model \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --vocab_size 120000 \
    --output_dir tokenizers/finance/

# 3. ê¸ˆìœµ íŠ¹í™” ëª¨ë¸ í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset BCCard/BCCard-Finance-Kor-QnA \
    --tokenizer_path tokenizers/finance/moai_tokenizer.model \
    --output_dir outputs/finance-pretrain
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì˜ì–´ â†’ ì½”ë“œ + ì˜ì–´

```bash
# 1. ì˜ì–´ í† í¬ë‚˜ì´ì €
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 80000 \
    --output_dir tokenizers/english/

# 2. ì½”ë“œ í† í° ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/english/moai_tokenizer.model \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --vocab_size 120000 \
    --max_samples 200000 \
    --output_dir tokenizers/code-en/

# 3. ì½”ë“œ + ì˜ì–´ ëª¨ë¸ í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset bigcode/the-stack \
    --dataset_config data/python \
    --tokenizer_path tokenizers/code-en/moai_tokenizer.model \
    --output_dir outputs/code-pretrain
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: í•œêµ­ì–´ â†’ í•œì˜ ë°”ì´ë§ê¶

```bash
# 1. í•œêµ­ì–´ ë² ì´ìŠ¤
python train_tokenizer.py \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --vocab_size 100000 \
    --output_dir tokenizers/korean/

# 2. ì˜ì–´ ì¶”ê°€
python train_tokenizer.py \
    --base_tokenizer tokenizers/korean/moai_tokenizer.model \
    --dataset wikipedia \
    --dataset_config 20220301.en \
    --vocab_size 180000 \
    --max_samples 1000000 \
    --output_dir tokenizers/bilingual/

# 3. ë°”ì´ë§ê¶ ëª¨ë¸ í•™ìŠµ
python train.py \
    --mode pretrain \
    --dataset wikipedia \
    --dataset_config 20220301.ko \
    --tokenizer_path tokenizers/bilingual/moai_tokenizer.model \
    --output_dir outputs/bilingual-pretrain
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. Vocab Size ì¦ê°€ í•„ìˆ˜

```bash
# âŒ ì˜ëª»ëœ ì˜ˆ (ê¸°ì¡´ê³¼ ê°™ì€ í¬ê¸°)
python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset new_data \
    --vocab_size 128000 \  # ê¸°ì¡´ê³¼ ë™ì¼
    --output_dir tokenizers/updated/

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ (í¬ê¸° ì¦ê°€)
python train_tokenizer.py \
    --base_tokenizer tokenizers/base/moai_tokenizer.model \
    --dataset new_data \
    --vocab_size 150000 \  # ê¸°ì¡´ë³´ë‹¤ í¼
    --output_dir tokenizers/updated/
```

**ì´ìœ **: ê¸°ì¡´ ì–´íœ˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒˆ ì–´íœ˜ë¥¼ ì¶”ê°€í•˜ë ¤ë©´ vocab_sizeê°€ ì»¤ì•¼ í•©ë‹ˆë‹¤.

---

### 2. ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ í˜¸í™˜ì„±

ì—…ë°ì´íŠ¸ëœ í† í¬ë‚˜ì´ì €ëŠ” **ìƒˆë¡œ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤**:

```bash
# âŒ ê¸°ì¡´ ëª¨ë¸ì— ì—…ë°ì´íŠ¸ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš© ë¶ˆê°€
python train.py \
    --mode sft \
    --pretrained_model outputs/old-model/ \  # ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë¡œ í•™ìŠµë¨
    --tokenizer_path tokenizers/updated/moai_tokenizer.model \  # ìƒˆ í† í¬ë‚˜ì´ì €
    --output_dir outputs/sft  # í† í° ID ë¶ˆì¼ì¹˜!

# âœ… ìƒˆ í† í¬ë‚˜ì´ì €ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµ
python train.py \
    --mode pretrain \
    --tokenizer_path tokenizers/updated/moai_tokenizer.model \
    --output_dir outputs/new-pretrain
```

---

### 3. ë©”ëª¨ë¦¬ ê³ ë ¤

- vocab_size ì¦ê°€ â†’ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¦ê°€ â†’ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

```python
# ì˜ˆì‹œ ê³„ì‚°
# vocab_size=128000, hidden_size=2048
embedding_params = 128000 * 2048 = 262M parameters (ì•½ 1GB)

# vocab_size=200000ìœ¼ë¡œ ì¦ê°€
embedding_params = 200000 * 2048 = 410M parameters (ì•½ 1.6GB)
```

**ê¶Œì¥**: vocab_sizeëŠ” í•„ìš”í•œ ë§Œí¼ë§Œ ì¦ê°€ì‹œí‚¤ì„¸ìš”.

---

## ğŸ“Š ë¹„êµí‘œ

| ë°©ì‹ | Vocab Size | í•™ìŠµ ì‹œê°„ | ê¸°ì¡´ ëª¨ë¸ | ìƒˆ ë„ë©”ì¸ | ì¶”ì²œ ìƒí™© |
|------|-----------|---------|---------|----------|----------|
| **ì²˜ìŒë¶€í„°** | ê³ ì • | ì§§ìŒ | âŒ í˜¸í™˜ ì•ˆë¨ | âœ… ìµœì í™” | ìƒˆ í”„ë¡œì íŠ¸ |
| **ì—…ë°ì´íŠ¸** | ì¦ê°€ | ì•½ê°„ ê¹€ | âœ… ìœ ì§€ | âœ… ì¶”ê°€ | ë„ë©”ì¸ í™•ì¥ |

---

## ğŸ¯ ë¹ ë¥¸ ì°¸ì¡°

### ì—…ë°ì´íŠ¸ ëª…ë ¹ì–´ í…œí”Œë¦¿

```bash
python train_tokenizer.py \
    --base_tokenizer <ê¸°ì¡´_í† í¬ë‚˜ì´ì €_ê²½ë¡œ> \
    --dataset <ìƒˆ_ë°ì´í„°ì…‹> \
    --dataset_config <ì„¤ì •> \
    --vocab_size <ê¸°ì¡´ë³´ë‹¤_í°_ê°’> \
    --output_dir <ì¶œë ¥_ê²½ë¡œ>
```

### ì˜ˆì‹œ

```bash
# ê¸ˆìœµ
python train_tokenizer.py --base_tokenizer tokenizers/base/moai_tokenizer.model --dataset BCCard/BCCard-Finance-Kor-QnA --vocab_size 150000 --output_dir tokenizers/finance/

# ì˜ë£Œ
python train_tokenizer.py --base_tokenizer tokenizers/base/moai_tokenizer.model --dataset medical_dataset --vocab_size 150000 --output_dir tokenizers/medical/

# ì½”ë“œ
python train_tokenizer.py --base_tokenizer tokenizers/base/moai_tokenizer.model --dataset bigcode/the-stack --dataset_config data/python --vocab_size 150000 --output_dir tokenizers/code/
```

---

## ğŸ“š ë” ì•Œì•„ë³´ê¸°

- **ê¸°ë³¸ í† í¬ë‚˜ì´ì € í•™ìŠµ**: `QUICKSTART.md`
- **ë°ì´í„°ì…‹ ëª©ë¡**: `POPULAR_DATASETS.md`
- **ì „ì²´ í•™ìŠµ ê°€ì´ë“œ**: `START_HERE.md`

---

**ğŸ’¡ í•µì‹¬ ì •ë¦¬:**
- âœ… ê¸°ì¡´ ì–´íœ˜ ìœ ì§€í•˜ë©´ì„œ ìƒˆ ë„ë©”ì¸ ì¶”ê°€
- âœ… vocab_sizeëŠ” ë°˜ë“œì‹œ ì¦ê°€
- âœ… ì—…ë°ì´íŠ¸ëœ í† í¬ë‚˜ì´ì €ë¡œ ìƒˆ ëª¨ë¸ í•™ìŠµ í•„ìš”
