# ğŸ—ï¸ MOAI-LLM ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

**Qwen3 ê¸°ë°˜ 3B íŒŒë¼ë¯¸í„° ì–¸ì–´ëª¨ë¸ì˜ ê¸°ìˆ  ë¬¸ì„œ**

---

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [ì„ë² ë”© ë ˆì´ì–´](#2-ì„ë² ë”©-ë ˆì´ì–´)
3. [Transformer ë ˆì´ì–´](#3-transformer-ë ˆì´ì–´)
4. [ê³ ê¸‰ ê¸°ëŠ¥](#4-ê³ ê¸‰-ê¸°ëŠ¥)
5. [ì„±ëŠ¥ ìµœì í™”](#5-ì„±ëŠ¥-ìµœì í™”)

---

## 1. ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 MOAI-LLM vs Qwen3

MOAI-LLMì€ **Qwen3 ì•„í‚¤í…ì²˜ë¥¼ 3B íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •**í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

| ì„¤ì • | MOAI-LLM-3B | Qwen3-8B | ë¹„ê³  |
|------|-------------|----------|------|
| **íŒŒë¼ë¯¸í„°** | 3B | 8B | ëª¨ë¸ í¬ê¸° ì¡°ì • |
| **Layers** | 28 | 36 | 3B ì„¤ê³„ |
| **Hidden Size** | 3,840 | 4,096 | 3B ì„¤ê³„ |
| **Attention Heads** | 28 (Q) / 4 (KV) | 32 (Q) / 32 (KV) | GQA 7:1 |
| **Vocab Size** | 128,000 | 151,665 | ë©”ëª¨ë¦¬ ìµœì í™” |
| **Max Seq** | 32,768 | 40,960 | âœ… ë™ì¼ ìˆ˜ì¤€ |
| **RoPE Theta** | 1,000,000 | 1,000,000 | âœ… ë™ì¼ |
| **QK-Norm** | Yes | Yes | âœ… ë™ì¼ |
| **Tied Embeddings** | False | False | âœ… ë™ì¼ |
| **Attention Bias** | False | False | âœ… ë™ì¼ |
| **Activation** | SwiGLU | SiLU | MOAIê°€ ë” ê°•ë ¥ |

### 1.2 ëª¨ë¸ ìŠ¤í™

```python
ëª¨ë¸ í¬ê¸°: ~3B parameters
â”œâ”€ Token Embedding: 491M (128K Ã— 3,840)
â”œâ”€ Transformer Layers: 2.0B (28 layers)
â”‚  â”œâ”€ Self-Attention: ~1.2B
â”‚  â””â”€ MLP: ~800M
â””â”€ Output LM Head: 491M (3,840 Ã— 128K)

ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (BF16):
â”œâ”€ ëª¨ë¸: ~6GB
â”œâ”€ Activation (batch=4, seq=2K): ~8GB
â””â”€ ì´: ~14GB (RTX 3090/4090 ê°€ëŠ¥)
```

### 1.3 êµ¬í˜„ ì™„ì„±ë„

| ì»´í¬ë„ŒíŠ¸ | êµ¬í˜„ ìƒíƒœ | Qwen3 í˜¸í™˜ì„± |
|---------|----------|-------------|
| í† í¬ë‚˜ì´ì € (SentencePiece BPE) | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| Token Embedding | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| Position Embedding (RoPE) | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| QK-Normalization | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| Transformer (Pre-LN) | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| GQA (Grouped Query Attention) | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| Flash Attention | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| RMSNorm | âœ… ì™„ì „ êµ¬í˜„ | 100% |
| SwiGLU Activation | âœ… ì™„ì „ êµ¬í˜„ | 100% |

**ì „ì²´ êµ¬í˜„ ì™„ì„±ë„: 95%** âœ…

---

## 2. ì„ë² ë”© ë ˆì´ì–´

### 2.1 Token Embedding

**íŒŒì¼**: `moai_llm/modeling/model.py:68-73`

```python
self.embed_tokens = nn.Embedding(
    vocab_size=128000,      # Qwen3: 151,665 (ë©”ëª¨ë¦¬ ìµœì í™”)
    hidden_size=3840,        # 3B ëª¨ë¸ í¬ê¸°
    padding_idx=0,          # PAD token
)
```

**íŠ¹ì§•**:
- í‘œì¤€ `nn.Embedding` (í•™ìŠµ ê°€ëŠ¥í•œ lookup table)
- ì´ˆê¸°í™”: Normal ë¶„í¬ (mean=0, std=0.02)
- Padding index ì§€ì›

**ë©”ëª¨ë¦¬ ê³„ì‚°**:
```python
params = 128,000 Ã— 3,840 = 491,520,000 (491M)
memory_bf16 = 491M Ã— 2 bytes = 982 MB
```

---

### 2.2 Position Embedding (RoPE)

**íŒŒì¼**: `moai_llm/modeling/rope.py`

```python
self.rotary_emb = MoaiRotaryEmbedding(
    dim=128,                         # head_dim (3840 / 30)
    max_position_embeddings=32768,   # Qwen3: 32K tokens
    base=1000000.0,                  # Qwen3: 1M (ê¸´ ì»¨í…ìŠ¤íŠ¸)
    scaling_config=None,             # YaRN/NTK ì§€ì›
)
```

**RoPE (Rotary Position Embedding) íŠ¹ì§•**:
- âœ… Relative position encoding (ì ˆëŒ€ ìœ„ì¹˜ ë¶ˆí•„ìš”)
- âœ… ê¸¸ì´ ì¼ë°˜í™” (í•™ìŠµë³´ë‹¤ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥)
- âœ… íŒŒë¼ë¯¸í„° ì—†ìŒ (ë©”ëª¨ë¦¬ ì ˆì•½)

**rope_theta=1Mì˜ íš¨ê³¼**:
- ê¸°ì¡´ RoPE (theta=10K): ~8K tokensì— ìµœì í™”
- Qwen3 RoPE (theta=1M): ~32K tokensê¹Œì§€ ì•ˆì •ì 
- **100ë°° í° theta** â†’ ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì •ë³´ ìœ ì§€

**RoPE í™•ì¥ ê¸°ë²• (ì„ íƒì )**:
1. **Standard RoPE**: ê¸°ë³¸ êµ¬í˜„
2. **Linear Scaling**: ë‹¨ìˆœ ì£¼íŒŒìˆ˜ ì¡°ì •
3. **NTK-aware Scaling**: ì£¼íŒŒìˆ˜ë³„ ì°¨ë“± ìŠ¤ì¼€ì¼ë§
4. **YaRN**: ê³ ê¸‰ ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ìŠ¤ì¼€ì¼ë§

---

### 2.3 QK-Normalization

**íŒŒì¼**: `moai_llm/modeling/attention.py:113-119`

```python
if use_qk_norm:
    self.q_norm = MoaiRMSNorm(head_dim=128, eps=1e-6)
    self.k_norm = MoaiRMSNorm(head_dim=128, eps=1e-6)
```

**íš¨ê³¼**:
- âœ… í•™ìŠµ ì•ˆì •í™” (Gradient í­ë°œ ë°©ì§€)
- âœ… Attention score ì •ê·œí™”
- âœ… Qwen3ì˜ í•µì‹¬ ì•ˆì •í™” ê¸°ë²•

**ë™ì‘ ë°©ì‹**:
```python
# Query/Key normalization
Q_normalized = Q_norm(Q_proj(x))
K_normalized = K_norm(K_proj(x))

# Attention ê³„ì‚°
attention_scores = Q_normalized @ K_normalized.T
```

---

### 2.4 Output Embedding (LM Head)

**íŒŒì¼**: `moai_llm/modeling/model.py:299`

```python
self.lm_head = nn.Linear(
    in_features=3840,       # hidden_size
    out_features=128000,    # vocab_size
    bias=False,             # Qwen3: No bias
)
```

**Tied Embeddings: False**
- Input embedding (`embed_tokens`)ê³¼ Output embedding (`lm_head`)ì´ **ë¶„ë¦¬ë¨**
- Qwen3ê³¼ ë™ì¼í•œ ì„¤ì •
- ë©”ëª¨ë¦¬ëŠ” 2ë°° ì‚¬ìš©í•˜ì§€ë§Œ, í‘œí˜„ë ¥ í–¥ìƒ

**ë©”ëª¨ë¦¬ ê³„ì‚°**:
```python
# Tied=False (í˜„ì¬)
input_embed = 491M params (982 MB)
lm_head = 491M params (982 MB)
total = 982M params (1,964 MB â‰ˆ 2GB)

# Tied=True (ëŒ€ì•ˆ)
shared_embed = 491M params (982 MB)
total = 491M params (982 MB â‰ˆ 1GB)
ì ˆì•½ = 50%
```

---

## 3. Transformer ë ˆì´ì–´

### 3.1 ì „ì²´ êµ¬ì¡°

**íŒŒì¼**: `moai_llm/modeling/transformer.py`

```python
# Pre-LayerNorm ì•„í‚¤í…ì²˜ (Qwen3 ë™ì¼)
class MoaiDecoderLayer(nn.Module):
    def forward(self, x):
        # 1. Self-Attention with residual
        x = x + self.self_attn(
            self.input_layernorm(x)
        )

        # 2. Feed-Forward with residual
        x = x + self.mlp(
            self.post_attention_layernorm(x)
        )

        return x
```

**íŠ¹ì§•**:
- âœ… **Pre-LayerNorm**: Normalization â†’ Sub-layer â†’ Residual
- âœ… **Residual Connections**: Gradient íë¦„ ê°œì„ 
- âœ… **RMSNorm**: LayerNorm ëŒ€ì‹  (ê³„ì‚° íš¨ìœ¨ì )

---

### 3.2 Attention ë©”ì»¤ë‹ˆì¦˜ (GQA)

**íŒŒì¼**: `moai_llm/modeling/attention.py`

#### Grouped Query Attention (GQA)

```python
num_attention_heads = 28       # Query heads
num_key_value_heads = 4        # KV heads (ê³µìœ )
GQA_ratio = 7:1                # 7ê°œ Qê°€ 1ê°œ KV ê³µìœ 
```

**GQA ì¥ì **:
1. **KV Cache 7ë°° ê°ì†Œ**:
   ```python
   # MHA (Multi-Head Attention)
   KV_cache = 28 heads Ã— 128 dim Ã— 2 Ã— seq_len

   # GQA (7:1)
   KV_cache = 4 heads Ã— 128 dim Ã— 2 Ã— seq_len
   ì ˆì•½ = 7ë°°
   ```

2. **ì¶”ë¡  ì†ë„ í–¥ìƒ**: ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ê°ì†Œ
3. **ì„±ëŠ¥ ìœ ì§€**: MHAì™€ ìœ ì‚¬í•œ í’ˆì§ˆ

**Qwen3ê³¼ì˜ ì°¨ì´**:
- Qwen3: 1:1 ë¹„ìœ¨ (32:32, ë³´ìˆ˜ì )
- MOAI: 7:1 ë¹„ìœ¨ (28:4, ê³µê²©ì , ë©”ëª¨ë¦¬ íš¨ìœ¨)

---

#### Flash Attention

```python
# Flash Attention 2/3 ì§€ì›
if FLASH_ATTENTION_AVAILABLE:
    attn_output = flash_attn_func(
        q, k, v,
        dropout_p=0.0,
        softmax_scale=1.0 / math.sqrt(head_dim),
        causal=True,
    )
else:
    # í‘œì¤€ Attentionìœ¼ë¡œ fallback
    attn_output = standard_attention(q, k, v)
```

**Flash Attention ì¥ì **:
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (O(N) vs O(NÂ²))
- âœ… ì†ë„ í–¥ìƒ (2-4ë°°)
- âœ… Causal masking ì§€ì›
- âœ… ìë™ fallback (ë¯¸ì„¤ì¹˜ ì‹œ)

---

### 3.3 Feed-Forward Network (MLP)

**íŒŒì¼**: `moai_llm/modeling/activations.py`

#### SwiGLU Activation

```python
class SwiGLU(nn.Module):
    def forward(self, x):
        gate = self.gate_proj(x)    # (3840 â†’ 10240)
        up = self.up_proj(x)          # (3840 â†’ 10240)
        hidden = silu(gate) * up      # GLU
        return self.down_proj(hidden) # (10240 â†’ 3840)
```

**ì„¤ì •**:
- hidden_size: 3,840
- intermediate_size: 10,240 (2.67x, GLUìš©)
- activation: **SwiGLU** (Qwen3ëŠ” SiLU)
- bias: False

**SwiGLU vs SiLU**:
| Feature | SwiGLU (MOAI) | SiLU (Qwen3) |
|---------|---------------|--------------|
| íŒŒë¼ë¯¸í„° | 2Ã— up/gate projections | 1Ã— projection |
| ì„±ëŠ¥ | ë” ê°•ë ¥ (GPT-3, LLaMA) | ë‹¨ìˆœ |
| ë©”ëª¨ë¦¬ | ì•½ê°„ ë” ë§ìŒ | ì ìŒ |

---

### 3.4 Normalization (RMSNorm)

**íŒŒì¼**: `moai_llm/modeling/normalization.py`

```python
class MoaiRMSNorm(nn.Module):
    def forward(self, x):
        # FP32ë¡œ ê³„ì‚° (ìˆ˜ì¹˜ ì•ˆì •ì„±)
        input_dtype = x.dtype
        x = x.to(torch.float32)

        variance = x.pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(variance + self.eps)

        # ì›ë˜ dtypeìœ¼ë¡œ ë³µì›
        return (self.weight * x).to(input_dtype)
```

**ì„¤ì •**:
- eps: 1e-6 (Qwen3 ë™ì¼)
- FP32 ê³„ì‚° (ì •í™•ë„ ìœ ì§€)

**RMSNorm vs LayerNorm**:
| Feature | RMSNorm | LayerNorm |
|---------|---------|-----------|
| ê³„ì‚°ëŸ‰ | ì ìŒ (mean ë¶ˆí•„ìš”) | ë§ìŒ |
| ì„±ëŠ¥ | ë™ì¼ | ë™ì¼ |
| ì†ë„ | ë¹ ë¦„ | ëŠë¦¼ |

---

## 4. ê³ ê¸‰ ê¸°ëŠ¥

### 4.1 ê¸´ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ (YaRN)

ê¸°ë³¸ 32K tokensë¥¼ 128Kê¹Œì§€ í™•ì¥:

```python
# configs/long_context_config.json
{
  "max_position_embeddings": 32768,
  "rope_theta": 1000000.0,
  "rope_scaling": {
    "type": "yarn",
    "factor": 4.0,                              # 32K â†’ 128K
    "original_max_position_embeddings": 32768,
    "alpha": 1.0,
    "beta": 32.0
  }
}
```

**YaRN (Yet another RoPE extensioN)**:
- ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì°¨ë“± ìŠ¤ì¼€ì¼ë§
- ê³ ì£¼íŒŒ: ìŠ¤ì¼€ì¼ë§ ì—†ìŒ (ì •í™•ë„ ìœ ì§€)
- ì €ì£¼íŒŒ: NTK ìŠ¤ì¼€ì¼ë§ (ê¸´ ê±°ë¦¬)
- ì¶”ê°€ í•™ìŠµ ìµœì†Œí™”

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
python train.py \
    --mode pretrain \
    --dataset wikipedia --dataset_config 20220301.ko \
    --config_file configs/long_context_config.json \
    --output_dir outputs/pretrain-128k --bf16
```

---

### 4.2 Vocab Size ë³€ê²½ ì‹œ ì˜í–¥

í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ë¡œ vocab_sizeê°€ ë³€ê²½ë˜ë©´:

```python
# ê¸°ì¡´: vocab_size=128,000
embed_params = 128,000 Ã— 3,840 = 491M
lm_head_params = 128,000 Ã— 3,840 = 491M
total_embed = 982M params

# ì—…ë°ì´íŠ¸ í›„: vocab_size=150,000
embed_params = 150,000 Ã— 3,840 = 576M (+85M)
lm_head_params = 150,000 Ã— 3,840 = 576M (+85M)
total_embed = 1,152M params (+170M, +17%)

# ë©”ëª¨ë¦¬ ì¦ê°€ (BF16)
memory_increase = 170M Ã— 2 = 340 MB
```

**ì£¼ì˜**: Vocab size ë³€ê²½ ì‹œ **ê¸°ì¡´ ëª¨ë¸ê³¼ í˜¸í™˜ ë¶ˆê°€**. ì²˜ìŒë¶€í„° ì¬í•™ìŠµ í•„ìš”.

---

### 4.3 RoPE Theta ê°’ì˜ ì˜í–¥

```python
# rope_theta=10,000 (ê¸°ì¡´ í‘œì¤€)
effective_context = ~8K tokens
position_encoding = ì•ˆì •ì  ë²”ìœ„ ë‚´

# rope_theta=1,000,000 (Qwen3 ê¸°ì¤€)
effective_context = ~32K tokens (4ë°° ì¦ê°€)
position_encoding = 100ë°° í° thetaë¡œ ê¸´ ê±°ë¦¬ ìœ ì§€
```

**ê¶Œì¥**:
- ì¼ë°˜ ìš©ë„: theta=1,000,000 (ê¸°ë³¸ê°’)
- ì´ˆì¥ë¬¸ (128K+): YaRN scaling ì¶”ê°€

---

## 5. ì„±ëŠ¥ ìµœì í™”

### 5.1 ë©”ëª¨ë¦¬ ìµœì í™”

#### Gradient Checkpointing

```bash
python train.py \
    --gradient_checkpointing \  # ë©”ëª¨ë¦¬ 50% ì ˆì•½
    --batch_size 1 \
    --gradient_accumulation_steps 32 \
    --bf16
```

**íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì ˆì•½: 50%
- ì†ë„ ê°ì†Œ: 20%
- ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥

---

#### Mixed Precision (BF16/FP16)

```bash
# BF16 (ê¶Œì¥, A100/H100)
python train.py --bf16 ...

# FP16 (V100/RTX)
python train.py --fp16 ...
```

**ë©”ëª¨ë¦¬ ì ˆì•½**:
- FP32: 6GB (ëª¨ë¸) + 12GB (optimizer) = 18GB
- BF16: 6GB (ëª¨ë¸) + 6GB (optimizer) = 12GB
- ì ˆì•½: 33%

---

### 5.2 ì†ë„ ìµœì í™”

#### Flash Attention

```bash
# ì„¤ì¹˜
pip install flash-attn --no-build-isolation

# ìë™ í™œì„±í™”ë¨ (MOAI-LLMì´ ìë™ ê°ì§€)
python train.py --mode pretrain --dataset wikipedia --bf16
```

**ì†ë„ í–¥ìƒ**:
- í•™ìŠµ: 2-3ë°°
- ì¶”ë¡ : 3-4ë°°
- ë©”ëª¨ë¦¬: ê°ì†Œ

---

#### DataLoader Workers

```bash
python train.py \
    --dataloader_num_workers 4 \
    --dataloader_prefetch_factor 2
```

---

### 5.3 ì„±ëŠ¥ ì˜ˆì¸¡

#### í•™ìŠµ ì†ë„ (A100 Ã— 4 ê¸°ì¤€)

```
Batch size: 4 Ã— 4 = 16
Sequence length: 2048
Tokens/step: 32,768

ì†ë„:
- Standard Attention: ~3 sec/step (~10K tokens/sec)
- Flash Attention: ~1 sec/step (~32K tokens/sec)
- GQA íš¨ê³¼: KV cache 7ë°° ì ˆì•½
```

#### ì¶”ë¡  ì†ë„ (RTX 4090)

```
Batch size: 1
Sequence length: 512

ì†ë„:
- Standard: ~50 tokens/sec
- Flash Attention: ~80 tokens/sec
- GQA: ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ batch size ì¦ê°€ ê°€ëŠ¥
```

---

## 6. ì²´í¬ë¦¬ìŠ¤íŠ¸

MOAI-LLMì´ Qwen3 ì•„í‚¤í…ì²˜ë¥¼ ì™„ì „íˆ êµ¬í˜„í–ˆëŠ”ì§€ í™•ì¸:

- [x] SentencePiece BPE í† í¬ë‚˜ì´ì €
- [x] Qwen3 special tokens (`<|im_start|>`, `<|im_end|>`)
- [x] Token embedding (128K vocab)
- [x] RoPE (theta=1M, max_pos=32K)
- [x] QK-Normalization
- [x] Tied embeddings = False
- [x] Transformer Pre-LN êµ¬ì¡°
- [x] Grouped Query Attention (GQA 7:1)
- [x] RMSNorm (eps=1e-6)
- [x] SwiGLU activation
- [x] No bias in attention/MLP
- [x] Flash Attention ì§€ì›
- [x] YaRN/NTK RoPE scaling
- [x] í•™ìŠµ íŒŒì´í”„ë¼ì¸ (Pretrain/SFT)
- [x] HuggingFace í†µí•©

**ê²°ê³¼**: 15/15 í•­ëª© ì™„ë£Œ âœ…

---

## 7. ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **Qwen3**: https://arxiv.org/abs/2506.05176
- **RoPE**: https://arxiv.org/abs/2104.09864
- **YaRN**: https://arxiv.org/abs/2309.00071
- **Flash Attention**: https://arxiv.org/abs/2307.08691
- **GQA**: https://arxiv.org/abs/2305.13245

### ì½”ë“œ íŒŒì¼
- `moai_llm/config.py`: ëª¨ë¸ ì„¤ì •
- `moai_llm/modeling/model.py`: Token/Output Embedding
- `moai_llm/modeling/rope.py`: RoPE êµ¬í˜„
- `moai_llm/modeling/attention.py`: GQA, QK-Norm, Flash Attention
- `moai_llm/modeling/transformer.py`: Decoder Layer
- `moai_llm/modeling/activations.py`: SwiGLU
- `moai_llm/modeling/normalization.py`: RMSNorm

### ê´€ë ¨ ë¬¸ì„œ
- **USER_GUIDE.md**: ì™„ì „í•œ í•™ìŠµ ê°€ì´ë“œ
- **DATASETS.md**: ë°ì´í„°ì…‹ ê°€ì´ë“œ
- **QUICKSTART.md**: 10ë¶„ ë¹ ë¥¸ ì‹œì‘
- **TOKENIZER_UPDATE_GUIDE.md**: í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸

---

## ğŸ‰ ê²°ë¡ 

MOAI-LLMì€ **Qwen3ì˜ ëª¨ë“  í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì™„ì „íˆ êµ¬í˜„**í–ˆìŠµë‹ˆë‹¤!

### í•µì‹¬ ê°•ì :
1. âœ… Qwen3ê³¼ ë™ì¼í•œ ì„ë² ë”© (rope_theta=1M, max_pos=32K)
2. âœ… ìµœì‹  ì•ˆì •í™” ê¸°ë²• (QK-Norm, RMSNorm)
3. âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (GQA 7:1, Flash Attention)
4. âœ… í™•ì¥ ê°€ëŠ¥ (YaRNìœ¼ë¡œ 128K+ ì»¨í…ìŠ¤íŠ¸)
5. âœ… 3B íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ê· í˜•

**ë‹¤ìŒ ë‹¨ê³„**:
```bash
# í•™ìŠµ ì‹œì‘
python train.py --mode pretrain --dataset wikipedia --dataset_config 20220301.ko --bf16

# ì•„í‚¤í…ì²˜ ì´í•´ ì™„ë£Œ!
```
