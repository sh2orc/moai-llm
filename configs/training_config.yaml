# MOAI-LLM Training Configuration
# Intermediate recipe: 60-100B tokens, production-ready

# Model configuration
model:
  vocab_size: 128000
  hidden_size: 3840
  intermediate_size: 10240
  num_hidden_layers: 28
  num_attention_heads: 28
  num_key_value_heads: 4
  max_position_embeddings: 8192
  rope_theta: 10000.0
  rope_scaling: null  # Add YaRN config here if needed
  rms_norm_eps: 1.0e-06
  use_qk_norm: true
  hidden_act: "swiglu"
  tie_word_embeddings: false
  attention_dropout: 0.0
  initializer_range: 0.02
  use_cache: true
  attention_bias: false
  mlp_bias: false

# Data configuration
data:
  dataset_name: "wikipedia"  # Replace with your dataset
  dataset_config: "20220301.en"  # Dataset configuration
  tokenizer_path: "tokenizers/moai_tokenizer"  # Path to trained tokenizer
  text_column: "text"
  num_proc: 8  # Number of processes for data preprocessing
  use_sequence_packing: true
  packing_method: "hierarchical"  # "basic" or "hierarchical"

# Training configuration
training:
  output_dir: "./outputs/moai-3b-run1"
  run_name: "moai-3b-intermediate"

  # Training hyperparameters
  num_epochs: 1
  per_device_train_batch_size: 4  # Adjust based on GPU memory
  gradient_accumulation_steps: 64  # Effective batch = 4 * 64 * num_gpus

  # Optimizer settings (AdamW)
  learning_rate: 3.0e-04
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-08
  max_grad_norm: 1.0

  # Learning rate schedule (Warmup-Stable-Decay)
  lr_scheduler_type: "cosine"  # Use "constant_with_warmup" for WSD
  warmup_steps: 2000
  # For WSD, set warmup_ratio, stable_ratio, decay_ratio
  warmup_ratio: 0.02  # 2% of total steps
  stable_ratio: 0.78  # 78% of total steps
  decay_ratio: 0.20   # 20% of total steps

  # Mixed precision training
  bf16: true  # BF16 recommended for modern GPUs (A100, H100)
  fp16: false

  # Memory optimization
  gradient_checkpointing: true

  # Logging and saving
  logging_steps: 100
  save_steps: 1000
  save_total_limit: 3
  evaluation_strategy: "steps"
  eval_steps: 1000

  # Data loading
  dataloader_num_workers: 4
  dataloader_pin_memory: true

  # Distributed training
  ddp_find_unused_parameters: false

  # Weights & Biases
  use_wandb: true
  wandb_project: "moai-llm"
  wandb_entity: null  # Your wandb entity/username

# Loss configuration
loss:
  type: "multi_objective"  # "cross_entropy", "focal", "label_smoothing", "multi_objective"
  params:
    ce_weight: 0.6
    focal_weight: 0.3
    smooth_weight: 0.1
    focal_gamma: 2.0
    smoothing: 0.1

# Evaluation configuration
evaluation:
  metrics:
    - perplexity
    - loss
  generation_config:
    max_new_tokens: 128
    temperature: 0.7
    top_p: 0.9
    do_sample: true
