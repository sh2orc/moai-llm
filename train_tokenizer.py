"""
í† í¬ë‚˜ì´ì € í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (HuggingFace datasets ì§€ì›)

ì‚¬ìš©ë²•:
    # ìƒˆë¡œ í•™ìŠµ (ì²˜ìŒë¶€í„°)
    python train_tokenizer.py \
        --dataset wikipedia \
        --dataset_config 20220301.ko \
        --vocab_size 128000 \
        --output_dir tokenizers/

    # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì—…ë°ì´íŠ¸ (ìƒˆ ë°ì´í„° ì¶”ê°€)
    python train_tokenizer.py \
        --base_tokenizer tokenizers/moai_tokenizer.model \
        --dataset pubmed \
        --vocab_size 150000 \
        --output_dir tokenizers/updated/

    # ë¡œì»¬ txt íŒŒì¼
    python train_tokenizer.py \
        --input_files data/*.txt \
        --vocab_size 128000 \
        --output_dir tokenizers/

    # ì—¬ëŸ¬ ë°ì´í„°ì…‹ í˜¼í•©
    python train_tokenizer.py \
        --datasets wikipedia bookcorpus \
        --dataset_configs 20220301.en None \
        --vocab_size 128000 \
        --output_dir tokenizers/
"""

import argparse
import sentencepiece as spm
from pathlib import Path
from typing import List, Optional
import logging
import tempfile

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(message)s"
)
logger = logging.getLogger(__name__)


def download_and_prepare_text(
    dataset_name: Optional[str] = None,
    dataset_config: Optional[str] = None,
    input_files: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    text_column: str = "text",
    text_columns: Optional[List[str]] = None,
) -> str:
    """
    HuggingFace ë°ì´í„°ì…‹ ë˜ëŠ” ë¡œì»¬ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„

    Args:
        text_column: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ (ê¸°ë³¸: "text")
        text_columns: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ (ì˜ˆ: ["instruction", "output"])
                     ì§€ì • ì‹œ text_column ë¬´ì‹œí•˜ê³  ëª¨ë“  ì»¬ëŸ¼ ê²°í•©

    Returns:
        ì„ì‹œ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
    """
    temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8')

    if input_files:
        # ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
        logger.info(f"ğŸ“‚ Loading from local files: {len(input_files)} files")
        for input_file in input_files:
            with open(input_file, 'r', encoding='utf-8') as f:
                temp_file.write(f.read())
                temp_file.write('\n')

    elif dataset_name:
        # HuggingFace ë°ì´í„°ì…‹
        logger.info(f"ğŸ“¥ Downloading from HuggingFace: {dataset_name}")

        from datasets import load_dataset

        if dataset_config:
            logger.info(f"   Config: {dataset_config}")
            dataset = load_dataset(dataset_name, dataset_config, split="train", streaming=True)
        else:
            dataset = load_dataset(dataset_name, split="train", streaming=True)

        count = 0
        for item in dataset:
            if max_samples and count >= max_samples:
                break

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if text_columns:
                # ì—¬ëŸ¬ ì»¬ëŸ¼ ê²°í•© (instruction-output ë“±)
                text_parts = []
                for col in text_columns:
                    col_text = item.get(col, "")
                    if col_text:
                        text_parts.append(str(col_text))
                text = " ".join(text_parts)
            else:
                # ë‹¨ì¼ ì»¬ëŸ¼
                text = item.get(text_column, "")

            if text and len(text.strip()) > 50:
                temp_file.write(text)
                temp_file.write('\n')
                count += 1

                if count % 10000 == 0:
                    logger.info(f"   Downloaded {count:,} samples...")

        logger.info(f"âœ“ Downloaded {count:,} samples")

    else:
        raise ValueError("Either dataset_name or input_files must be provided")

    temp_file.close()
    return temp_file.name


def merge_training_data(base_tokenizer_path: str, new_data_path: str) -> str:
    """
    ê¸°ì¡´ í† í¬ë‚˜ì´ì €ì˜ í•™ìŠµ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„°ë¥¼ ë³‘í•©

    Args:
        base_tokenizer_path: ê¸°ì¡´ í† í¬ë‚˜ì´ì € ëª¨ë¸ ê²½ë¡œ
        new_data_path: ìƒˆ í•™ìŠµ ë°ì´í„° ê²½ë¡œ

    Returns:
        ë³‘í•©ëœ ë°ì´í„° ì„ì‹œ íŒŒì¼ ê²½ë¡œ
    """
    logger.info("ğŸ”„ Merging existing tokenizer data with new data...")

    # ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë¡œ ìƒ˜í”Œë§í•œ ë°ì´í„° ìƒì„±
    sp = spm.SentencePieceProcessor()
    sp.load(base_tokenizer_path)

    # ê¸°ì¡´ ì–´íœ˜ì˜ ëŒ€í‘œ ë¬¸ì¥ë“¤ ì¶”ì¶œ (vocabì˜ ì¼ë¶€ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜)
    existing_samples = []
    vocab_size = min(sp.vocab_size(), 10000)  # ìµœëŒ€ 10000ê°œ ìƒ˜í”Œ

    for i in range(vocab_size):
        piece = sp.id_to_piece(i)
        if piece.startswith('<') or piece.startswith('['):  # íŠ¹ìˆ˜ í† í° ì œì™¸
            continue
        # byte fallback í† í° ì œì™¸
        if piece.startswith('<0x'):
            continue
        existing_samples.append(piece)

    # ë³‘í•© íŒŒì¼ ìƒì„±
    merged_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8')

    # ê¸°ì¡´ ì–´íœ˜ ìƒ˜í”Œ ì¶”ê°€ (50%)
    logger.info(f"   Adding {len(existing_samples):,} samples from existing tokenizer")
    merged_file.write(' '.join(existing_samples))
    merged_file.write('\n')

    # ìƒˆ ë°ì´í„° ì¶”ê°€ (50%)
    logger.info(f"   Adding new training data from {new_data_path}")
    with open(new_data_path, 'r', encoding='utf-8') as f:
        merged_file.write(f.read())

    merged_file.close()
    logger.info(f"âœ“ Data merged successfully")

    return merged_file.name


def train_tokenizer(
    input_file: str,
    vocab_size: int,
    output_dir: str,
    model_prefix: str = "moai_tokenizer",
    character_coverage: float = 0.9995,
    base_tokenizer: Optional[str] = None,
):
    """
    SentencePiece í† í¬ë‚˜ì´ì € í•™ìŠµ

    Args:
        input_file: ì…ë ¥ í…ìŠ¤íŠ¸ íŒŒì¼
        vocab_size: ì–´íœ˜ í¬ê¸°
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        model_prefix: ëª¨ë¸ íŒŒì¼ëª… prefix
        character_coverage: ë¬¸ì ì»¤ë²„ë¦¬ì§€ (ë‹¤êµ­ì–´: 0.9995)
        base_tokenizer: ê¸°ì¡´ í† í¬ë‚˜ì´ì € ê²½ë¡œ (ì—…ë°ì´íŠ¸ ëª¨ë“œ)
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    full_model_prefix = str(output_path / model_prefix)

    # ì—…ë°ì´íŠ¸ ëª¨ë“œì¸ ê²½ìš° ë°ì´í„° ë³‘í•©
    if base_tokenizer:
        logger.info("="*80)
        logger.info("ğŸ”„ Updating Existing Tokenizer")
        logger.info("="*80)
        logger.info(f"Base tokenizer: {base_tokenizer}")
        logger.info(f"New vocab size: {vocab_size:,}")
        logger.info("="*80)

        # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì •ë³´ ì¶œë ¥
        sp_base = spm.SentencePieceProcessor()
        sp_base.load(base_tokenizer)
        logger.info(f"Original vocab size: {sp_base.vocab_size():,}")
        logger.info(f"Vocab increase: +{vocab_size - sp_base.vocab_size():,}")
        logger.info("="*80)

        # ë°ì´í„° ë³‘í•©
        merged_input = merge_training_data(base_tokenizer, input_file)
        input_file = merged_input
    else:
        logger.info("="*80)
        logger.info("ğŸ”¤ Training SentencePiece Tokenizer")
        logger.info("="*80)
        logger.info(f"Vocabulary size: {vocab_size:,}")
        logger.info(f"Character coverage: {character_coverage}")
        logger.info(f"Output: {full_model_prefix}")
        logger.info("="*80)

    # Special tokens (Qwen3 ìŠ¤íƒ€ì¼)
    special_tokens = [
        "<|endoftext|>",    # End of text
        "<|im_start|>",     # Instruction message start
        "<|im_end|>",       # Instruction message end
    ]

    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    train_args = {
        "input": input_file,
        "model_prefix": full_model_prefix,
        "model_type": "bpe",
        "vocab_size": vocab_size,
        "character_coverage": character_coverage,
        "num_threads": 16,
        "max_sentence_length": 16384,
        "shuffle_input_sentence": True,
        "add_dummy_prefix": True,
        "remove_extra_whitespaces": True,
        "normalization_rule_name": "nmt_nfkc_cf",
        "pad_id": 0,
        "bos_id": 1,
        "eos_id": 2,
        "unk_id": 3,
        "user_defined_symbols": ",".join(special_tokens),
        "split_digits": True,
        "split_by_unicode_script": True,
        "split_by_whitespace": True,
        "split_by_number": True,
        "byte_fallback": True,
    }

    # í•™ìŠµ
    logger.info("ğŸƒ Training...")
    spm.SentencePieceTrainer.train(**train_args)

    # ë³‘í•©ëœ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if base_tokenizer:
        import os
        os.remove(merged_input)

    # ê²€ì¦
    sp = spm.SentencePieceProcessor()
    sp.load(f"{full_model_prefix}.model")

    logger.info("="*80)
    logger.info("âœ… Tokenizer trained successfully!")
    logger.info("="*80)
    logger.info(f"Vocabulary size: {sp.vocab_size():,}")
    logger.info(f"BOS: {sp.id_to_piece(sp.bos_id())} (id={sp.bos_id()})")
    logger.info(f"EOS: {sp.id_to_piece(sp.eos_id())} (id={sp.eos_id()})")
    logger.info(f"PAD: {sp.id_to_piece(sp.pad_id())} (id={sp.pad_id()})")
    logger.info(f"UNK: {sp.id_to_piece(sp.unk_id())} (id={sp.unk_id()})")

    # í…ŒìŠ¤íŠ¸
    test_texts = [
        "Hello, world! This is a test.",
        "ì•ˆë…•í•˜ì„¸ìš”. í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
        "print('Hello, World!')",
    ]

    logger.info("="*80)
    logger.info("ğŸ§ª Tokenization Test")
    logger.info("="*80)
    for text in test_texts:
        tokens = sp.encode(text, out_type=str)
        logger.info(f"Text: {text}")
        logger.info(f"Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        logger.info(f"Count: {len(tokens)}")
        logger.info("-"*80)

    logger.info("="*80)
    logger.info(f"ğŸ“ Saved to: {full_model_prefix}.model")
    logger.info("="*80)


def main():
    parser = argparse.ArgumentParser(description="Train SentencePiece Tokenizer")

    # ë°ì´í„° ì†ŒìŠ¤
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--dataset",
        type=str,
        help="HuggingFace dataset name (e.g., wikipedia)"
    )
    group.add_argument(
        "--input_files",
        type=str,
        nargs="+",
        help="Local text files"
    )

    # ë°ì´í„°ì…‹ ì˜µì…˜
    parser.add_argument(
        "--dataset_config",
        type=str,
        help="Dataset config (e.g., 20220301.ko)"
    )
    parser.add_argument(
        "--text_column",
        type=str,
        default="text",
        help="Text column name (default: text)"
    )
    parser.add_argument(
        "--text_columns",
        type=str,
        nargs="+",
        default=None,
        help="Multiple text columns to combine (e.g., instruction output). Overrides --text_column"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Max samples from dataset (default: all)"
    )

    # í† í¬ë‚˜ì´ì € ì„¤ì •
    parser.add_argument(
        "--vocab_size",
        type=int,
        default=128000,
        help="Vocabulary size (default: 128000)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="tokenizers",
        help="Output directory (default: tokenizers)"
    )
    parser.add_argument(
        "--model_prefix",
        type=str,
        default="moai_tokenizer",
        help="Model file prefix (default: moai_tokenizer)"
    )
    parser.add_argument(
        "--character_coverage",
        type=float,
        default=0.9995,
        help="Character coverage (default: 0.9995 for multilingual)"
    )

    # ì—…ë°ì´íŠ¸ ëª¨ë“œ
    parser.add_argument(
        "--base_tokenizer",
        type=str,
        default=None,
        help="Base tokenizer model path to update (optional)"
    )

    args = parser.parse_args()

    # 1. í…ìŠ¤íŠ¸ ì¤€ë¹„
    logger.info("ğŸ“š Preparing training data...")
    temp_file = download_and_prepare_text(
        dataset_name=args.dataset,
        dataset_config=args.dataset_config,
        input_files=args.input_files,
        max_samples=args.max_samples,
        text_column=args.text_column,
        text_columns=args.text_columns,
    )

    # 2. í† í¬ë‚˜ì´ì € í•™ìŠµ
    train_tokenizer(
        input_file=temp_file,
        vocab_size=args.vocab_size,
        output_dir=args.output_dir,
        model_prefix=args.model_prefix,
        character_coverage=args.character_coverage,
        base_tokenizer=args.base_tokenizer,
    )

    # 3. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    import os
    os.remove(temp_file)

    logger.info("\nâœ¨ All done!")


if __name__ == "__main__":
    main()
