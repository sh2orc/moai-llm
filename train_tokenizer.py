"""
í† í¬ë‚˜ì´ì € í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (HuggingFace Tokenizers - Rust ê¸°ë°˜, ì´ˆê³ ì†)

ğŸš€ SentencePiece ëŒ€ë¹„ 10~100ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„!

ì‚¬ìš©ë²•:
    # Step 1: ë‹¤êµ­ì–´ 64K
    python train_tokenizer.py \
        --multilingual ko en ja zh \
        --vocab_size 64000 \
        --max_samples_per_lang 60000 \
        --turbo \
        --output_dir tokenizers/ \
        --model_prefix moai_multilingual

    # Step 2: Alpaca +16K â†’ 80K
    python train_tokenizer.py \
        --base_tokenizer tokenizers/moai_multilingual \
        --dataset unoooo/alpaca-korean \
        --vocab_size 80000 \
        --max_samples_per_lang 30000 \
        --turbo \
        --output_dir tokenizers/ \
        --model_prefix moai_alpaca


    # Step 3: ê¸ˆìœµ +16K â†’ 96K
    python train_tokenizer.py \
        --base_tokenizer tokenizers/moai_alpaca \
        --dataset Mineru/kor-open-finance \
        --vocab_size 96000 \
        --max_samples 30000 \
        --turbo \
        --output_dir tokenizers/ \
        --model_prefix moai_finance

    # Step 4: ê¸ˆìœµ +16K â†’ 128K
    python train_tokenizer.py \
        --base_tokenizer tokenizers/moai_finance \
        --dataset BCCard/BCAI-Finance-Kor \
        --vocab_size 128000 \
        --max_samples 100000 \
        --turbo \
        --output_dir tokenizers/ \
        --model_prefix moai_finance_bccard

----------------------------------------------------------

    # ë¡œì»¬ txt íŒŒì¼
    python train_tokenizer.py \
        --input_files data/*.txt \
        --vocab_size 128000 \
        --output_dir tokenizers/

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ ì œí•œ)
    python train_tokenizer.py \
        --dataset wikimedia/wikipedia \
        --dataset_config 20231101.ko \
        --vocab_size 32000 \
        --max_samples 100000 \
        --output_dir tokenizers/test

    # ğŸš€ FAST ëª¨ë“œ (ëŒ€ìš©ëŸ‰ì—ì„œë„ ë¹ ë¥´ê²Œ!)
    python train_tokenizer.py \
        --multilingual ko en ja zh \
        --vocab_size 128000 \
        --max_samples_per_lang 500000 \
        --fast \
        --output_dir tokenizers/

    # âš¡ ULTRAFAST ëª¨ë“œ (Unigram ì•Œê³ ë¦¬ì¦˜, ìµœëŒ€ ì†ë„!)
    python train_tokenizer.py \
        --multilingual ko en ja zh \
        --vocab_size 128000 \
        --max_samples_per_lang 500000 \
        --ultrafast \
        --output_dir tokenizers/
"""

import argparse
import os
import signal
import sys
from pathlib import Path
from typing import List, Optional, Iterator, Dict
import logging
import time
import threading
from queue import Queue
from concurrent.futures import ThreadPoolExecutor, as_completed

# tokenizers ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë°©ì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"


def signal_handler(sig, frame):
    """Ctrl+C ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
    print("\n\nâš ï¸  Interrupted by user. Exiting...")
    sys.exit(1)

# Ctrl+C í•¸ë“¤ëŸ¬ ë“±ë¡
signal.signal(signal.SIGINT, signal_handler)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ì–¸ì–´ë³„ Wikipedia config ë§¤í•‘
WIKI_LANG_CONFIG = {
    "ko": "20231101.ko",    # í•œêµ­ì–´
    "en": "20231101.en",    # ì˜ì–´
    "ja": "20231101.ja",    # ì¼ë³¸ì–´
    "zh": "20231101.zh",    # ì¤‘êµ­ì–´
    "de": "20231101.de",    # ë…ì¼ì–´
    "fr": "20231101.fr",    # í”„ë‘ìŠ¤ì–´
    "es": "20231101.es",    # ìŠ¤í˜ì¸ì–´
    "ru": "20231101.ru",    # ëŸ¬ì‹œì•„ì–´
    "pt": "20231101.pt",    # í¬ë¥´íˆ¬ê°ˆì–´
    "it": "20231101.it",    # ì´íƒˆë¦¬ì•„ì–´
    "vi": "20231101.vi",    # ë² íŠ¸ë‚¨ì–´
    "th": "20231101.th",    # íƒœêµ­ì–´
    "ar": "20231101.ar",    # ì•„ëì–´
}


def text_iterator(
    dataset_name: Optional[str] = None,
    dataset_config: Optional[str] = None,
    input_files: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    text_column: str = "text",
    text_columns: Optional[List[str]] = None,
    batch_size: int = 1000,
) -> Iterator[List[str]]:
    """
    í…ìŠ¤íŠ¸ ë°°ì¹˜ ì´í„°ë ˆì´í„° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    
    HuggingFace TokenizersëŠ” ì´í„°ë ˆì´í„°ë¥¼ ì§ì ‘ ë°›ì•„ì„œ í•™ìŠµ ê°€ëŠ¥
    â†’ ì„ì‹œ íŒŒì¼ ë¶ˆí•„ìš”, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    """
    
    if input_files:
        # ë¡œì»¬ íŒŒì¼ì—ì„œ ì½ê¸°
        logger.info(f"ğŸ“‚ Loading from local files: {len(input_files)} files")
        batch = []
        count = 0
        
        for input_file in input_files:
            with open(input_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if len(line) > 50:
                        batch.append(line)
                        count += 1
                        
                        if max_samples and count >= max_samples:
                            if batch:
                                yield batch
                            logger.info(f"âœ“ Loaded {count:,} samples")
                            return
                        
                        if len(batch) >= batch_size:
                            yield batch
                            batch = []
                            
                            if count % 100000 == 0:
                                logger.info(f"   Loaded {count:,} samples...")
        
        if batch:
            yield batch
        logger.info(f"âœ“ Loaded {count:,} samples")
    
    elif dataset_name:
        # HuggingFace ë°ì´í„°ì…‹
        logger.info(f"ğŸ“¥ Streaming from HuggingFace: {dataset_name}")
        
        from datasets import load_dataset
        
        if dataset_config:
            logger.info(f"   Config: {dataset_config}")
            dataset = load_dataset(dataset_name, dataset_config, split="train", streaming=True)
        else:
            dataset = load_dataset(dataset_name, split="train", streaming=True)
        
        batch = []
        count = 0
        detected_columns = None
        
        # ìë™ ì»¬ëŸ¼ ê°ì§€ë¥¼ ìœ„í•œ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì´ë¦„ë“¤
        common_text_columns = [
            "text", "content", "sentence", "document", "body",
            "instruction", "input", "output", "response", "answer", "question",
            "prompt", "completion", "context", "passage",
            "query", "reply", "message", "description",
        ]
        
        for item in dataset:
            # ì²« ì•„ì´í…œì—ì„œ ì»¬ëŸ¼ ìë™ ê°ì§€
            if detected_columns is None and not text_columns:
                if text_column in item and item.get(text_column):
                    detected_columns = [text_column]
                else:
                    # ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
                    detected_columns = []
                    for col in common_text_columns:
                        if col in item and item.get(col):
                            detected_columns.append(col)
                    
                    # ì—¬ì „íˆ ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ìì—´ í•„ë“œ ì‚¬ìš©
                    if not detected_columns:
                        detected_columns = [k for k, v in item.items() if isinstance(v, str) and len(v) > 10]
                    
                    if detected_columns:
                        logger.info(f"   Auto-detected columns: {detected_columns}")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            cols_to_use = text_columns or detected_columns or [text_column]
            if len(cols_to_use) > 1 or text_columns:
                text_parts = []
                for col in cols_to_use:
                    col_text = item.get(col, "")
                    if col_text:
                        text_parts.append(str(col_text))
                text = " ".join(text_parts)
            else:
                text = item.get(cols_to_use[0], "") if cols_to_use else ""
            
            if text and len(text.strip()) > 50:
                batch.append(text)
                count += 1
                
                if max_samples and count >= max_samples:
                    if batch:
                        yield batch
                    logger.info(f"âœ“ Downloaded {count:,} samples")
                    return
                
                if len(batch) >= batch_size:
                    yield batch
                    batch = []
                    
                    if count % 100000 == 0:
                        logger.info(f"   Downloaded {count:,} samples...")
        
        if batch:
            yield batch
        logger.info(f"âœ“ Downloaded {count:,} samples")
    
    else:
        raise ValueError("Either dataset_name or input_files must be provided")


def _load_language_data(
    lang: str,
    config: str,
    max_samples: Optional[int],
    output_queue: Queue,
    stop_event: threading.Event
) -> Dict:
    """
    ë‹¨ì¼ ì–¸ì–´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
    """
    from datasets import load_dataset
    
    result = {"lang": lang, "count": 0, "error": None}
    
    try:
        dataset = load_dataset(
            "wikimedia/wikipedia",
            config,
            split="train",
            streaming=True
        )
        
        count = 0
        for item in dataset:
            if stop_event.is_set():
                break
                
            text = item.get("text", "")
            if text and len(text.strip()) > 50:
                output_queue.put((lang, text))
                count += 1
                
                if count % 10000 == 0:
                    logger.info(f"   [{lang.upper()}] {count:,} samples...")
                
                if max_samples and count >= max_samples:
                    break
        
        result["count"] = count
        
    except Exception as e:
        result["error"] = str(e)
        logger.warning(f"âš ï¸  Failed to load {lang}: {e}")
    
    # ì™„ë£Œ ì‹ í˜¸
    output_queue.put((lang, None))
    return result


def multilingual_text_iterator(
    languages: List[str],
    max_samples_per_lang: Optional[int] = None,
    batch_size: int = 1000,
    parallel: bool = True,
) -> Iterator[List[str]]:
    """
    ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì´í„°ë ˆì´í„° - ì—¬ëŸ¬ ì–¸ì–´ì˜ Wikipediaë¥¼ ë³‘ë ¬ ë¡œë”©!
    
    Args:
        languages: ì–¸ì–´ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ko", "en", "ja", "zh"])
        max_samples_per_lang: ì–¸ì–´ë³„ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        parallel: ë³‘ë ¬ ë¡œë”© í™œì„±í™” (ê¸°ë³¸: True)
    """
    from datasets import load_dataset
    
    valid_languages = [lang for lang in languages if lang in WIKI_LANG_CONFIG]
    if not valid_languages:
        raise ValueError(f"No valid languages found. Available: {list(WIKI_LANG_CONFIG.keys())}")
    
    logger.info("="*80)
    logger.info("ğŸŒ Multilingual Wikipedia Loading" + (" (PARALLEL)" if parallel else ""))
    logger.info("="*80)
    logger.info(f"Languages: {', '.join(valid_languages)}")
    logger.info(f"Max samples per language: {max_samples_per_lang or 'unlimited'}")
    if parallel:
        logger.info(f"ğŸš€ Loading {len(valid_languages)} languages in PARALLEL!")
    logger.info("="*80)
    
    if parallel and len(valid_languages) > 1:
        # ========== ë³‘ë ¬ ë¡œë”© ëª¨ë“œ ==========
        data_queue = Queue(maxsize=50000)  # ë©”ëª¨ë¦¬ ì œí•œì„ ìœ„í•œ ìµœëŒ€ í í¬ê¸°
        stop_event = threading.Event()
        
        # ê° ì–¸ì–´ë³„ ìŠ¤ë ˆë“œ ì‹œì‘
        futures = []
        with ThreadPoolExecutor(max_workers=len(valid_languages)) as executor:
            for lang in valid_languages:
                config = WIKI_LANG_CONFIG[lang]
                logger.info(f"ğŸ“¥ [{lang.upper()}] Starting parallel download ({config})...")
                future = executor.submit(
                    _load_language_data,
                    lang, config, max_samples_per_lang, data_queue, stop_event
                )
                futures.append((lang, future))
            
            # ë°ì´í„° ìˆ˜ì§‘ ë° yield
            batch = []
            total_count = 0
            lang_counts = {lang: 0 for lang in valid_languages}
            completed_langs = set()
            
            try:
                while len(completed_langs) < len(valid_languages):
                    try:
                        lang, text = data_queue.get(timeout=1.0)
                        
                        if text is None:
                            # í•´ë‹¹ ì–¸ì–´ ì™„ë£Œ
                            completed_langs.add(lang)
                            logger.info(f"   [{lang.upper()}] âœ“ {lang_counts[lang]:,} samples complete")
                            continue
                        
                        batch.append(text)
                        total_count += 1
                        lang_counts[lang] += 1
                        
                        if len(batch) >= batch_size:
                            yield batch
                            batch = []
                            
                    except Exception:
                        # Queue íƒ€ì„ì•„ì›ƒ - ê³„ì† ì§„í–‰
                        pass
                
                # ë‚¨ì€ ë°°ì¹˜ yield
                if batch:
                    yield batch
                    
            except GeneratorExit:
                stop_event.set()
                raise
        
        logger.info(f"\nğŸŒ Total: {total_count:,} samples from {len(valid_languages)} languages (PARALLEL)")
        for lang, count in lang_counts.items():
            logger.info(f"   [{lang.upper()}] {count:,}")
    
    else:
        # ========== ìˆœì°¨ ë¡œë”© ëª¨ë“œ (ë‹¨ì¼ ì–¸ì–´ ë˜ëŠ” parallel=False) ==========
        total_count = 0
        
        for lang in valid_languages:
            config = WIKI_LANG_CONFIG[lang]
            logger.info(f"\nğŸ“¥ [{lang.upper()}] Loading wikimedia/wikipedia ({config})...")
            
            try:
                dataset = load_dataset(
                    "wikimedia/wikipedia",
                    config,
                    split="train",
                    streaming=True
                )
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to load {lang}: {e}")
                continue
            
            batch = []
            count = 0
            
            for item in dataset:
                text = item.get("text", "")
                
                if text and len(text.strip()) > 50:
                    batch.append(text)
                    count += 1
                    total_count += 1
                    
                    if max_samples_per_lang and count >= max_samples_per_lang:
                        if batch:
                            yield batch
                        logger.info(f"   [{lang.upper()}] âœ“ {count:,} samples (limit reached)")
                        break
                    
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
                        
                        if count % 100000 == 0:
                            logger.info(f"   [{lang.upper()}] Downloaded {count:,} samples...")
            
            if batch:
                yield batch
            
            if not (max_samples_per_lang and count >= max_samples_per_lang):
                logger.info(f"   [{lang.upper()}] âœ“ {count:,} samples (complete)")
        
        logger.info(f"\nğŸŒ Total: {total_count:,} samples from {len(valid_languages)} languages")


def extend_tokenizer(
    base_tokenizer_path: str,
    dataset_name: Optional[str] = None,
    dataset_config: Optional[str] = None,
    input_files: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    text_column: str = "text",
    text_columns: Optional[List[str]] = None,
    new_vocab_size: int = 150000,
    output_dir: str = "tokenizers",
    model_prefix: str = "moai_tokenizer_extended",
    min_frequency: int = 2,
    languages: Optional[List[str]] = None,
    max_samples_per_lang: Optional[int] = None,
    fast_mode: bool = False,
    ultrafast_mode: bool = False,
    turbo_mode: bool = False,
    parallel: bool = True,
):
    """
    ê¸°ì¡´ í† í¬ë‚˜ì´ì €ë¥¼ í™•ì¥í•˜ì—¬ ìƒˆ í† í° ì¶”ê°€ í•™ìŠµ
    
    í•µì‹¬: ê¸°ì¡´ vocabê³¼ mergesë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒˆ í† í°ë§Œ ì¶”ê°€!
    """
    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
    from tokenizers.normalizers import NFKC, Sequence
    from transformers import PreTrainedTokenizerFast
    import json
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ë¡œë“œ
    base_path = Path(base_tokenizer_path)
    
    if base_path.is_file() and base_path.suffix == '.json':
        tokenizer_json_path = base_path
    elif base_path.is_dir():
        tokenizer_json_path = base_path / "tokenizer.json"
        if not tokenizer_json_path.exists():
            raise FileNotFoundError(f"tokenizer.json not found in {base_path}")
    else:
        raise FileNotFoundError(f"Tokenizer not found: {base_tokenizer_path}")
    
    # JSON ì§ì ‘ ë¡œë“œí•˜ì—¬ vocabê³¼ merges ì¶”ì¶œ
    with open(tokenizer_json_path, 'r', encoding='utf-8') as f:
        tokenizer_data = json.load(f)
    
    # ê¸°ì¡´ vocabê³¼ merges ì¶”ì¶œ
    model_data = tokenizer_data.get("model", {})
    existing_vocab = model_data.get("vocab", {})
    existing_merges = model_data.get("merges", [])
    
    original_vocab_size = len(existing_vocab)
    original_merges_count = len(existing_merges)
    
    logger.info("="*80)
    logger.info("ğŸ”„ Extending Existing Tokenizer (Preserving Vocab & Merges)")
    logger.info("="*80)
    logger.info(f"Base tokenizer: {base_tokenizer_path}")
    logger.info(f"Original vocab size: {original_vocab_size:,}")
    logger.info(f"Original merges: {original_merges_count:,}")
    logger.info(f"Target vocab size: {new_vocab_size:,}")
    logger.info(f"Vocab to add: +{new_vocab_size - original_vocab_size:,}")
    logger.info("="*80)
    
    if new_vocab_size <= original_vocab_size:
        raise ValueError(f"new_vocab_size ({new_vocab_size}) must be greater than original ({original_vocab_size})")
    
    # ì¶”ê°€í•  vocab ìˆ˜ ê³„ì‚°
    vocab_to_add = new_vocab_size - original_vocab_size
    
    # ëª¨ë“œë³„ ìµœì í™”
    if ultrafast_mode:
        min_frequency = max(min_frequency, 3)
        limit_alphabet = 10000
        logger.info("âš¡âš¡ ULTRAFAST MODE ENABLED")
    elif turbo_mode:
        min_frequency = max(min_frequency, 10)
        limit_alphabet = 5000
        logger.info("ğŸš€ TURBO MODE ENABLED")
    elif fast_mode:
        min_frequency = max(min_frequency, 5)
        limit_alphabet = 10000
        logger.info("âš¡ FAST MODE ENABLED")
    else:
        limit_alphabet = None
    
    # ìƒˆ ë°ì´í„°ì—ì„œ ì¶”ê°€ í† í° í•™ìŠµì„ ìœ„í•œ ì„ì‹œ í† í¬ë‚˜ì´ì €
    temp_tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    temp_tokenizer.normalizer = Sequence([NFKC()])
    temp_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
    temp_tokenizer.decoder = decoders.ByteLevel()
    
    special_tokens = [
        "<pad>", "<s>", "</s>", "<unk>",
        "<|endoftext|>", "<|im_start|>", "<|im_end|>",
    ]
    
    # ì¶”ê°€ í† í°ë§Œ í•™ìŠµ (ì¶”ê°€í•  ì–‘ + ë²„í¼)
    # í•µì‹¬: ì „ì²´ í¬ê¸°ê°€ ì•„ë‹Œ, í•„ìš”í•œ ë§Œí¼ë§Œ í•™ìŠµ!
    # merge ê²°ê³¼ í† í°ë„ ê³ ë ¤í•´ì„œ ì•½ê°„ì˜ ë²„í¼ ì¶”ê°€ (1.5ë°°)
    train_vocab_size = int(vocab_to_add * 1.5) + len(special_tokens)
    logger.info(f"   Training for ~{train_vocab_size:,} new tokens (buffer included)")
    
    trainer_kwargs = {
        "vocab_size": train_vocab_size,
        "min_frequency": min_frequency,
        "special_tokens": special_tokens,
        "show_progress": True,
        "initial_alphabet": pre_tokenizers.ByteLevel.alphabet(),
    }
    
    if limit_alphabet:
        trainer_kwargs["limit_alphabet"] = limit_alphabet
    
    trainer = trainers.BpeTrainer(**trainer_kwargs)
    
    logger.info("ğŸƒ Training additional tokens from new data...")
    start_time = time.time()
    
    def get_training_corpus():
        # 1. ê¸°ì¡´ í† í°ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ í¬í•¨ (merges í•™ìŠµì— ë„ì›€)
        existing_tokens = list(existing_vocab.keys())
        vocab_sentences = [t for t in existing_tokens if not t.startswith('<') and len(t) > 1]
        batch_size = 1000
        for i in range(0, len(vocab_sentences), batch_size):
            yield vocab_sentences[i:i+batch_size]
        logger.info(f"   Included {len(vocab_sentences):,} existing tokens as seed")
        
        # 2. ìƒˆ ë°ì´í„°
        if languages:
            for batch in multilingual_text_iterator(languages, max_samples_per_lang, parallel=parallel):
                yield batch
        else:
            for batch in text_iterator(
                dataset_name=dataset_name,
                dataset_config=dataset_config,
                input_files=input_files,
                max_samples=max_samples,
                text_column=text_column,
                text_columns=text_columns,
            ):
                yield batch
    
    temp_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
    
    elapsed = time.time() - start_time
    logger.info(f"â±ï¸  Additional training completed in {elapsed:.1f} seconds!")
    
    # ìƒˆë¡œ í•™ìŠµëœ vocabê³¼ merges ì¶”ì¶œ
    temp_json = json.loads(temp_tokenizer.to_str())
    new_trained_vocab = temp_json["model"]["vocab"]
    new_trained_merges = temp_json["model"]["merges"]
    
    logger.info(f"   New training produced {len(new_trained_vocab):,} tokens, {len(new_trained_merges):,} merges")
    
    # ========== í•µì‹¬: ê¸°ì¡´ vocab/merges ìœ ì§€ + ìƒˆ í† í° ì¶”ê°€ ==========
    logger.info("ğŸ”— Merging vocabularies (preserving original)...")
    
    # ìµœì¢… vocab: ê¸°ì¡´ vocabì„ ê·¸ëŒ€ë¡œ ìœ ì§€
    final_vocab = dict(existing_vocab)
    next_id = max(existing_vocab.values()) + 1
    
    # ìµœì¢… merges: ê¸°ì¡´ merges ìœ ì§€ + ìƒˆ merges ì¶”ê°€
    # existing_mergesëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ìˆ˜ ìˆìŒ ([['a', 'b'], ...])
    # new_trained_mergesëŠ” ë¬¸ìì—´ í˜•íƒœ (['a b', ...])
    final_merges = list(existing_merges)
    
    # ê¸°ì¡´ mergesë¥¼ íŠœí”Œ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ set ìƒì„± (ë¦¬ìŠ¤íŠ¸ëŠ” í•´ì‹œ ë¶ˆê°€)
    def normalize_merge(m):
        """mergeë¥¼ íŠœí”Œ í˜•íƒœë¡œ ì •ê·œí™”"""
        if isinstance(m, list):
            return tuple(m)
        elif isinstance(m, str):
            return tuple(m.split(" "))
        return m
    
    existing_merges_set = set(normalize_merge(m) for m in existing_merges)
    
    # ìƒˆë¡œ í•™ìŠµëœ í† í° ì¤‘ ê¸°ì¡´ì— ì—†ëŠ” ê²ƒë§Œ ì¶”ê°€ (merge ê²°ê³¼ í† í° í¬í•¨í•´ì„œ ì´ëŸ‰ ì œí•œ)
    total_added = 0
    
    # 1ë‹¨ê³„: ìƒˆë¡œ í•™ìŠµëœ vocabì—ì„œ ì§ì ‘ í† í° ì¶”ê°€
    added_tokens = 0
    for token in new_trained_vocab:
        if token not in final_vocab and total_added < vocab_to_add:
            final_vocab[token] = next_id
            next_id += 1
            added_tokens += 1
            total_added += 1
    
    logger.info(f"   Added {added_tokens:,} new tokens from vocab")
    
    # 2ë‹¨ê³„: merge ì¶”ê°€ (ê²°ê³¼ í† í°ë„ ì´ëŸ‰ ì œí•œ ë‚´ì—ì„œë§Œ ì¶”ê°€)
    added_merges = 0
    added_merge_tokens = 0
    skipped_merges = 0
    
    for merge in new_trained_merges:
        merge_tuple = normalize_merge(merge)
        if merge_tuple not in existing_merges_set:
            parts = list(merge_tuple) if isinstance(merge_tuple, tuple) else merge.split(" ")
            if len(parts) == 2:
                part1, part2 = parts[0], parts[1]
                merged_token = part1 + part2
                
                # ì…ë ¥ í† í°ë“¤ì´ vocabì— ìˆì–´ì•¼ í•¨
                if part1 not in final_vocab or part2 not in final_vocab:
                    continue
                
                # ê²°ê³¼ í† í°ì´ vocabì— ì—†ìœ¼ë©´ ì¶”ê°€ (ì´ëŸ‰ ì œí•œ ì²´í¬!)
                if merged_token not in final_vocab:
                    if total_added >= vocab_to_add:
                        # ì œí•œ ì´ˆê³¼: ì´ mergeëŠ” ê±´ë„ˆë›°ê¸°
                        skipped_merges += 1
                        continue
                    final_vocab[merged_token] = next_id
                    next_id += 1
                    added_merge_tokens += 1
                    total_added += 1
                
                # merge ì¶”ê°€ (ê¸°ì¡´ merges í˜•íƒœì— ë§ì¶°ì„œ)
                if existing_merges and isinstance(existing_merges[0], list):
                    final_merges.append(list(merge_tuple))
                else:
                    final_merges.append(merge)
                existing_merges_set.add(merge_tuple)
                added_merges += 1
    
    logger.info(f"   Added {added_merges:,} new merges")
    if added_merge_tokens > 0:
        logger.info(f"   Added {added_merge_tokens:,} merge result tokens to vocab")
    if skipped_merges > 0:
        logger.info(f"   Skipped {skipped_merges:,} merges (vocab limit reached)")
    logger.info(f"   Total tokens added: {total_added:,} / {vocab_to_add:,}")
    
    # ìµœì¢… í† í¬ë‚˜ì´ì € JSON ìƒì„±
    final_tokenizer_data = dict(tokenizer_data)
    final_tokenizer_data["model"]["vocab"] = final_vocab
    final_tokenizer_data["model"]["merges"] = final_merges
    
    # JSONìœ¼ë¡œ ì €ì¥ í›„ ë¡œë“œ
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(final_tokenizer_data, f, ensure_ascii=False)
        temp_path = f.name
    
    final_tokenizer = Tokenizer.from_file(temp_path)
    os.unlink(temp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    
    # Post-processor ì„¤ì •
    final_tokenizer.post_processor = processors.TemplateProcessing(
        single="<s> $A </s>",
        pair="<s> $A </s> $B:1 </s>:1",
        special_tokens=[
            ("<s>", final_tokenizer.token_to_id("<s>")),
            ("</s>", final_tokenizer.token_to_id("</s>")),
        ],
    )
    
    # ì €ì¥
    tokenizer_path = output_path / f"{model_prefix}.json"
    final_tokenizer.save(str(tokenizer_path))
    
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=final_tokenizer,
        bos_token="<s>", eos_token="</s>", unk_token="<unk>", pad_token="<pad>",
        clean_up_tokenization_spaces=False,
    )
    hf_tokenizer.add_special_tokens({
        "additional_special_tokens": ["<|endoftext|>", "<|im_start|>", "<|im_end|>"]
    })
    hf_tokenizer.save_pretrained(str(output_path / model_prefix))
    
    # ê²€ì¦
    final_vocab_size = final_tokenizer.get_vocab_size()
    
    logger.info("="*80)
    logger.info("âœ… Tokenizer extended successfully!")
    logger.info("="*80)
    logger.info(f"Original vocab: {original_vocab_size:,}")
    logger.info(f"Final vocab: {final_vocab_size:,}")
    logger.info(f"Added tokens: +{final_vocab_size - original_vocab_size:,}")
    logger.info(f"Original merges: {original_merges_count:,}")
    logger.info(f"Final merges: {len(final_merges):,}")
    
    # ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸
    _test_tokenizer(final_tokenizer)
    
    logger.info("="*80)
    logger.info(f"ğŸ“ Saved to:")
    logger.info(f"   - {tokenizer_path}")
    logger.info(f"   - {output_path / model_prefix}/")
    logger.info("="*80)
    
    return final_tokenizer


def train_tokenizer(
    dataset_name: Optional[str] = None,
    dataset_config: Optional[str] = None,
    input_files: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    text_column: str = "text",
    text_columns: Optional[List[str]] = None,
    vocab_size: int = 128000,
    output_dir: str = "tokenizers",
    model_prefix: str = "moai_tokenizer",
    min_frequency: int = 2,
    languages: Optional[List[str]] = None,
    max_samples_per_lang: Optional[int] = None,
    fast_mode: bool = False,
    ultrafast_mode: bool = False,
    turbo_mode: bool = False,
    parallel: bool = True,
):
    """
    HuggingFace Tokenizersë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ (Rust ê¸°ë°˜, ì´ˆê³ ì†)
    
    Args:
        fast_mode: BPE + min_frequency=5, limit_alphabet=10000 (10ë°° ë¹ ë¦„)
        turbo_mode: BPE + min_frequency=10, limit_alphabet=5000 (20ë°° ë¹ ë¦„)
        ultrafast_mode: Unigram ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (50ë°° ë¹ ë¦„, Compute merges ì—†ìŒ)
    """
    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors
    from tokenizers.normalizers import NFKC, Sequence
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    use_unigram = ultrafast_mode
    
    # ëª¨ë“œë³„ ìµœì í™” (turbo > fast > ì¼ë°˜)
    if ultrafast_mode:
        min_frequency = max(min_frequency, 3)
        limit_alphabet = 10000
        logger.info("="*80)
        logger.info("âš¡âš¡ ULTRAFAST MODE - Unigram algorithm (NO Compute merges!)")
        logger.info("="*80)
    elif turbo_mode:
        min_frequency = max(min_frequency, 10)  # 10íšŒ ì´ìƒë§Œ!
        limit_alphabet = 5000  # ë” ì‘ì€ ì•ŒíŒŒë²³
        logger.info("="*80)
        logger.info("ğŸš€ TURBO MODE - Aggressive BPE optimization")
        logger.info("="*80)
    elif fast_mode:
        min_frequency = max(min_frequency, 5)
        limit_alphabet = 10000
        logger.info("="*80)
        logger.info("âš¡ FAST MODE - Optimized BPE")
        logger.info("="*80)
    else:
        limit_alphabet = None
    
    logger.info("="*80)
    if languages:
        logger.info("ğŸŒ Training Multilingual Tokenizer")
        logger.info(f"Languages: {', '.join(languages)}")
    else:
        logger.info("ğŸš€ Training Tokenizer (HuggingFace Tokenizers - Rust)")
    logger.info("="*80)
    logger.info(f"Algorithm: {'Unigram' if use_unigram else 'BPE'}")
    logger.info(f"Vocabulary size: {vocab_size:,}")
    logger.info(f"Min frequency: {min_frequency}")
    if limit_alphabet:
        logger.info(f"Limit alphabet: {limit_alphabet:,}")
    logger.info(f"Output: {output_path / model_prefix}")
    logger.info("="*80)
    
    special_tokens = [
        "<pad>", "<s>", "</s>", "<unk>",
        "<|endoftext|>", "<|im_start|>", "<|im_end|>",
    ]
    
    if use_unigram:
        # Unigram ëª¨ë¸ (í›¨ì”¬ ë¹ ë¦„!)
        tokenizer = Tokenizer(models.Unigram())
        tokenizer.normalizer = Sequence([NFKC()])
        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
        tokenizer.decoder = decoders.ByteLevel()
        
        trainer = trainers.UnigramTrainer(
            vocab_size=vocab_size,
            special_tokens=special_tokens,
            show_progress=True,
            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
        )
    else:
        # BPE ëª¨ë¸
        tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
        tokenizer.normalizer = Sequence([NFKC()])
        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
        tokenizer.decoder = decoders.ByteLevel()
        
        trainer_kwargs = {
            "vocab_size": vocab_size,
            "min_frequency": min_frequency,
            "special_tokens": special_tokens,
            "show_progress": True,
            "initial_alphabet": pre_tokenizers.ByteLevel.alphabet(),
        }
        
        if limit_alphabet:
            trainer_kwargs["limit_alphabet"] = limit_alphabet
        
        trainer = trainers.BpeTrainer(**trainer_kwargs)
    
    logger.info("ğŸƒ Training... (this is FAST with Rust!)")
    start_time = time.time()
    
    def get_training_corpus():
        if languages:
            # ë‹¤êµ­ì–´ ëª¨ë“œ
            for batch in multilingual_text_iterator(languages, max_samples_per_lang, parallel=parallel):
                yield batch
        else:
            # ë‹¨ì¼ ë°ì´í„°ì…‹ ëª¨ë“œ
            for batch in text_iterator(
                dataset_name=dataset_name,
                dataset_config=dataset_config,
                input_files=input_files,
                max_samples=max_samples,
                text_column=text_column,
                text_columns=text_columns,
            ):
                yield batch
    
    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
    
    elapsed = time.time() - start_time
    logger.info(f"â±ï¸  Training completed in {elapsed:.1f} seconds!")
    
    # Post-processor
    tokenizer.post_processor = processors.TemplateProcessing(
        single="<s> $A </s>",
        pair="<s> $A </s> $B:1 </s>:1",
        special_tokens=[
            ("<s>", tokenizer.token_to_id("<s>")),
            ("</s>", tokenizer.token_to_id("</s>")),
        ],
    )
    
    # ì €ì¥
    tokenizer_path = output_path / f"{model_prefix}.json"
    tokenizer.save(str(tokenizer_path))
    
    from transformers import PreTrainedTokenizerFast
    
    hf_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        bos_token="<s>", eos_token="</s>", unk_token="<unk>", pad_token="<pad>",
        clean_up_tokenization_spaces=False,
    )
    hf_tokenizer.add_special_tokens({
        "additional_special_tokens": ["<|endoftext|>", "<|im_start|>", "<|im_end|>"]
    })
    hf_tokenizer.save_pretrained(str(output_path / model_prefix))
    
    # ê²€ì¦
    logger.info("="*80)
    logger.info("âœ… Tokenizer trained successfully!")
    logger.info("="*80)
    logger.info(f"Vocabulary size: {tokenizer.get_vocab_size():,}")
    logger.info(f"PAD: <pad> (id={tokenizer.token_to_id('<pad>')})")
    logger.info(f"BOS: <s> (id={tokenizer.token_to_id('<s>')})")
    logger.info(f"EOS: </s> (id={tokenizer.token_to_id('</s>')})")
    logger.info(f"UNK: <unk> (id={tokenizer.token_to_id('<unk>')})")
    
    # ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸
    _test_tokenizer(tokenizer)
    
    logger.info("="*80)
    logger.info(f"ğŸ“ Saved to:")
    logger.info(f"   - {tokenizer_path} (raw tokenizer)")
    logger.info(f"   - {output_path / model_prefix}/ (HuggingFace format)")
    logger.info("="*80)
    
    return tokenizer


def _test_tokenizer(tokenizer):
    """ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    test_texts = [
        ("English", "Hello, world! This is a test."),
        ("í•œêµ­ì–´", "ì•ˆë…•í•˜ì„¸ìš”. í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."),
        ("æ—¥æœ¬èª", "ã“ã‚“ã«ã¡ã¯ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"),
        ("ä¸­æ–‡", "ä½ å¥½ã€‚è¿™æ˜¯åˆ†è¯å™¨æµ‹è¯•ã€‚"),
        ("Code", "print('Hello, World!')"),
    ]
    
    logger.info("="*80)
    logger.info("ğŸ§ª Multilingual Tokenization Test")
    logger.info("="*80)
    
    for lang, text in test_texts:
        encoded = tokenizer.encode(text)
        tokens = encoded.tokens[:12]
        logger.info(f"[{lang}] {text}")
        logger.info(f"   Tokens: {tokens}{'...' if len(encoded.tokens) > 12 else ''}")
        logger.info(f"   Count: {len(encoded.tokens)}")


def main():
    parser = argparse.ArgumentParser(
        description="Train BPE Tokenizer (HuggingFace Tokenizers - Rust, 10-100x faster!)"
    )
    
    # ë°ì´í„° ì†ŒìŠ¤ (3ê°€ì§€ ì¤‘ í•˜ë‚˜)
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--dataset",
        type=str,
        help="HuggingFace dataset name (e.g., wikimedia/wikipedia)"
    )
    group.add_argument(
        "--input_files",
        type=str,
        nargs="+",
        help="Local text files"
    )
    group.add_argument(
        "--multilingual",
        type=str,
        nargs="+",
        metavar="LANG",
        help="Language codes for multilingual training (e.g., ko en ja zh)"
    )
    
    # ë°ì´í„°ì…‹ ì˜µì…˜
    parser.add_argument(
        "--dataset_config",
        type=str,
        help="Dataset config (e.g., 20231101.ko)"
    )
    parser.add_argument(
        "--text_column",
        type=str,
        default="text",
        help="Text column name (default: text)"
    )
    parser.add_argument(
        "--text_columns",
        type=str,
        nargs="+",
        default=None,
        help="Multiple text columns to combine"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="Max samples from dataset (default: all)"
    )
    parser.add_argument(
        "--max_samples_per_lang",
        type=int,
        default=None,
        help="Max samples per language for multilingual mode"
    )
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    parser.add_argument(
        "--vocab_size",
        type=int,
        default=128000,
        help="Vocabulary size (default: 128000)"
    )
    parser.add_argument(
        "--min_frequency",
        type=int,
        default=2,
        help="Minimum token frequency (default: 2)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="tokenizers",
        help="Output directory (default: tokenizers)"
    )
    parser.add_argument(
        "--model_prefix",
        type=str,
        default="moai_tokenizer",
        help="Model file prefix (default: moai_tokenizer)"
    )
    
    # ì¶”ê°€ í•™ìŠµ ëª¨ë“œ
    parser.add_argument(
        "--base_tokenizer",
        type=str,
        default=None,
        help="Path to base tokenizer for extension"
    )
    
    # ìµœì í™” ì˜µì…˜
    parser.add_argument(
        "--fast",
        action="store_true",
        help="Fast mode: higher min_frequency, limited alphabet (10x faster)"
    )
    parser.add_argument(
        "--ultrafast",
        action="store_true",
        help="Ultrafast mode: Unigram algorithm instead of BPE (50x faster, no 'Compute merges')"
    )
    parser.add_argument(
        "--turbo",
        action="store_true",
        help="Turbo BPE mode: min_frequency=10, limit_alphabet=5000 (20x faster BPE)"
    )
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        dest="no_parallel",
        help="Disable parallel loading for multilingual mode (sequential, one language at a time)"
    )
    
    args = parser.parse_args()
    
    logger.info("ğŸ“š Starting tokenizer training...")
    logger.info("ğŸš€ Using HuggingFace Tokenizers (Rust-based, blazing fast!)")
    
    if args.base_tokenizer:
        # í™•ì¥ ëª¨ë“œ
        extend_tokenizer(
            base_tokenizer_path=args.base_tokenizer,
            dataset_name=args.dataset,
            dataset_config=args.dataset_config,
            input_files=args.input_files,
            max_samples=args.max_samples,
            text_column=args.text_column,
            text_columns=args.text_columns,
            new_vocab_size=args.vocab_size,
            min_frequency=args.min_frequency,
            output_dir=args.output_dir,
            model_prefix=args.model_prefix,
            languages=args.multilingual,
            max_samples_per_lang=args.max_samples_per_lang,
            fast_mode=args.fast,
            ultrafast_mode=args.ultrafast,
            turbo_mode=args.turbo,
            parallel=not args.no_parallel,
        )
    else:
        # ìƒˆë¡œ í•™ìŠµ
        train_tokenizer(
            dataset_name=args.dataset,
            dataset_config=args.dataset_config,
            input_files=args.input_files,
            max_samples=args.max_samples,
            text_column=args.text_column,
            text_columns=args.text_columns,
            vocab_size=args.vocab_size,
            min_frequency=args.min_frequency,
            output_dir=args.output_dir,
            model_prefix=args.model_prefix,
            languages=args.multilingual,
            max_samples_per_lang=args.max_samples_per_lang,
            fast_mode=args.fast,
            ultrafast_mode=args.ultrafast,
            turbo_mode=args.turbo,
            parallel=not args.no_parallel,
        )
    
    logger.info("\nâœ¨ All done!")


if __name__ == "__main__":
    main()
